From 9046264de9bd7e2f287815968d2e170a030f2921 Mon Sep 17 00:00:00 2001
From: Shaun Tancheff <shaun.tancheff@seagate.com>
Date: Mon, 12 Oct 2015 14:02:06 -0500
Subject: [PATCH 5/5] ZDM: New DM target 'zoned'

    ZDM presents a traditional block device for ZBC/ZAC zone based
    devices.
    User space utilities in zdm-tools for creating, repairing and restore
    DM instances at: https://github.com/Seagate

Signed-off-by: Shaun Tancheff <shaun.tancheff@seagate.com>
---
 Documentation/device-mapper/zoned.txt |   75 +
 MAINTAINERS                           |   21 +-
 drivers/md/Kconfig                    |   15 +-
 drivers/md/Makefile                   |    1 +
 drivers/md/dm-zoned.c                 | 1979 +++++++++++++
 drivers/md/dm-zoned.h                 |  399 +++
 drivers/md/libzoned.c                 | 5127 +++++++++++++++++++++++++++++++++
 7 files changed, 7608 insertions(+), 9 deletions(-)
 create mode 100644 Documentation/device-mapper/zoned.txt
 create mode 100644 drivers/md/dm-zoned.c
 create mode 100644 drivers/md/dm-zoned.h
 create mode 100644 drivers/md/libzoned.c

diff --git a/Documentation/device-mapper/zoned.txt b/Documentation/device-mapper/zoned.txt
new file mode 100644
index 0000000..2d1db28
--- /dev/null
+++ b/Documentation/device-mapper/zoned.txt
@@ -0,0 +1,75 @@
+Overview of Host Aware ZBC/ZAC Device Mapper
+  - Zone size (256MiB)
+  - Reset WP, Open Zone, Close Zone, Get Zone Info ...
+
+The Zoned DM treats a zoned device as a collection of 1024 zones 256GiB,
+referred to internally as 'megazones' as with zoned devices the last
+megazone may be less than 1024 zones in size. If the last zone is of
+different size it is ignored.
+
+What that means is that drives which report SAME=0 are not supported
+and likely won't be supported within this architecture. However drives
+which report a SAME code of: all same, last differs or same length
+different types, would all be supported by this architecture.
+
+The initial implementation focuses on drives with same sized zones of
+256MB which is 65536 4k blocks. In future the zone size of 256MB will
+be relaxed to allow any size of zone as long as they are all the same.
+
+Internally all addressing is on 4k boundaries. Currently a 4k PAGE_SIZE is
+assumed. Architectures with 8k (or other) PAGE_SIZE have not been tested
+and are likly broken at the moment.
+
+Host Managed drives should work if the zone type at the start of the partition
+is Conventional, or Preferred.
+
+
+Megazones:
+	Each megazone is managed independently and partitioned into
+	meta data and data. The Device Mapper Meta Data is logically
+	located in the 2nd and 3rd zones of the megazone. The 1st and
+	2nd zones are reserved for the megazone's minimally relocatable
+	super block which must be the first block of the first or
+	second zone. The most recent is determined by the generation
+	number embedded in the super block. The meta data is sized
+	for two zones and logically located in sectors non-addressable
+	to the upper device. The actual storage of the meta data is pooled
+	with the data using the same mapping scheme.
+	The device mapper internally is a COW device with a 4k per block
+	addressing scheme. There are some fix-ups to handle non-4k aligned
+	requests to support applications which read and write in 512 byte
+	blocks, however it is still desirable to submit patches for these
+	subsystems assuming the respective maintainers are willing to
+	accept 4k alignment patches.
+
+Address space:
+	The zoned device mapper presents a smaller block device than
+	the amount of data available on the physical media. The extra
+	space is used to hold the meta data needed for managing the
+	data being stored on the drive performing COW block [re]mapping.
+	The 'shrink' is done by appropriately sizing the device via
+	dmsetup.
+	See the zdmadm utility will detect and size the device appropriaty.
+
+Map Strategy:
+	Map incoming sector onto device mapper sector space.
+
+Read Strategy:
+	Check each block requested in the bio to determine if the data
+	blocks are consecutively stored on disk. Pass as much per-bio
+	as possible through to the backing block device.
+
+Write Strategy:
+	Allocate space for entire bio on the backing block device
+	redirecting all incoming write requests to the most recently
+	written zone until the zone is filled or the bio is too large
+	to fit and a new zone is opened. Note that if the zone is not
+	filled this zone will likely become used by meta data writes
+	which are typically single blocks.
+
+Sync Strategy:
+	On SYNC bios all the meta data need to restore the zoned device
+	mapper for disk is written to one of the well known zones at
+	the beginning of the mega zone. Data consistency is only
+	'guaranteed' to be on-disk and consistent following sync
+	events [same as ext4].
diff --git a/MAINTAINERS b/MAINTAINERS
index 052b031..42dc5ae 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -11058,13 +11058,12 @@ L:	zd1211-devs@lists.sourceforge.net (subscribers-only)
 S:	Maintained
 F:	drivers/net/wireless/zd1211rw/
 
-ZR36067 VIDEO FOR LINUX DRIVER
-L:	mjpeg-users@lists.sourceforge.net
-L:	linux-media@vger.kernel.org
-W:	http://mjpeg.sourceforge.net/driver-zoran/
-T:	hg http://linuxtv.org/hg/v4l-dvb
-S:	Odd Fixes
-F:	drivers/media/pci/zoran/
+ZDM ZONED DEVICE MAPPER TARGET
+M:	Shaun Tancheff <shaun.tancheff@seagate.com>
+L:	dm-devel@redhat.com
+S:	Maintained
+F:	drivers/md/dm-zoned.*
+F:	drivers/md/libzoned.c
 
 ZONED BLOCK DEVICE CONTROL SUPPORT
 M:	Shaun Tancheff <shaun.tancheff@seagate.com>
@@ -11074,6 +11073,14 @@ F:	block/blk-zoned-ctrl.c
 F:	include/linux/blk-zoned-ctrl.h
 F:	include/uapi/linux/blk-zoned-ctrl.h
 
+ZR36067 VIDEO FOR LINUX DRIVER
+L:	mjpeg-users@lists.sourceforge.net
+L:	linux-media@vger.kernel.org
+W:	http://mjpeg.sourceforge.net/driver-zoran/
+T:	hg http://linuxtv.org/hg/v4l-dvb
+S:	Odd Fixes
+F:	drivers/media/pci/zoran/
+
 ZRAM COMPRESSED RAM BLOCK DEVICE DRVIER
 M:	Minchan Kim <minchan@kernel.org>
 M:	Nitin Gupta <ngupta@vflare.org>
diff --git a/drivers/md/Kconfig b/drivers/md/Kconfig
index edcf4ab..4f507f5 100644
--- a/drivers/md/Kconfig
+++ b/drivers/md/Kconfig
@@ -37,9 +37,9 @@ config MD_AUTODETECT
 	default y
 	---help---
 	  If you say Y here, then the kernel will try to autodetect raid
-	  arrays as part of its boot process. 
+	  arrays as part of its boot process.
 
-	  If you don't use raid and say Y, this autodetection can cause 
+	  If you don't use raid and say Y, this autodetection can cause
 	  a several-second delay in the boot time due to various
 	  synchronisation steps that are part of this step.
 
@@ -323,6 +323,17 @@ config DM_ERA
          over time.  Useful for maintaining cache coherency when using
          vendor snapshots.
 
+config DM_ZONED
+       tristate "ZDM: Zoned based device target (EXPERIMENTAL)"
+       depends on BLK_DEV_DM
+       default n
+       select BLK_ZONED_CTRL
+       ---help---
+         dm-zoned provides a rand access block device on top of a
+         ZBC/ZAC block device.
+         Forward writing within zones, garbage collection within zones.
+         Use zdmadm to create, repair and/or restore ZDM instances.
+
 config DM_MIRROR
        tristate "Mirror target"
        depends on BLK_DEV_DM
diff --git a/drivers/md/Makefile b/drivers/md/Makefile
index dba4db5..a97d763 100644
--- a/drivers/md/Makefile
+++ b/drivers/md/Makefile
@@ -57,6 +57,7 @@ obj-$(CONFIG_DM_CACHE_MQ)	+= dm-cache-mq.o
 obj-$(CONFIG_DM_CACHE_CLEANER)	+= dm-cache-cleaner.o
 obj-$(CONFIG_DM_ERA)		+= dm-era.o
 obj-$(CONFIG_DM_LOG_WRITES)	+= dm-log-writes.o
+obj-$(CONFIG_DM_ZONED)		+= dm-zoned.o
 
 ifeq ($(CONFIG_DM_UEVENT),y)
 dm-mod-objs			+= dm-uevent.o
diff --git a/drivers/md/dm-zoned.c b/drivers/md/dm-zoned.c
new file mode 100644
index 0000000..1a5ff4c
--- /dev/null
+++ b/drivers/md/dm-zoned.c
@@ -0,0 +1,1979 @@
+/*
+ * Kernel Device Mapper for abstracting ZAC/ZBC devices as normal
+ * block devices for linux file systems.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include "dm.h"
+#include <linux/dm-io.h>
+#include <linux/init.h>
+#include <linux/mempool.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/random.h>	/* uuid */
+#include <linux/crc32c.h>	/* crc32c */
+#include <linux/crc16.h>
+#include <linux/sort.h>		/* sort [heapsort impl] */
+#include <linux/ctype.h>	/* isdigit() */
+#include <linux/blk-zoned-ctrl.h>
+#include <linux/timer.h>
+#include <linux/delay.h>
+
+#include "dm-zoned.h"
+
+
+/*
+ * FUTURE FIXME:
+ * Current sd.c does not swizzle on report zones and no
+ * scsi native drives exists so ... therefore all results are
+ * little endian ...
+ * When sd.c translates the output of report zones
+ * then remove the 'everything is little endian' assumption.
+ */
+#define REPORT_ZONES_LE_ONLY 1
+
+#define PRIu64 "llu"
+#define PRIx64 "llx"
+#define PRId32 "d"
+
+static inline char *_zdisk(struct zoned *znd)
+{
+	return znd->bdev_name;
+}
+
+#define Z_ERR(znd, fmt, arg...) \
+	pr_err("dm-zoned(%s): " fmt "\n", _zdisk(znd), ## arg)
+
+#define Z_INFO(znd, fmt, arg...) \
+	pr_info("dm-zoned(%s): " fmt "\n", _zdisk(znd), ## arg)
+
+#define Z_DBG(znd, fmt, arg...) \
+	pr_debug("dm-zoned(%s): " fmt "\n", _zdisk(znd), ## arg)
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static int dev_is_congested(struct dm_dev *dev, int bdi_bits);
+static int zoned_is_congested(struct dm_target_callbacks *cb, int bdi_bits);
+static int zoned_constructor(struct dm_target *ti, unsigned argc, char **argv);
+static void do_io_work(struct work_struct *work);
+static int block_io(struct zoned *, enum dm_io_mem_type, void *, sector_t,
+		    unsigned int, int, int);
+static int zoned_map_write(struct megazone*, struct bio*, struct map_addr*);
+static int zoned_map_read(struct zoned *znd, struct bio *bio);
+static int zoned_map(struct dm_target *ti, struct bio *bio);
+static sector_t get_dev_size(struct dm_target *ti);
+static int zoned_iterate_devices(struct dm_target *ti,
+				 iterate_devices_callout_fn fn, void *data);
+static int zoned_merge(struct dm_target *ti, struct bvec_merge_data *bvm,
+		       struct bio_vec *biovec, int max_size);
+static void zoned_io_hints(struct dm_target *ti, struct queue_limits *limits);
+static int is_zoned_inquiry(struct zoned *znd, int trim, int ata);
+static int dmz_reset_wp(struct megazone *megaz, u64 z_id);
+static int dmz_open_zone(struct megazone *megaz, u64 z_id);
+static int dmz_close_zone(struct megazone *megaz, u64 z_id);
+static u32 dmz_report_count(struct zoned *znd,
+			    struct bdev_zone_report *report, size_t bufsz);
+static int dmz_report_zones(struct zoned *znd, u64 z_id,
+			    struct bdev_zone_report *report, size_t bufsz);
+static void activity_timeout(unsigned long data);
+static void zoned_destroy(struct zoned *);
+static int gc_can_cherrypick(struct megazone *megaz);
+static void bg_work_task(struct work_struct *work);
+static void on_timeout_activity(struct zoned *znd);
+
+/**
+ * Get primary backing device inode
+ *   @param znd
+ */
+static inline struct inode *get_bdev_bd_inode(struct zoned *znd)
+{
+	return znd->dev->bdev->bd_inode;
+}
+
+#include "libzoned.c"
+
+static int dev_is_congested(struct dm_dev *dev, int bdi_bits)
+{
+	struct request_queue *q = bdev_get_queue(dev->bdev);
+
+	return bdi_congested(&q->backing_dev_info, bdi_bits);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_is_congested(struct dm_target_callbacks *cb, int bdi_bits)
+{
+	struct zoned *zoned = container_of(cb, struct zoned, callbacks);
+	int backing = dev_is_congested(zoned->dev, bdi_bits);
+
+	if (zoned->gc_backlog > 1) {
+		/*
+		 * Was BDI_async_congested;
+		 * Was BDI_sync_congested;
+		 */
+		backing |= 1 << BDI_async_congested;
+		backing |= 1 << BDI_sync_congested;
+	}
+	return backing;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void set_discard_support(struct gendisk *disk, int trim)
+{
+	DMINFO("dm-zoned(%s) - Discard Support: %s", trim ? "on" : "off",
+		disk->disk_name);
+
+	if (disk && disk->queue) {
+		struct queue_limits *limits = &disk->queue->limits;
+
+		limits->logical_block_size =
+			limits->physical_block_size =
+			limits->io_min = Z_C4K;
+		if (trim) {
+			limits->discard_granularity = Z_C4K;
+			limits->max_discard_sectors = 1 << 16;
+			limits->discard_zeroes_data = 1;
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void discard_support(struct zoned *znd, int trim)
+{
+	struct gendisk *disk = znd->dev->bdev->bd_disk;
+
+	if (disk->queue)
+		set_discard_support(disk, trim);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int is_zoned_inquiry(struct zoned *znd, int trim, int ata)
+{
+	struct gendisk *disk = znd->dev->bdev->bd_disk;
+
+	if (disk->queue) {
+		u8 extended = 1;
+		u8 page_op = 0xb1;
+		u8 *buf = NULL;
+		u16 sz = 64;
+		int wp_err;
+
+		set_discard_support(disk, trim);
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+		if (ata) {
+			struct zoned_identity ident;
+
+			wp_err = blk_zoned_identify_ata(disk, &ident);
+			if (!wp_err) {
+				if (ident.type_id == HOST_AWARE) {
+					znd->zinqtype = Z_TYPE_SMR_HA;
+					znd->ata_passthrough = 1;
+				}
+			}
+			return 0;
+		}
+
+		buf = ZDM_ALLOC(znd, Z_C4K, PG_01); /* zoned inq */
+		if (!buf)
+			return -ENOMEM;
+
+		wp_err = blk_zoned_inquiry(disk, extended, page_op, sz, buf);
+		if (!wp_err) {
+			znd->zinqtype = buf[Z_VPD_INFO_BYTE] >> 4 & 0x03;
+			if (znd->zinqtype != Z_TYPE_SMR_HA &&
+			    buf[4] == 0x17 && buf[5] == 0x5c) {
+				Z_ERR(znd, "Forcing ResetWP capability ... ");
+				znd->zinqtype = Z_TYPE_SMR_HA;
+				znd->ata_passthrough = 0;
+			}
+		}
+#else
+	#warning "CONFIG_BLK_ZONED_CTRL required."
+#endif
+		if (buf)
+			ZDM_FREE(znd, buf, Z_C4K, PG_01);
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_map_discard(struct zoned *znd, struct bio *bio)
+{
+	u64 lba     = 0;
+	int rcode   = DM_MAPIO_SUBMITTED;
+	u64 s_up    = bio->bi_iter.bi_sector >> 3;
+	u64 blks    = bio->bi_iter.bi_size / Z_C4K;
+	u64 count;
+	struct map_addr maddr;
+	struct megazone *megaz = NULL;
+	int err;
+
+	if (znd->is_empty)
+		goto out;
+
+	for (count = 0; count < blks; count++) {
+		u64 s_map = s_up + count;
+
+		map_addr_to_zdm(znd, s_map, &maddr);
+		megaz = &znd->z_mega[maddr.mz_id];
+
+		mutex_lock(&megaz->mz_io_mutex);
+		lba = z_lookup(megaz, &maddr);
+		if (lba) {
+			Z_DBG(znd, "TRIM: FS: %llx -> dm_s: %llx lba: %llx",
+				s_map, maddr.dm_s, lba);
+			err = z_mapped_discard(megaz, maddr.dm_s, lba);
+		}
+		mutex_unlock(&megaz->mz_io_mutex);
+
+		if (err) {
+			rcode = err;
+			goto out;
+		}
+
+		if (0 == (count & 0xFFFFF)) {
+			if (test_bit(DO_JOURNAL_MOVE, &megaz->flags) ||
+			    test_bit(DO_MEMPOOL, &megaz->flags)) {
+				if (!test_bit(DO_METAWORK_QD, &megaz->flags)) {
+
+					Z_ERR(znd, "Large discard ...");
+
+					set_bit(DO_METAWORK_QD, &megaz->flags);
+					queue_work(znd->meta_wq,
+						&megaz->meta_work);
+					flush_workqueue(znd->meta_wq);
+
+					Z_ERR(znd, " continue large discard");
+				}
+			}
+		}
+	}
+out:
+	if (rcode == DM_MAPIO_SUBMITTED)
+		bio_endio(bio, 0);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int dmz_reset_wp(struct megazone *megaz, u64 z_id)
+{
+	int wp_err = 0;
+
+	/*
+	 * FUTURE: Check zone 'type' flag is ZONED [and non conventional].
+	 */
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (megaz->znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = megaz->znd->dev->bdev->bd_disk;
+		u64 mapped_zoned = z_id + megaz->znd->first_zone;
+		u64 lba = Z_BLKSZ * ((megaz->mega_nr * 1024) + mapped_zoned);
+		u64 s_addr = lba * Z_BLOCKS_PER_DM_SECTOR;
+		int retry = 5;
+
+		wp_err = -1;
+		while (wp_err && --retry > 0) {
+			if (megaz->znd->ata_passthrough)
+				wp_err = blk_zoned_reset_wp_ata(disk, s_addr);
+			else
+				wp_err = blk_zoned_reset_wp(disk, s_addr);
+		}
+
+		if (wp_err) {
+			Z_ERR(megaz->znd, "Reset WP: %llu -> %d failed.",
+			       s_addr, wp_err);
+			Z_ERR(megaz->znd, "Disabling Reset WP capability");
+			megaz->znd->zinqtype = 0;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int dmz_open_zone(struct megazone *megaz, u64 z_id)
+{
+	int wp_err = 0;
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (megaz->znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = megaz->znd->dev->bdev->bd_disk;
+		u64 mapped_zoned = z_id + megaz->znd->first_zone;
+		u64 lba = Z_BLKSZ * ((megaz->mega_nr * 1024) + mapped_zoned);
+		u64 s_addr = lba * Z_BLOCKS_PER_DM_SECTOR;
+		int retry = 5;
+
+		wp_err = -1;
+		while (wp_err && --retry > 0) {
+			if (megaz->znd->ata_passthrough)
+				wp_err = blk_zoned_open_ata(disk, s_addr);
+			else
+				wp_err = blk_zoned_open(disk, s_addr);
+		}
+
+		if (wp_err) {
+			Z_ERR(megaz->znd, "Open Zone: %llx -> %d failed.",
+			       s_addr, wp_err);
+			Z_ERR(megaz->znd, "ZAC/ZBC support disabled.");
+			megaz->znd->zinqtype = 0;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int dmz_close_zone(struct megazone *megaz, u64 z_id)
+{
+	int wp_err = 0;
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (megaz->znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = megaz->znd->dev->bdev->bd_disk;
+		u64 mapped_zoned = z_id + megaz->znd->first_zone;
+		u64 lba = Z_BLKSZ * ((megaz->mega_nr * 1024) + mapped_zoned);
+		u64 s_addr = lba * Z_BLOCKS_PER_DM_SECTOR;
+		int retry = 5;
+
+		wp_err = -1;
+		while (wp_err && --retry > 0) {
+			if (megaz->znd->ata_passthrough)
+				wp_err = blk_zoned_close_ata(disk, s_addr);
+			else
+				wp_err = blk_zoned_close(disk, s_addr);
+		}
+
+		if (wp_err) {
+			Z_ERR(megaz->znd, "Open Zone: %llu -> %d failed.",
+			       s_addr, wp_err);
+			Z_ERR(megaz->znd, "Disabling Reset WP capability");
+			megaz->znd->zinqtype = 0;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u32 dmz_report_count(struct zoned *znd,
+			    struct bdev_zone_report *report, size_t bufsz)
+{
+	u32 count;
+	u32 max_count = (bufsz - sizeof(struct bdev_zone_report))
+		      /	 sizeof(struct bdev_zone_descriptor);
+
+	if (REPORT_ZONES_LE_ONLY || znd->ata_passthrough) {
+		/* ZAC: ata results are little endian */
+		if (max_count > le32_to_cpu(report->descriptor_count))
+			report->descriptor_count = cpu_to_le32(max_count);
+		count = le32_to_cpu(report->descriptor_count);
+	} else {
+		/* ZBC: scsi results are big endian */
+		if (max_count > be32_to_cpu(report->descriptor_count))
+			report->descriptor_count = cpu_to_be32(max_count);
+		count = be32_to_cpu(report->descriptor_count);
+	}
+	return count;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int dmz_report_zones(struct zoned *znd, u64 z_id,
+			    struct bdev_zone_report *report, size_t bufsz)
+{
+	int wp_err = -ENOTSUPP;
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = znd->dev->bdev->bd_disk;
+		u64 s_addr = (z_id + znd->first_zone) << 19;
+		u8  opt = ZOPT_NON_SEQ_AND_RESET;
+		int retry = 5;
+
+		wp_err = -1;
+		while (wp_err && --retry > 0) {
+			if (znd->ata_passthrough)
+				wp_err = blk_zoned_report_ata(disk, s_addr, opt,
+							      report, bufsz);
+			else
+				wp_err = blk_zoned_report(disk, s_addr, opt,
+							  report, bufsz);
+		}
+
+		if (wp_err) {
+			Z_ERR(znd, "Open Zone: %llu -> %d failed.",
+			       s_addr, wp_err);
+			Z_ERR(znd, "Disabling Reset WP capability");
+			znd->zinqtype = 0;
+			wp_err = -ENOTSUPP;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_zone_reset(struct bdev_zone_descriptor *dentry)
+{
+	u8 type = dentry->type & 0x0F;
+	u8 cond = (dentry->flags & 0xF0) >> 4;
+
+	return (ZCOND_ZC1_EMPTY == cond || ZTYP_CONVENTIONAL == type) ? 1 : 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int megazone_wp_sync(struct zoned *znd, int reset_non_empty)
+{
+	int rcode = 0;
+	u32 rcount = 0;
+	u32 iter;
+	size_t bufsz = REPORT_BUFFER * Z_C4K;
+	struct bdev_zone_report *report = kmalloc(bufsz, GFP_KERNEL);
+
+	if (!report) {
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	Z_ERR(znd, "%s: reset_non_empty: %d", __func__, reset_non_empty);
+
+	for (iter = 0; iter < znd->mega_zones_count; iter++) {
+		struct megazone *megaz = &znd->z_mega[iter];
+		int entry = (iter % 4) * 1024;
+		int z_nr;
+
+		if (0 == entry) {
+			u64 from = megaz->mega_nr * 1024;
+			int err = dmz_report_zones(znd, from, report, bufsz);
+
+			if (err) {
+				if (err != -ENOTSUPP)
+					rcode = err;
+				goto out;
+			}
+			rcount = dmz_report_count(znd, report, bufsz);
+		}
+
+		for (z_nr = 0;
+		     z_nr < megaz->z_count && entry < rcount;
+		     z_nr++, entry++) {
+			struct bdev_zone_descriptor *dentry
+				= &report->descriptors[entry];
+			u32 wp_at;
+			u32 wp;
+
+			if (reset_non_empty) {
+				int err = 0;
+
+				if (!is_zone_reset(dentry))
+					err = dmz_reset_wp(megaz, z_nr);
+
+				if (err) {
+					if (err != -ENOTSUPP)
+						rcode = err;
+					goto out;
+				}
+				wp = wp_at = 0;
+				megaz->z_ptrs[z_nr] = 0;
+				megaz->zfree_count[z_nr] = Z_BLKSZ;
+				continue;
+			}
+
+
+			/*
+			 * If passthrough then ZAC results are little endian.
+			 * otherwise ZBC results are big endian.
+			 */
+			if (REPORT_ZONES_LE_ONLY || znd->ata_passthrough)
+				wp = le64_to_cpu(dentry->lba_start)
+				   - le64_to_cpu(dentry->lba_wptr);
+			else
+				wp = be64_to_cpu(dentry->lba_start)
+				   - be64_to_cpu(dentry->lba_wptr);
+
+			wp >>= 3; /* 512 sectors to 4k sectors */
+			wp_at = megaz->z_ptrs[z_nr] & Z_WP_VALUE_MASK;
+
+			if (((0 == megaz->mega_nr) && (0 == z_nr))
+			    || (ZTYP_CONVENTIONAL == (dentry->type & 0x0F))) {
+				wp = wp_at; /* ignore the drive. */
+				megaz->z_ptrs[z_nr] |= Z_WP_NON_SEQ;
+			} else {
+				megaz->z_ptrs[z_nr] &= ~Z_WP_NON_SEQ;
+			}
+
+			if (wp > wp_at) {
+				u32 wp_flgs;
+				u32 lost = wp - wp_at;
+
+				wp_flgs = megaz->z_ptrs[z_nr] & Z_WP_FLAGS_MASK;
+				megaz->z_ptrs[z_nr] = wp & wp_flgs;
+				megaz->zfree_count[z_nr] += lost;
+
+				Z_ERR(znd,
+				     "MZ#%u z:%x [wp:%x rz:%x] lost %u blocks.",
+				     megaz->mega_nr, z_nr, wp_at, wp, lost);
+			}
+		}
+	}
+
+out:
+	kfree(report);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_actual_size(struct dm_target *ti, struct zoned *zoned)
+{
+	u64 size = i_size_read(zoned->dev->bdev->bd_inode);
+
+	zoned->nr_blocks = size / 4096;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_integrity_check(struct zoned *znd)
+{
+	int rc = 0;
+
+	if (znd->z_mega) {
+		u32 iter;
+
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			struct megazone *megaz = &znd->z_mega[iter];
+
+			set_bit(DO_META_CHECK, &megaz->flags);
+			queue_work(znd->meta_wq, &megaz->meta_work);
+		}
+
+		flush_workqueue(znd->meta_wq);
+
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			struct megazone *megaz = &znd->z_mega[iter];
+
+			if (megaz->meta_result)
+				rc = megaz->meta_result;
+		}
+	}
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * <data dev> <format|check|force>
+ */
+static int zoned_constructor(struct dm_target *ti, unsigned argc, char **argv)
+{
+	const int reset_non_empty = 0;
+	int create = 0;
+	int force = 0;
+	int check = 0;
+	int zbc_probe = 1;
+	int zac_probe = 1;
+	int trim = 1;
+	int r;
+	struct zoned *zoned;
+	long long starting_zone_nr = 0;
+	long long mz_md_provision = MZ_METADATA_ZONES;
+
+	BUILD_BUG_ON(Z_C4K != (sizeof(struct map_sect_to_lba) * Z_UNSORTED));
+	BUILD_BUG_ON(Z_C4K != (sizeof(struct io_4k_block)));
+	BUILD_BUG_ON(Z_C4K != (sizeof(struct mz_superkey)));
+	BUILD_BUG_ON(SYNC_IO_SZ < (sizeof(struct mz_state)));
+
+	if (argc < 1) {
+		ti->error = "Invalid argument count";
+		return -EINVAL;
+	}
+
+	for (r = 1; r < argc; r++) {
+		if (isdigit(*argv[r])) {
+			int krc = kstrtoll(argv[r], 0, &starting_zone_nr);
+
+			if (krc != 0) {
+				DMERR("Failed to parse %s: %d", argv[r], krc);
+				starting_zone_nr = 0;
+			}
+		}
+		if (!strcasecmp("create", argv[r]))
+			create = 1;
+		if (!strcasecmp("load", argv[r]))
+			create = 0;
+		if (!strcasecmp("check", argv[r]))
+			check = 1;
+		if (!strcasecmp("force", argv[r]))
+			force = 1;
+		if (!strcasecmp("nozbc", argv[r]))
+			zbc_probe = 0;
+		if (!strcasecmp("nozac", argv[r]))
+			zac_probe = 0;
+		if (!strcasecmp("discard", argv[r]))
+			trim = 1;
+		if (!strcasecmp("nodiscard", argv[r]))
+			trim = 0;
+
+		if (!strncasecmp("reserve=", argv[r], 8)) {
+			long long mz_resv;
+			int krc = kstrtoll(argv[r] + 8, 0, &mz_resv);
+
+			if (krc == 0) {
+				if (mz_resv > mz_md_provision)
+					mz_md_provision = mz_resv;
+			} else {
+				DMERR("Reserved 'FAILED TO PARSE.' %s: %d",
+					argv[r]+8, krc);
+				mz_resv = 0;
+			}
+		}
+	}
+
+	zoned = ZDM_ALLOC(NULL, sizeof(*zoned), KM_00);
+	if (!zoned) {
+		ti->error = "Error allocating zoned structure";
+		return -ENOMEM;
+	}
+
+	zoned->ti = ti;
+	ti->private = zoned;
+	zoned->first_zone = starting_zone_nr;
+	zoned->mz_provision = mz_md_provision;
+
+	r = dm_get_device(ti, argv[0], FMODE_READ | FMODE_WRITE, &zoned->dev);
+	if (r) {
+		ti->error = "Error opening backing device";
+		zoned_destroy(zoned);
+		return -EINVAL;
+	}
+
+	if (zoned->dev->bdev)
+		bdevname(zoned->dev->bdev, zoned->bdev_name);
+
+	Z_INFO(zoned, "First zone on device: %llx", starting_zone_nr);
+
+	/*
+	 * Set if this target needs to receive flushes regardless of
+	 * whether or not its underlying devices have support.
+	 */
+	ti->flush_supported = true;
+
+	/*
+	 * Set if this target needs to receive discards regardless of
+	 * whether or not its underlying devices have support.
+	 */
+	ti->discards_supported = true;
+
+	/*
+	 * Set if the target required discard bios to be split
+	 * on max_io_len boundary.
+	 */
+	ti->split_discard_bios = false;
+
+	/*
+	 * Set if this target does not return zeroes on discarded blocks.
+	 */
+	ti->discard_zeroes_data_unsupported = false;
+	/*
+	 * Set if this target wants discard bios to be sent.
+	 */
+	ti->num_discard_bios = 1;
+
+	if (!trim) {
+		ti->discards_supported = false;
+		ti->num_discard_bios = 0;
+	}
+
+	zoned_actual_size(ti, zoned);
+	zoned->callbacks.congested_fn = zoned_is_congested;
+	dm_table_add_target_callbacks(ti->table, &zoned->callbacks);
+	r = zoned_init(ti, zoned);
+	if (r) {
+		ti->error = "Error in zoned init";
+		zoned_destroy(zoned);
+		return -EINVAL;
+	}
+	if (zbc_probe) {
+		Z_ERR(zoned, "Checking for ZONED support %s",
+			trim ? "with trim" : "");
+		is_zoned_inquiry(zoned, trim, 0);
+	} else if (zac_probe) {
+		Z_ERR(zoned, "Checking for ZONED [ATA PASSTHROUGH] support %s",
+			trim ? "with trim" : "");
+		is_zoned_inquiry(zoned, trim, 1);
+	} else {
+		Z_ERR(zoned, "No PROBE");
+		discard_support(zoned, trim);
+	}
+
+	r = megazone_init(zoned);
+	if (r) {
+		ti->error = "Error in zoned init megazone";
+		zoned_destroy(zoned);
+		return -EINVAL;
+	}
+	r = zoned_init_disk(ti, zoned, create, check, force);
+	if (r) {
+		ti->error = "Error in zoned init from disk";
+		zoned_destroy(zoned);
+		return -EINVAL;
+	}
+	r = megazone_wp_sync(zoned, reset_non_empty);
+	if (r) {
+		ti->error = "Error in zoned re-sync WP";
+		zoned_destroy(zoned);
+		return -EINVAL;
+	}
+
+	/*
+	 * for each megaz,
+	 *    for each non-zero entry in the crc_md table,
+	 *	load the page  (and check the crc).
+	 */
+	if (check)
+		zoned_integrity_check(zoned);
+
+	mod_timer(&zoned->timer, jiffies + msecs_to_jiffies(5000));
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_dtr(struct dm_target *ti)
+{
+	struct zoned *znd = ti->private;
+
+	if (znd->z_superblock) {
+		struct mz_superkey *key_blk = znd->z_superblock;
+		struct zdm_superblock *sblock = &key_blk->sblock;
+
+		sblock->flags = cpu_to_le32(0);
+		sblock->csum = sb_crc32(sblock);
+	}
+
+	megazone_destroy(znd);
+	zoned_destroy(znd);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Read or write a chunk aligned and sized block of data from a device.
+ */
+static void do_io_work(struct work_struct *work)
+{
+	struct z_io_req_t *req = container_of(work, struct z_io_req_t, work);
+	struct dm_io_request *io_req = req->io_req;
+	unsigned long error_bits = 0;
+
+	req->result = dm_io(io_req, 1, req->where, &error_bits);
+	if (error_bits)
+		DMERR("ERROR: dm_io error: %lx", error_bits);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int block_io(struct zoned *znd,
+		    enum dm_io_mem_type dtype,
+		    void *data,
+		    sector_t block, unsigned int nDMsect, int rw, int queue)
+{
+	unsigned long error_bits = 0;
+	int rcode;
+	struct dm_io_region where = {
+		.bdev = znd->dev->bdev,
+		.sector = block,
+		.count = nDMsect,
+	};
+	struct dm_io_request io_req = {
+		.bi_rw = rw,
+		.mem.type = dtype,
+		.mem.offset = 0,
+		.mem.ptr.vma = data,
+		.client = znd->io_client,
+		.notify.fn = NULL,
+		.notify.context = NULL,
+	};
+
+	switch (dtype) {
+	case DM_IO_KMEM:
+		io_req.mem.ptr.addr = data;
+		break;
+	case DM_IO_BIO:
+		io_req.mem.ptr.bio = data;
+		where.count = nDMsect;
+		break;
+	case DM_IO_VMA:
+		io_req.mem.ptr.vma = data;
+		break;
+	default:
+		Z_ERR(znd, "page list not handled here ..  see dm-io.");
+		break;
+	}
+
+	if (queue) {
+		struct z_io_req_t req;
+
+		/*
+		 * Issue the synchronous I/O from a different thread
+		 * to avoid generic_make_request recursion.
+		 */
+		INIT_WORK_ONSTACK(&req.work, do_io_work);
+		req.where = &where;
+		req.io_req = &io_req;
+		queue_work(znd->io_wq, &req.work);
+		flush_workqueue(znd->io_wq);
+		destroy_work_on_stack(&req.work);
+
+		return req.result;
+	}
+
+	rcode = dm_io(&io_req, 1, &where, &error_bits);
+	if (error_bits)
+		Z_ERR(znd, "ERROR: dm_io error: %lx", error_bits);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * count -> count in 4k sectors.
+ */
+static int read_block(struct dm_target *ti, enum dm_io_mem_type dtype,
+		      void *data, u64 lba, unsigned int count, int queue)
+{
+	struct zoned *znd = ti->private;
+	sector_t block = lba * Z_BLOCKS_PER_DM_SECTOR;
+	unsigned int nDMsect = count * Z_BLOCKS_PER_DM_SECTOR;
+	int rc;
+
+	BUG_ON(lba >= znd->nr_blocks);
+
+	if (lba >= znd->nr_blocks) {
+		Z_ERR(znd, "Error reading past end of media: %llx.", lba);
+		rc = -EIO;
+		return rc;
+	}
+
+	rc = block_io(znd, dtype, data, block, nDMsect, READ, queue);
+	if (rc) {
+		Z_ERR(znd, "read error: %d -- R: %llx [%u dm sect] (Q:%d)",
+			rc, lba, nDMsect, queue);
+		dump_stack();
+	}
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * count -> count in 4k sectors.
+ */
+static int write_block(struct dm_target *ti, enum dm_io_mem_type dtype,
+		       void *data, u64 lba, unsigned int count, int queue)
+{
+	struct zoned *znd = ti->private;
+	sector_t block = lba * Z_BLOCKS_PER_DM_SECTOR;
+	unsigned int nDMsect = count * Z_BLOCKS_PER_DM_SECTOR;
+	int rc;
+
+	BUG_ON(lba >= znd->nr_blocks);
+
+	rc = block_io(znd, dtype, data, block, nDMsect, WRITE, queue);
+	if (rc) {
+		Z_ERR(znd, "write error: %d W: %llx [%u dm sect] (Q:%d)",
+			rc, lba, nDMsect, queue);
+		dump_stack();
+	}
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zm_cow(struct megazone *megaz, struct bio *bio,
+		  struct map_addr *maddr, u32 blks, u64 origin)
+{
+	struct dm_target *ti = megaz->znd->ti;
+	int count = 1;
+	int use_wq = 1;
+	unsigned int bytes = bio_cur_bytes(bio);
+	u8 *data = bio_data(bio);
+	u8 *io = NULL;
+	u16 ua_off = bio->bi_iter.bi_sector & 0x0007;
+	u16 ua_size = bio->bi_iter.bi_size & 0x0FFF;	/* in bytes */
+	u32 mapped = 0;
+	u64 disk_lba = 0;
+
+	if (!megaz->cow_block)
+		megaz->cow_block = ZDM_ALLOC(megaz->znd, Z_C4K, PG_02);
+
+	io = megaz->cow_block;
+	if (!io)
+		return -EIO;
+
+	disk_lba = z_acquire(megaz, Z_AQ_NORMAL, blks, &mapped);
+	if (!disk_lba || !mapped)
+		return -ENOSPC;
+
+	while (bytes) {
+		int rd;
+		unsigned int iobytes = Z_C4K;
+
+		/* ---------------------------------------------------------- */
+		if (origin) {
+			if (maddr->dm_s != megaz->cow_addr) {
+				Z_ERR(megaz->znd,
+					"Copy block from %llx <= %llx",
+					origin, maddr->dm_s);
+				rd = read_block(ti, DM_IO_KMEM, io, origin,
+						count, use_wq);
+				if (rd)
+					return -EIO;
+
+				megaz->cow_addr = maddr->dm_s;
+			} else {
+				Z_ERR(megaz->znd,
+					"Cached block from %llx <= %llx",
+					origin, maddr->dm_s);
+			}
+		} else {
+			memset(io, 0, Z_C4K);
+		}
+
+		if (ua_off)
+			iobytes -= ua_off * 512;
+
+		if (bytes < iobytes)
+			iobytes = bytes;
+
+		Z_ERR(megaz->znd, "Moving %u bytes from origin [offset:%u]",
+		      iobytes, ua_off * 512);
+
+		memcpy(io + (ua_off * 512), data, iobytes);
+
+		/* ---------------------------------------------------------- */
+
+		rd = write_block(ti, DM_IO_KMEM, io, disk_lba, count, use_wq);
+		if (rd)
+			return -EIO;
+
+		rd = z_mapped_addmany(megaz, maddr->dm_s, disk_lba, mapped);
+		if (rd) {
+			Z_ERR(megaz->znd, "%s: Journal MANY failed.", __func__);
+			return -EIO;
+		}
+
+		data += iobytes;
+		bytes -= iobytes;
+		ua_size -= (ua_size > iobytes) ? iobytes : ua_size;
+		ua_off = 0;
+		disk_lba++;
+
+		if (bytes && (ua_size || ua_off)) {
+			map_addr_calc(maddr->dm_s + 1, maddr);
+			origin = z_lookup(megaz, maddr);
+		}
+	}
+	bio_endio(bio, 0);
+
+	return DM_MAPIO_SUBMITTED;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+#define BIO_CACHE_SECTORS (SYNC_CACHE_PAGES * Z_BLOCKS_PER_DM_SECTOR)
+
+/**
+ * Write 4k blocks from cache to lba.
+ * Move any remaining 512 byte blocks to the start of cache and update
+ * the @param _cached count is update
+ */
+static int zm_write_cache(struct zoned *znd, struct io_dm_block *sync_cache,
+			  u64 lba, u32 *_cached)
+{
+	int use_wq    = 1;
+	int cached    = *_cached;
+	int blks      = cached / 8;
+	int sectors   = blks * 8;
+	int remainder = cached - sectors;
+	int err;
+
+	err = write_block(znd->ti, DM_IO_VMA, sync_cache, lba, blks, use_wq);
+	if (!err) {
+		if (remainder)
+			memcpy(sync_cache[0].data,
+			       sync_cache[sectors].data, remainder * 512);
+		*_cached = remainder;
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zm_write_pages(struct megazone *megaz, struct bio *bio,
+			  struct map_addr *maddr)
+{
+	struct zoned *znd = megaz->znd;
+	u64 sect_ori = maddr->dm_s;
+	u32 blks     = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u64 lba      = 0;
+	u32 cached   = 0; /* total: SYNC_CACHE_PAGES * 8 */
+	u32 written  = 0;
+	int avail    = 0;
+	int err;
+	struct bvec_iter start;
+	struct bvec_iter iter;
+	struct bio_vec bv;
+	struct io_4k_block *sync_cache_4k = get_sync_cache(megaz);
+	struct io_dm_block *sync_cache = NULL;
+
+	if (sync_cache_4k) {
+		sync_cache = (struct io_dm_block *)sync_cache_4k;
+	} else {
+		Z_ERR(megaz->znd, "FAILED to get SYNC CACHE.");
+		err = -ENOMEM;
+		goto out;
+	}
+
+	/* USE: megaz->sync_cache for dumping bio pages to disk ... */
+	start = bio->bi_iter; /* struct implicit copy */
+	do {
+		u64 alloc_ori = 0;
+		u32 mcount = 0;
+		u32 mapped = 0;
+
+reacquire:
+		/* when lba is zero blocks were not allocated. retry with the
+		   smaller request */
+		lba = z_acquire(megaz, Z_AQ_NORMAL, blks - written, &mapped);
+		if (!lba && mapped)
+			lba = z_acquire(megaz, Z_AQ_NORMAL, mapped, &mapped);
+
+		if (!lba) {
+			if (!znd->gc_throttle)
+				return -ENOSPC;
+
+			Z_ERR(znd, "Throttle input ... Mandatory GC.");
+			if (delayed_work_pending(&znd->gc_work)) {
+				mod_delayed_work(znd->gc_wq, &znd->gc_work, 0);
+				mutex_unlock(&megaz->mz_io_mutex);
+				flush_delayed_work(&znd->gc_work);
+				mutex_lock(&megaz->mz_io_mutex);
+			}
+			goto reacquire;
+		}
+
+		/* this may be redundant .. if we have lba we have mapped > 0 */
+		if (lba && mapped)
+			avail += mapped * 8; /* claimed pages in dm blocks */
+
+		alloc_ori = lba;
+
+		/* copy [upto mapped] pages to buffer */
+		__bio_for_each_segment(bv, bio, iter, start) {
+			int issue_write = 0;
+			void *src;
+
+			if (avail <= 0) {
+				Z_ERR(megaz->znd, "%s: TBD: Close Z# %llu",
+					__func__, alloc_ori >> 16);
+				start = iter;
+				break;
+			}
+
+			src = kmap_atomic(bv.bv_page);
+			memcpy(sync_cache[cached].data,
+			       src + bv.bv_offset, bv.bv_len);
+			kunmap_atomic(src);
+			cached += bv.bv_len / 512;
+			avail  -= bv.bv_len / 512;
+
+			if ((cached >= (mapped * 8)) ||
+			    (cached >= (BIO_CACHE_SECTORS - 8)))
+				issue_write = 1;
+
+			/* if there is less than 1 4k block in out cache,
+			 * send the available blocks to disk */
+			if (issue_write) {
+				int blks = cached / 8;
+
+				err = zm_write_cache(megaz->znd, sync_cache,
+						     lba, &cached);
+				if (err) {
+					Z_ERR(megaz->znd, "%s: bio-> %" PRIx64
+					      " [%d of %d blks] -> %d",
+					      __func__, lba, cached, blks, err);
+					bio_endio(bio, err);
+					goto out;
+				}
+				lba     += blks;
+				written += blks;
+				mcount  += blks;
+				mapped  -= blks;
+
+				if (0 == mapped) {
+					bio_advance_iter(bio, &iter, bv.bv_len);
+					start = iter;
+					break;
+				}
+				if (mapped < 0) {
+					Z_ERR(megaz->znd, "ERROR: Bad write %"
+					      PRId32 " beyond alloc'd space",
+					      mapped);
+				}
+			}
+		}
+		if ((mapped > 0) && ((cached / 8) > 0)) {
+			int blks = cached / 8;
+
+			err = zm_write_cache(megaz->znd, sync_cache, lba,
+					     &cached);
+			if (err) {
+				Z_ERR(megaz->znd, "%s: bio-> %" PRIx64
+				      " [%d of %d blks] -> %d",
+				      __func__, lba, cached, blks, err);
+				bio_endio(bio, err);
+				goto out;
+			}
+			lba     += blks;
+			written += blks;
+			mcount  += blks;
+			mapped  -= blks;
+
+			if (mapped < 0) {
+				Z_ERR(megaz->znd, "ERROR: [2] Bad write %"
+				      PRId32 " beyond alloc'd space",
+				      mapped);
+			}
+		}
+		err = z_mapped_addmany(megaz, maddr->dm_s, alloc_ori, mcount);
+		if (err) {
+			Z_ERR(megaz->znd, "%s: Journal MANY failed.", __func__);
+			err = DM_MAPIO_REQUEUE;
+			/*
+			 * FIXME:
+			 * Ending the BIO here is causing a GFP:
+			 -       DEBUG_PAGEALLOC
+			 -    in Workqueue:
+			 -        writeback bdi_writeback_workfn (flush-252:0)
+			 -    backtrace:
+			 -      __map_bio+0x7a/0x280
+			 -      __split_and_process_bio+0x2e3/0x4e0
+			 -      ? __split_and_process_bio+0x22/0x4e0
+			 -      ? generic_start_io_acct+0x5/0x210
+			 -      dm_make_request+0x6b/0x100
+			 -      generic_make_request+0xc0/0x110
+			 -      ....
+
+			 - bio_endio(bio, err);
+			 */
+			goto out;
+		}
+
+		if (written < blks)
+			map_addr_calc(sect_ori + written, maddr);
+
+		if (written == blks && cached > 0)
+			Z_ERR(megaz->znd, "%s: cached: %d un-written blocks!!",
+			      __func__, cached);
+	} while (written < blks);
+	bio_endio(bio, 0);
+	err = DM_MAPIO_SUBMITTED;
+
+out:
+	put_sync_cache(megaz, sync_cache_4k);
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static int zoned_map_write(struct megazone *megaz, struct bio *bio,
+			   struct map_addr *maddr)
+{
+	u32 blks     = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u16 ua_off   = bio->bi_iter.bi_sector & 0x0007;
+	u16 ua_size  = bio->bi_iter.bi_size & 0x0FFF;	/* in bytes */
+
+	if (ua_size || ua_off) {
+		u64 origin = z_lookup(megaz, maddr);
+
+		if (origin)
+			return zm_cow(megaz, bio, maddr, blks, origin);
+	}
+	return zm_write_pages(megaz, bio, maddr);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_map_read(struct zoned *znd, struct bio *bio)
+{
+	int rcode = DM_MAPIO_REMAPPED;
+	u64 ua_off = bio->bi_iter.bi_sector & 0x0007;
+	u64 ua_size = bio->bi_iter.bi_size & 0x0FFF;	/* in bytes */
+	u64 s_up = bio->bi_iter.bi_sector >> 3;
+	u64 blks = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	struct map_addr maddr;
+	u64 start_lba;
+	struct megazone *megaz = NULL;
+
+	map_addr_to_zdm(znd, s_up, &maddr);
+	megaz = &znd->z_mega[maddr.mz_id];
+	start_lba = z_lookup(megaz, &maddr);
+
+	if (start_lba) {
+		u64 sz;
+
+		bio->bi_iter.bi_sector = start_lba << 3;
+		if (ua_off)
+			bio->bi_iter.bi_sector += ua_off;
+
+		for (sz = 1; sz < blks; sz++) {
+			u64 next_lba;
+
+			map_addr_to_zdm(znd, s_up+sz, &maddr);
+			megaz = &znd->z_mega[maddr.mz_id];
+			next_lba = z_lookup(megaz, &maddr);
+			if (next_lba != (start_lba + sz)) {
+				unsigned nsect = sz * 8;
+
+				if (ua_size) {
+					unsigned ua_blocks = ua_size / 512;
+
+					nsect -= 8;
+					nsect += ua_blocks;
+				}
+				Z_DBG(megaz->znd,
+					"NON SEQ @ %llx + %llu [%llx] [%llx]",
+					 maddr.dm_s, sz, start_lba, next_lba);
+
+				dm_accept_partial_bio(bio, nsect);
+				return rcode;
+			}
+		}
+
+		if (ua_off || ua_size)
+			Z_ERR(megaz->znd, "(R): bio: sector: %lx bytes: %u",
+			      bio->bi_iter.bi_sector, bio->bi_iter.bi_size);
+
+		generic_make_request(bio);
+		rcode = DM_MAPIO_SUBMITTED;
+	} else {
+		/* drop read-ahead if not marked as used */
+		if (READA == bio_rw(bio))
+			return -EIO;
+
+		/* return 0's for deleted/unused blocks */
+		Z_DBG(znd, "%s: R s:%lx -> %llx sz:%llu (zero fill)",
+			 __func__, bio->bi_iter.bi_sector,
+			 maddr.dm_s, blks);
+
+		zero_fill_bio(bio);
+		bio_endio(bio, 0);
+		return DM_MAPIO_SUBMITTED;
+	}
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 mz_final(struct zoned *znd, struct bio *bio)
+{
+	u64 blks = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u64 s_up = bio->bi_iter.bi_sector >> 3;
+	struct map_addr maddr;
+
+	if (blks > 0)
+		s_up += (blks - 1);
+
+	map_addr_to_zdm(znd, s_up, &maddr);
+
+	return maddr.mz_id;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+/*
+ * Return the number of 4k sectors available
+ */
+static u32 mz_bio_blocks(struct zoned *znd, struct bio *bio, u64 mz_id)
+{
+	u64 s_up = bio->bi_iter.bi_sector >> 3;
+	u32 blks = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u32 count;
+	struct map_addr maddr;
+
+	for (count = 0; count < blks; count++) {
+		map_addr_to_zdm(znd, s_up+count, &maddr);
+		if (mz_id != maddr.mz_id)
+			break;
+	}
+	return count;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_map(struct dm_target *ti, struct bio *bio)
+{
+	struct zoned *znd = ti->private;
+	bool is_write = (bio_data_dir(bio) == WRITE);
+	sector_t sector_nr = bio->bi_iter.bi_sector / Z_BLOCKS_PER_DM_SECTOR;
+	int rcode = DM_MAPIO_REMAPPED;
+	struct map_addr maddr;
+	struct request_queue *q;
+	struct megazone *megaz = NULL;
+	int force_sync_now = 0;
+	struct block_device *bdev = bio->bi_bdev;
+
+	/* map to backing device ... NOT dm-zoned device */
+	bio->bi_bdev = znd->dev->bdev;
+
+	q = bdev_get_queue(bio->bi_bdev);
+	q->queue_flags |= QUEUE_FLAG_NOMERGES;
+
+	/* sector is from the upper layer (fs, gparted, etc) */
+	map_addr_to_zdm(znd, sector_nr, &maddr);
+
+	if (maddr.dm_s >= znd->nr_blocks) {
+		Z_ERR(znd,
+		      "%s requested %lu -> %llu is too large for device (%llu)",
+		      __func__, sector_nr, maddr.dm_s, znd->nr_blocks);
+		return -ENOSPC;
+	}
+
+	megaz = &znd->z_mega[maddr.mz_id];
+	if (is_write && megaz->meta_result)
+		return megaz->meta_result;
+
+	mutex_lock(&megaz->mz_io_mutex);
+
+	/* check for SYNC flag */
+	if (bio->bi_rw & REQ_SYNC) {
+		set_bit(DO_SYNC, &megaz->flags);
+		force_sync_now = 1;
+	}
+
+	Z_DBG(znd, "%s: s:%lu sz:%u -> %s [%llu]", __func__,
+		 sector_nr, bio->bi_iter.bi_size,
+		 is_write ? "W" : "R", maddr.mz_id);
+
+	if (bio->bi_iter.bi_size) {
+		if (bio->bi_rw & REQ_DISCARD) {
+			znd->gc_mz_pref = maddr.mz_id;
+			mutex_unlock(&megaz->mz_io_mutex);
+			rcode = zoned_map_discard(znd, bio);
+			mutex_lock(&megaz->mz_io_mutex);
+		} else if (is_write) {
+			znd->is_empty = 0;
+			znd->gc_mz_pref = maddr.mz_id;
+			if (mz_final(znd, bio) != maddr.mz_id) {
+				u32 accept;
+
+				accept = mz_bio_blocks(znd, bio, maddr.mz_id);
+				/*
+				 * accept number of 4k blocks -> 512 blocks
+				 * and have the upper layer remap them back
+				 * to ZDM.
+				 */
+				bio->bi_bdev = bdev;
+				dm_accept_partial_bio(bio, accept << 3);
+				rcode = DM_MAPIO_REMAPPED;
+				Z_ERR(znd, "ReMap to self [crossing MZ] %u",
+					accept);
+				mutex_unlock(&megaz->mz_io_mutex);
+				return rcode;
+			}
+			rcode = zoned_map_write(megaz, bio, &maddr);
+		} else {
+			rcode = zoned_map_read(znd, bio);
+		}
+		megaz->age = jiffies;
+	}
+
+	if (test_bit(DO_SYNC, &megaz->flags) ||
+	    test_bit(DO_JOURNAL_MOVE, &megaz->flags) ||
+	    test_bit(DO_MEMPOOL, &megaz->flags)) {
+		if (!test_bit(DO_METAWORK_QD, &megaz->flags)) {
+			set_bit(DO_METAWORK_QD, &megaz->flags);
+			queue_work(znd->meta_wq, &megaz->meta_work);
+		}
+	}
+
+	if (megaz->z_gc_free < 5) {
+		Z_ERR(znd, "... issue gc low on free space.");
+		gc_immediate(megaz);
+	}
+	mutex_unlock(&megaz->mz_io_mutex);
+
+	if (force_sync_now)
+		flush_workqueue(znd->meta_wq);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int _do_mem_purge(struct megazone *megaz, u64 mem_time)
+{
+	int do_work = 0;
+
+	if (time_before64(megaz->age, mem_time)) {
+		if (megaz->incore_count > 3) {
+			set_bit(DO_MEMPOOL, &megaz->flags);
+			if (!work_pending(&megaz->meta_work))
+				do_work = 1;
+		}
+	}
+	return do_work;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_can_cherrypick(struct megazone *megaz)
+{
+	int delay = 1;
+	int z_gc = megaz->z_data - 1;
+
+	for (; z_gc < megaz->z_count; z_gc++) {
+		int is_ready = is_ready_for_gc(megaz, z_gc);
+		const u32 wp = megaz->z_ptrs[z_gc] & Z_WP_VALUE_MASK;
+		const u32 nfree = megaz->zfree_count[z_gc];
+
+/* Maybe 'nfree > GC_PRIO_DEFAULT' ? */
+		if (is_ready && (wp == Z_BLKSZ) && (nfree == Z_BLKSZ)) {
+			if (gc_compact_check(megaz, delay))
+				return 1;
+		}
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void on_timeout_activity(struct zoned *znd)
+{
+	int gc_idle = 0;
+	int delay = 1;
+	u64 mem_time = msecs_to_jiffies(5000);
+	u64 tnow = jiffies;
+	unsigned long flags;
+	struct megazone *megaz;
+	u32 itr;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	if (!znd->gc_active)
+		gc_idle = 1;
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+	mem_time = (mem_time < tnow) ? tnow - mem_time : 0;
+
+	if (!znd->z_mega)
+		return;
+
+	if (test_bit(ZF_FREEZE, &znd->flags))
+		return;
+
+	/*
+	 * sort on discard ratio:
+	 *   - discard / zone count
+	 *
+	 * 1. Highest ratio gets first check for cherrypick
+	 * 2. otherwise: Scan all for cherrypick.
+	 * 3. otherwise: Highest ratio gets normal GC
+	 * 4. otherwise: Scan all for normal GC.
+	 * 5. For all issue memory purge.
+	 */
+
+	if (gc_idle) {
+		int pref_ratio;
+		int ratio;
+
+		/* sort on discard ratio */
+		megaz = &znd->z_mega[0];
+		znd->gc_mz_pref = 0;
+		pref_ratio = megaz->discard_count / megaz->z_count;
+		for (itr = 1; itr < znd->mega_zones_count; itr++) {
+			megaz = &znd->z_mega[itr];
+			ratio = megaz->discard_count / megaz->z_count;
+			if (ratio > pref_ratio) {
+				znd->gc_mz_pref = itr;
+				pref_ratio = ratio;
+			}
+		}
+
+		/* 1. CP highest */
+		megaz = &znd->z_mega[znd->gc_mz_pref];
+		if (gc_idle && gc_can_cherrypick(megaz))
+			gc_idle = 0;
+
+		/* 2. Scan for any CP */
+		for (itr = 0; gc_idle && itr < znd->mega_zones_count; itr++) {
+			megaz = &znd->z_mega[itr];
+			if (gc_idle && gc_can_cherrypick(megaz))
+				gc_idle = 0;
+		}
+
+		/* 3. Normal GC on highest */
+		megaz = &znd->z_mega[znd->gc_mz_pref];
+		if (gc_idle && gc_compact_check(megaz, delay))
+			gc_idle = 0;
+
+		/* 4. Scan all for normal GC. */
+		for (itr = 0; gc_idle && itr < znd->mega_zones_count; itr++) {
+			megaz = &znd->z_mega[itr];
+			if (gc_idle && gc_compact_check(megaz, delay))
+				gc_idle = 0;
+		}
+	}
+
+	/* 5. For all issue memory purge. */
+	for (itr = 0; itr < znd->mega_zones_count; itr++) {
+		megaz = &znd->z_mega[itr];
+		if (_do_mem_purge(megaz, mem_time))
+			queue_work(znd->meta_wq, &megaz->meta_work);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void bg_work_task(struct work_struct *work)
+{
+	struct zoned *znd;
+
+	if (!work)
+		return;
+
+	znd = container_of(work, struct zoned, bg_work);
+	on_timeout_activity(znd);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void activity_timeout(unsigned long data)
+{
+	struct zoned *znd = (struct zoned *) data;
+
+	if (!work_pending(&znd->bg_work))
+		queue_work(znd->bg_wq, &znd->bg_work);
+
+	if (!test_bit(ZF_FREEZE, &znd->flags))
+		mod_timer(&znd->timer, jiffies + msecs_to_jiffies(2500));
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static sector_t get_dev_size(struct dm_target *ti)
+{
+	struct zoned *znd = ti->private;
+	u64 sz = i_size_read(get_bdev_bd_inode(znd));	/* size in bytes. */
+	u64 lut_resv;
+
+	lut_resv = (znd->mega_zones_count * znd->mz_provision);
+
+	Z_DBG(znd, "%s size: %llu (/8) -> %llu blks -> zones -> %llu mz: %llu",
+		 __func__, sz, sz / 4096, (sz / 4096) / 65536,
+		 ((sz / 4096) / 65536) / 1024);
+
+	sz -= (lut_resv * Z_SMR_SZ_BYTES);
+
+	Z_DBG(znd, "%s backing device size: %llu (4k blocks)", __func__, sz);
+
+	/*
+	 * NOTE: `sz` should match `ti->len` when the dm_table
+	 *       is setup correctly
+	 */
+
+	return to_sector(sz);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_iterate_devices(struct dm_target *ti,
+				 iterate_devices_callout_fn fn, void *data)
+{
+	struct zoned *zoned = ti->private;
+	int rc = fn(ti, zoned->dev, 0, get_dev_size(ti), data);
+
+	Z_DBG(zoned, "%s: %p -> rc: %d", __func__, fn, rc);
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Follow the backing device limits for READ [and DISCARD].
+ * Limit WRITE requests to the current zone max [Enforced in ->map()]
+ */
+static int zoned_merge(struct dm_target *ti, struct bvec_merge_data *bvm,
+		       struct bio_vec *biovec, int max_size)
+{
+	struct zoned *znd = ti->private;
+	struct request_queue *q = bdev_get_queue(znd->dev->bdev);
+	sector_t sector_nr = bvm->bi_sector / Z_BLOCKS_PER_DM_SECTOR;
+	struct megazone *megaz = NULL;
+	struct map_addr maddr;
+	u32 wptr = 0;
+	u32 avail = 0;
+	int zmax = 4096;
+	int bdev_max = 4096;
+
+	map_addr_to_zdm(znd, sector_nr, &maddr);
+	bvm->bi_bdev = znd->dev->bdev;
+	bvm->bi_sector = maddr.dm_s * Z_BLOCKS_PER_DM_SECTOR;
+
+	if (q->merge_bvec_fn) {
+		bdev_max = q->merge_bvec_fn(q, bvm, biovec);
+		if (max_size > bdev_max)
+			max_size = bdev_max;
+	}
+
+	megaz = &znd->z_mega[maddr.mz_id];
+	wptr = megaz->z_ptrs[megaz->z_current];
+
+	if (wptr < Z_BLKSZ)
+		avail = Z_BLKSZ - wptr;
+
+	if (avail > 25)
+		avail = 25; /* arbitrary I/O limit in 4k blocks.*/
+
+	avail *= 4096;
+	if (avail)
+		zmax = avail;
+
+	if (max_size > zmax)
+		max_size = zmax;
+
+	return max_size;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_io_hints(struct dm_target *ti, struct queue_limits *limits)
+{
+	u64 io_opt_sectors = limits->io_opt >> SECTOR_SHIFT;
+
+	/*
+	 * If the system-determined stacked limits are compatible with the
+	 * zoned device's blocksize (io_opt is a factor) do not override them.
+	 */
+	if (io_opt_sectors < 8 || do_div(io_opt_sectors, 8)) {
+		blk_limits_io_min(limits, 0);
+		blk_limits_io_opt(limits, 8 << SECTOR_SHIFT);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_status(struct dm_target *ti, status_type_t type,
+			 unsigned status_flags, char *result, unsigned maxlen)
+{
+	struct zoned *znd = (struct zoned *) ti->private;
+
+	switch (type) {
+	case STATUSTYPE_INFO:
+		result[0] = '\0';
+		break;
+
+	case STATUSTYPE_TABLE:
+		scnprintf(result, maxlen, "%s Z#%llu", znd->dev->name,
+			 znd->first_zone);
+		break;
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_ioctl_fwd(struct dm_dev *dev, unsigned int cmd,
+			   unsigned long arg)
+{
+	int r = scsi_verify_blk_ioctl(NULL, cmd);
+
+	if (0 == r)
+		r = __blkdev_driver_ioctl(dev->bdev, dev->mode, cmd, arg);
+
+	return r;
+}
+
+static int do_ioc_wpstat(struct zoned *znd, unsigned long arg, int what)
+{
+	void __user *parg = (void __user *)arg;
+	int error = -EFAULT;
+	struct zdm_ioc_request *req;
+
+	req = kzalloc(sizeof(*req), GFP_KERNEL);
+	if (!req) {
+		error = -ENOMEM;
+		goto out;
+	}
+
+	if (copy_from_user(req, parg, sizeof(*req)))
+		goto out;
+
+	if (req->megazone_nr < znd->mega_zones_count) {
+		struct megazone *megaz = &znd->z_mega[req->megazone_nr];
+		u32 reply_sz =
+		    req->result_size < Z_C4K ? req->result_size : Z_C4K;
+		void *send_what = what ? megaz->z_ptrs : megaz->zfree_count;
+
+		if (copy_to_user(parg, send_what, reply_sz))
+			error = -EFAULT;
+
+		error = 0;
+	}
+out:
+	kfree(req);
+
+	return error;
+}
+
+static void fill_ioc_status(struct megazone *megaz,
+			    struct zdm_ioc_status *status)
+{
+	int entry;
+
+	memset(status, 0, sizeof(*status));
+	status->mc_entries = megaz->mc_entries;
+
+	for (entry = megaz->z_data; entry < megaz->z_count; entry++) {
+		u32 used = megaz->z_ptrs[entry] & Z_WP_VALUE_MASK;
+
+		status->b_used += used;
+		status->b_available += Z_BLKSZ - used;
+	}
+	status->b_discard = megaz->discard_count;
+
+	/*  fixed array of ->sectortm and ->reversetm */
+	status->m_used = 2 * ((sizeof(struct map_pg *) * Z_BLKSZ) / 4096);
+	status->inpool = megaz->znd->memstat;
+	memcpy(status->bins, megaz->znd->bins, sizeof(status->bins));
+	status->mlut_blocks = megaz->incore_count;
+
+	for (entry = 0; entry < MZKY_NCRC; entry++) {
+		if (megaz->stm_crc[entry].cdata)
+			status->crc_blocks++;
+		if (megaz->rtm_crc[entry].cdata)
+			status->crc_blocks++;
+	}
+}
+
+static int do_ioc_status(struct zoned *znd, unsigned long arg)
+{
+	void __user *parg = (void __user *)arg;
+	int error = -EFAULT;
+	struct zdm_ioc_request *req;
+	struct zdm_ioc_status *stats;
+
+	req = kzalloc(sizeof(*req), GFP_KERNEL);
+	stats = kzalloc(sizeof(*stats), GFP_KERNEL);
+
+	if (!req || !stats) {
+		error = -ENOMEM;
+		goto out;
+	}
+
+	if (copy_from_user(req, parg, sizeof(*req)))
+		goto out;
+
+	if (req->megazone_nr < znd->mega_zones_count) {
+		struct megazone *megaz = &znd->z_mega[req->megazone_nr];
+
+		if (req->result_size < sizeof(*stats)) {
+			error = -EBADTYPE;
+			goto out;
+		}
+		fill_ioc_status(megaz, stats);
+		if (copy_to_user(parg, stats, sizeof(*stats))) {
+			error = -EFAULT;
+			goto out;
+		}
+		error = 0;
+	}
+
+out:
+	kfree(req);
+	kfree(stats);
+	return error;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_ioctl(struct dm_target *ti, unsigned int cmd,
+		       unsigned long arg)
+{
+	int rcode = 0;
+	struct zoned *znd = (struct zoned *) ti->private;
+
+	switch (cmd) {
+	case ZDM_IOC_MZCOUNT:
+		rcode = znd->mega_zones_count;
+		break;
+	case ZDM_IOC_WPS:
+		do_ioc_wpstat(znd, arg, 1);
+		break;
+	case ZDM_IOC_FREE:
+		do_ioc_wpstat(znd, arg, 0);
+		break;
+	case ZDM_IOC_STATUS:
+		do_ioc_status(znd, arg);
+		break;
+	default:
+		rcode = zoned_ioctl_fwd(znd->dev, cmd, arg);
+		break;
+	}
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void start_worker(struct zoned *znd)
+{
+	clear_bit(ZF_FREEZE, &znd->flags);
+	atomic_set(&znd->suspended, 0);
+	mod_timer(&znd->timer, jiffies + msecs_to_jiffies(5000));
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void stop_worker(struct zoned *znd)
+{
+	set_bit(ZF_FREEZE, &znd->flags);
+	atomic_set(&znd->suspended, 1);
+	megazone_flush_all(znd);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_postsuspend(struct dm_target *ti)
+{
+	struct zoned *zoned = ti->private;
+
+	stop_worker(zoned);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_preresume(struct dm_target *ti)
+{
+	struct zoned *zoned = ti->private;
+
+	start_worker(zoned);
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct target_type zoned_target = {
+	.name = "zoned",
+	.module = THIS_MODULE,
+	.version = {1, 0, 0},
+	.ctr = zoned_constructor,
+	.dtr = zoned_dtr,
+	.map = zoned_map,
+
+	.postsuspend = zoned_postsuspend,
+	.preresume = zoned_preresume,
+	.status = zoned_status,
+		/*  .message = zoned_message, */
+	.ioctl = zoned_ioctl,
+
+	.iterate_devices = zoned_iterate_devices,
+	.merge = zoned_merge,
+	.io_hints = zoned_io_hints
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int __init dm_zoned_init(void)
+{
+	int rcode = dm_register_target(&zoned_target);
+
+	if (rcode)
+		DMERR("zoned target registration failed: %d", rcode);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static void __exit dm_zoned_exit(void)
+{
+	dm_unregister_target(&zoned_target);
+}
+
+module_init(dm_zoned_init);
+module_exit(dm_zoned_exit);
+
+MODULE_DESCRIPTION(DM_NAME " zoned target for Host Aware/Managed drives.");
+MODULE_AUTHOR("Shaun Tancheff <shaun.tancheff@seagate.com>");
+MODULE_LICENSE("GPL");
diff --git a/drivers/md/dm-zoned.h b/drivers/md/dm-zoned.h
new file mode 100644
index 0000000..44983af
--- /dev/null
+++ b/drivers/md/dm-zoned.h
@@ -0,0 +1,399 @@
+/*
+ * Kernel Device Mapper for abstracting ZAC/ZBC devices as normal
+ * block devices for linux file systems.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef _DM_ZONED_H
+#define _DM_ZONED_H
+
+#define ZDM_IOC_MZCOUNT 0x5a4e0001
+#define ZDM_IOC_WPS     0x5a4e0002
+#define ZDM_IOC_FREE    0x5a4e0003
+#define ZDM_IOC_STATUS  0x5a4e0004
+
+#define DM_MSG_PREFIX "zoned"
+
+#define ZDM_RESERVED_ZNR         0
+#define ZDM_CRC_STASH_ZNR        1 /* first 64 blocks */
+#define ZDM_REVERSE_MAP_ZNR      2
+#define ZDM_SECTOR_MAP_ZNR       3
+#define ZDM_DATA_START_ZNR       4
+
+#define Z_WP_GC_FULL            (1u << 31)
+#define Z_WP_GC_ACTIVE          (1u << 30)
+#define Z_WP_GC_TARGET          (1u << 29)
+#define Z_WP_GC_READY           (1u << 28)
+#define Z_WP_GC_BITS            (0xFu << 28)
+
+#define Z_WP_GC_PENDING         (Z_WP_GC_FULL|Z_WP_GC_ACTIVE)
+#define Z_WP_NON_SEQ            (1u << 27)
+#define Z_WP_RESV_01            (1u << 26)
+#define Z_WP_RESV_02            (1u << 25)
+#define Z_WP_RESV_03            (1u << 24)
+
+#define Z_WP_VALUE_MASK         (~0u >> 8)
+#define Z_WP_FLAGS_MASK         (~0u << 24)
+
+#define Z_AQ_GC                 (1u << 31)
+#define Z_AQ_META               (1u << 30)
+#define Z_AQ_NORMAL             (0)
+
+#define Z_C4K                   (4096ul)
+#define Z_UNSORTED              (Z_C4K / sizeof(struct map_sect_to_lba))
+#define Z_BLOCKS_PER_DM_SECTOR  (Z_C4K/512)
+#define MZ_METADATA_ZONES       (8ul)
+
+#define SUPERBLOCK_LOCATION     0
+#define SUPERBLOCK_MAGIC        0x5a6f4e65	/* ZoNe */
+#define SUPERBLOCK_CSUM_XOR     146538381
+#define MIN_ZONED_VERSION       1
+#define Z_VERSION               1
+#define MAX_ZONED_VERSION       1
+#define INVALID_WRITESET_ROOT   SUPERBLOCK_LOCATION
+
+#define UUID_LEN		16
+
+#define Z_TYPE_SMR		2
+#define Z_TYPE_SMR_HA		1
+#define Z_VPD_INFO_BYTE		8
+
+enum superblock_flags_t {
+	SB_DIRTY = 1,
+};
+
+struct z_io_req_t {
+	struct dm_io_region *where;
+	struct dm_io_request *io_req;
+	struct work_struct work;
+	int result;
+};
+
+#define Z_LOWER48 (~0ul >> 16)
+#define Z_UPPER16 (~Z_LOWER48)
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+enum mapped_flags_enum {
+	IS_DIRTY,
+	IS_GC,
+};
+
+enum work_flags_enum {
+	DO_JOURNAL_MOVE,
+	DO_MEMPOOL,
+	DO_SYNC,
+	DO_JOURNAL_LOAD,
+	DO_META_CHECK,
+	DO_GC_NO_PURGE,
+	DO_METAWORK_QD,
+};
+
+enum gc_flags_enum {
+	DO_GC_NEW,
+	DO_GC_PREPARE,		/* -> READ or COMPLETE state */
+	DO_GC_WRITE,
+	DO_GC_META,		/* -> PREPARE state */
+	DO_GC_COMPLETE,
+};
+
+enum znd_flags_enum {
+	ZF_FREEZE,
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct zoned;
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct gc_state {
+	struct megazone *megaz;
+	unsigned long gc_flags;
+
+	u32 r_ptr;
+	u32 w_ptr;
+
+	u32 nblks;		/* 1-65536 */
+	int result;
+
+	u16 z_gc;
+	u16 tag;
+
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct map_sect_to_lba {
+	__le64 logical;		/* record type [16 bits] + logical sector # */
+	__le64 physical;	/* csum 16 [16 bits] + 'physical' block lba */
+} __packed;
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct map_pg {
+	struct list_head inpool;
+
+	u64 age;		/* most recent access in jiffies */
+	u64 lba;		/* Z_LOWER48 contains the BLOCK where this
+				 * data originates from .. */
+	unsigned long flags;
+	struct mutex md_lock;	/* lock mdata i/o */
+	u32 *mdata;		/* 4k page of table entries */
+	atomic_t refcount;
+
+	u64 last_write;		/* last known position on disk */
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct map_addr {
+	u64 dm_s;		/* full map on dm layer         */
+	u64 z_id;		/* z_id match zone_list_t.z_id  */
+	u64 mz_off;		/* megazone offset              */
+	u64 mz_id;		/* mega zone #                  */
+	u64 offentry;		/* entry in lut (0-1023)        */
+	u64 lut_s;		/* sector table lba  */
+	u64 lut_r;		/* reverse table lba */
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct mzlam {
+	u64 mz_base;
+	u64 r_base;
+	u64 s_base;
+	u64 sk_low;
+	u64 sk_high;
+	u64 crc_low;
+	u64 crc_hi;
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct map_cache {
+	struct list_head jlist;
+	struct map_sect_to_lba *jdata;	/* 4k page of data */
+	atomic_t refcount;
+	struct mutex cached_lock;
+	unsigned long no_sort_flag;
+	u32 jcount;
+	u32 jsorted;
+	u32 jsize;
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct crc_pg {
+	u64 age;		/* most recent access in jiffies */
+	u64 lba;		/* logical home */
+	unsigned long flags;	/* IS_DIRTY flag */
+	atomic_t refcount;	/* REF count (move to flags?) */
+	u64 last_write;
+	struct mutex lock_pg;
+	u16 *cdata;		/* attached 4K page: [2048] entries */
+};
+
+#define MZKY_NBLKS  64
+#define MZKY_NCRC   32
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct zdm_superblock {
+	u8 uuid[UUID_LEN];	/* 16 */
+	__le64 nr_zones;	/*  8 */
+	__le64 magic;		/*  8 */
+	__le64 first_zone;	/*  8 */
+	__le32 version;		/*  4 */
+	__le32 packed_meta;	/*  4 */
+	__le32 flags;		/*  4 */
+	__le32 csum;		/*  4 */
+} __packed;			/* 56 */
+
+#define MAX_CACHE_SYNC 400
+
+/* stm_keys    -  512 - LBA64 for each key page of the Sector Table */
+/* stm_crc_lba -  256 - LBA64 for each CRC page */
+/* stm_crc_pg  -   64 - CRC16 for each CRC page */
+/* rtm_crc_lba -  256 - LBA64 for each CRC page */
+/* rtm_crc_pg  -   64 - CRC16 for each CRC page */
+/* crcs        -  816 - Testing worst case so far - 142 entries. */
+/* reserved    - 2040 */
+/* gc_resv     -    8 */
+/* meta_resv   -    8 */
+/* n_crcs;     -    2 */
+/* zp_crc;     -    2 */
+/* free_crc    -    2 */
+/* sblock;     -   56 */
+/* generation  -    8 */
+/* key_crc     -    2 */
+/* magic       -    8 */
+
+struct mz_superkey {
+	u64 sig[2];
+	u64 stm_keys[MZKY_NBLKS];
+	u64 stm_crc_lba[MZKY_NCRC];
+	u16 stm_crc_pg[MZKY_NCRC];
+	u64 rtm_crc_lba[MZKY_NCRC];
+	u16 rtm_crc_pg[MZKY_NCRC];
+	u16 crcs[MAX_CACHE_SYNC];
+	u16 reserved[1020];
+	u32 gc_resv;
+	u32 meta_resv;
+	u16 n_crcs;
+	u16 zp_crc;
+	u16 free_crc;
+	struct zdm_superblock sblock;
+	u64 generation;
+	u16 key_crc;
+	u64 magic;
+} __packed;
+
+struct mz_state {
+	struct mz_superkey  bmkeys;
+	u32        z_ptrs[1024];
+	u32             zfree[1024];
+} __packed;
+
+struct io_4k_block {
+	u8 data[Z_C4K];
+};
+
+struct io_dm_block {
+	u8 data[512];
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct megazone {
+	unsigned long flags;
+	struct zoned *znd;
+	struct list_head jlist;		/* journal */
+	struct list_head smtpool;	/* in-use sm table  entries */
+	struct map_pg **sectortm;
+	struct map_pg **reversetm;
+	struct mz_state *sync_io;
+	u32 *z_ptrs;
+	u32 *zfree_count;
+	u32  z_commit[1024];
+	struct mz_superkey *bmkeys;
+	struct mzlam       logical_map;
+	struct crc_pg stm_crc[MZKY_NCRC];
+	struct crc_pg rtm_crc[MZKY_NCRC];
+	struct work_struct meta_work;
+	sector_t last_w;
+	u8 *cow_block;
+	u64 cow_addr;
+	struct mutex mapkey_lock;	/* for normal i/o */
+	struct mutex mz_io_mutex;	/* for normal i/o */
+	struct mutex zp_lock;		/* general lock (block acquire)  */
+	spinlock_t jlock;		/* journal lock */
+	spinlock_t map_pool_lock;	/* smtpool: memory pool lock */
+	struct mutex discard_lock;
+	u64 age;			/* most recent access in jiffies */
+	u32 mega_nr;
+	u32 z_count;			/* megazone data span: 4-1024 */
+	u32 z_gc_free;			/* current empty zone count */
+	u32 z_data;			/* Range: 2->1023 */
+	u32 z_current;			/* Range: 2->1023 */
+	u32 z_gc_resv;
+	u32 z_meta_resv;
+	s32 incore_count;
+	u32 discard_count;
+	int mc_entries;
+	int meta_result;
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct zoned {
+	struct dm_target *ti;
+	struct dm_target_callbacks callbacks;
+	struct dm_dev *dev;
+	u64 first_zone;	/* delta from lba 0 .. TBD use report_zones IOCTL */
+	unsigned long flags;
+
+	/* background activity: */
+	struct work_struct bg_work;
+	struct workqueue_struct *bg_wq;
+	spinlock_t stats_lock;
+
+	/* zoned gc: */
+	struct gc_state *gc_active;
+	spinlock_t gc_lock;
+	struct delayed_work gc_work;
+	struct workqueue_struct *gc_wq;
+	int gc_backlog;
+	void *gc_io_buf;
+	struct io_4k_block *sync_cache[32];
+	unsigned long sync_cache_flags;
+
+	/* superblock: */
+	void *z_superblock;
+	struct zdm_superblock *super_block;
+
+	/* array of mega-zones: */
+	struct megazone *z_mega;
+	struct workqueue_struct *meta_wq;
+
+	u64 device_zone_count;	/* zones on device */
+	u64 mega_zones_count;	/* # of 256G mega-zones */
+	u64 nr_blocks;		/* 4k blocks on backing device */
+
+	struct map_cache gc_postmap;
+
+	struct dm_io_client *io_client;
+	struct workqueue_struct *io_wq;
+	struct timer_list timer;
+
+	u32 bins[40];
+	char bdev_name[BDEVNAME_SIZE];
+
+	size_t memstat;
+	atomic_t suspended;
+	u16 gc_mz_pref;
+	u16 mz_provision;
+	u8 zinqtype;
+	u8 ata_passthrough;
+	u8 is_empty; /* for fast discards on initial format */
+	u8 gc_throttle;
+};
+
+struct zdm_ioc_request {
+	u32 result_size;
+	u32 megazone_nr;
+};
+
+struct zdm_ioc_status {
+	u64 b_used;
+	u64 b_available;
+	u64 b_discard;
+	u64 m_used;
+	u64 mc_entries;
+	u64 mlut_blocks;
+	u64 crc_blocks;
+	u64 inpool;
+	u32 bins[40];
+};
+
+#endif /* _DM_ZONED_H */
diff --git a/drivers/md/libzoned.c b/drivers/md/libzoned.c
new file mode 100644
index 0000000..b464daf
--- /dev/null
+++ b/drivers/md/libzoned.c
@@ -0,0 +1,5127 @@
+/*
+ * Kernel Device Mapper for abstracting ZAC/ZBC devices as normal
+ * block devices for linux file systems.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#define BUILD_NO                91
+
+#define EXTRA_DEBUG              0
+
+#define MZ_MEMPOOL_SZ          128
+#define JOURNAL_MEMCACHE_BLOCKS  4
+#define MEM_PURGE_MSECS       2500
+
+/* acceptable 'free' count for MZ free levels */
+#define GC_PRIO_DEFAULT     0xFF00
+#define GC_PRIO_LOW         0x7FFF
+#define GC_PRIO_HIGH        0x0400
+#define GC_PRIO_CRIT        0x0040
+
+/* When less than 20 zones are free use aggressive gc in the megazone */
+#define GC_COMPACT_AGGRESSIVE   32
+
+/*
+ *  For performance tuning:
+ *   Q? smaller strips give smoother performance
+ *      a single drive I/O is 8 (or 32?) blocks?
+ *   A? Does not seem to ...
+ */
+#define GC_MAX_STRIPE          256
+#define REPORT_BUFFER           65 /* 65 -> min # pages for 4096 descriptors */
+#define SYNC_CACHE_ORDER         8
+#define SYNC_CACHE_PAGES        (1 << SYNC_CACHE_ORDER)
+#define SYNC_IO_ORDER            2
+#define SYNC_IO_SZ             ((1 << SYNC_IO_ORDER) * PAGE_SIZE)
+
+#define MZTEV_UNUSED    0xFFFFFFFFu
+#define MZTEV_NF        0x80000000u
+#define MZTEV_MAX       0x03ffFFFFu
+
+#define Z_TABLE_MAGIC  0x123456787654321Eul
+#define Z_KEY_SIG      0xFEDCBA987654321Ful
+
+#define Z_CRC_4K            4096
+#define Z_BLKSZ          0x10000
+#define MAX_ZONES_PER_MZ    1024
+#define Z_SMR_SZ_BYTES   (Z_BLKSZ*Z_C4K)
+
+#define GC_READ          (1ul << 15)
+
+/**
+ * 16 bit crc
+ */
+static inline u16 crc16_md(void const *data, size_t len)
+{
+	const u16 init = 0xFFFF;
+	const u8 *p = data;
+
+	return crc16(init, p, len);
+}
+
+/**
+ * 16 bit CRC converted to little endian
+ */
+static inline u16 crc_md_le16(void const *data, size_t len)
+{
+	u16 crc = crc16_md(data, len);
+
+	return cpu_to_le16(crc);
+}
+
+/**
+ * 32 bit CRC [NOTE: 32c is HW assisted on Intel]
+ */
+static inline u32 crcpg(void *data)
+{
+	return crc32c(~0u, data, Z_CRC_4K) ^ SUPERBLOCK_CSUM_XOR;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u64 le64_to_lba48(u64 enc, u16 *flg)
+{
+	const u64 lba64 = le64_to_cpu(enc);
+
+	if (flg)
+		*flg = (lba64 >> 48) & 0xFFFF;
+
+	return lba64 & Z_LOWER48;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u64 lba48_to_le64(u16 flags, u64 lba48)
+{
+	u64 high_bits = flags;
+
+	return cpu_to_le64((high_bits << 48) | (lba48 & Z_LOWER48));
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int sb_test_flag(struct zdm_superblock *sb, int bit_no)
+{
+	u32 flags = le32_to_cpu(sb->flags);
+
+	return (flags & (1 << bit_no)) ? 1 : 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void sb_set_flag(struct zdm_superblock *sb, int bit_no)
+{
+	u32 flags = le32_to_cpu(sb->flags);
+
+	flags |= (1 << bit_no);
+	sb->flags = cpu_to_le32(flags);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u16 _calc_zone(u64 lba)
+{
+	return (lba >> 16) & 0x3FF;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_reverse_table_zone(struct megazone *megaz,
+					struct map_addr *maddr)
+{
+	int is_rtm = 0;
+	int zoff = maddr->z_id % 1024;
+	int rtm_zone = ZDM_REVERSE_MAP_ZNR;	/* Z: #2 */
+
+	if (zoff == rtm_zone)
+		is_rtm = 1;
+
+	return is_rtm;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void megazone_fill_lam(struct megazone *megaz, struct mzlam *lam)
+{
+	lam->mz_base = (megaz->mega_nr * 1024);
+	lam->r_base  = (lam->mz_base + ZDM_REVERSE_MAP_ZNR) * Z_BLKSZ;
+	lam->s_base  = (lam->mz_base + ZDM_SECTOR_MAP_ZNR)  * Z_BLKSZ;
+	lam->crc_low = (lam->mz_base + ZDM_CRC_STASH_ZNR)   * Z_BLKSZ;
+	lam->crc_hi  =  lam->crc_low + (MZKY_NCRC * 2);
+	lam->sk_low  =  lam->s_base  + (ZDM_SECTOR_MAP_ZNR * MZKY_NBLKS);
+	lam->sk_high =  lam->sk_low  +  MZKY_NBLKS;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_ready_for_gc(struct megazone *megaz, u16 z_id)
+{
+	if ((megaz->z_ptrs[z_id] & Z_WP_GC_BITS) == Z_WP_GC_READY)
+		if (Z_BLKSZ == (megaz->z_ptrs[z_id] & Z_WP_VALUE_MASK))
+			return 1;
+	return 0;
+}
+
+/**
+ * Convert an upper layer sector number to the locally managed
+ * data sector number
+ */
+static int map_addr_to_zdm(struct zoned *, u64 sector_nr, struct map_addr *out);
+
+/**
+ * Locate meta data for locally managed sector.
+ */
+static int map_addr_calc(u64 dm_s, struct map_addr *out);
+
+/**
+ * Megazone:
+ */
+static int megazone_init(struct zoned *znd);
+static void megazone_destroy(struct zoned *znd);
+static void megazone_flush_all(struct zoned *znd);
+static void megazone_free_all(struct zoned *znd);
+static int megazone_wp_sync(struct zoned *znd, int reset_non_empty);
+static int write_if_dirty(struct megazone *, struct map_pg *, int use_wq);
+static void gc_work_task(struct work_struct *work);
+static void meta_work_task(struct work_struct *work);
+static u64 mcache_greatest_gen(struct megazone *, int, u64 *, u64 *);
+static u64 mcache_find_gen(struct megazone *, u64 base, int, u64 *out);
+static int find_superblock(struct megazone *megaz, int use_wq, int do_init);
+static int do_sync_tables(struct megazone *megaz, int need_table_push);
+static int sync_crc_pages(struct megazone *megaz);
+static int unused_phy(struct megazone *megaz, u64 lba, u64 orig_s);
+static struct io_4k_block *get_sync_cache(struct megazone *megaz);
+static int put_sync_cache(struct megazone *megaz, struct io_4k_block *cache);
+static int move_to_map_tables(struct megazone *megaz, struct map_cache *jrnl);
+static int load_page(struct megazone *, struct map_pg *, u64 lba, int);
+static struct map_pg *get_map_entry(struct megazone *, struct map_addr *, int);
+static struct map_pg *sector_map_entry(struct megazone *, struct map_addr *);
+static struct map_pg *reverse_map_entry(struct megazone *, struct map_addr *);
+static u64 locate_sector(struct megazone *megaz, struct map_addr *maddr);
+static int load_crc_meta_pg(struct megazone *, struct crc_pg *, u64, u16, int);
+static struct map_pg *get_map_table_entry(struct megazone *megaz, u64 lba, int);
+static int map_entry_page(struct megazone *, struct map_pg *, u64, int);
+static int zoned_init(struct dm_target *ti, struct zoned *znd);
+static int fpages(struct megazone *megaz, int allowed_pages);
+static int zoned_create_disk(struct dm_target *ti, struct zoned *znd);
+static sector_t jentry_value(struct map_sect_to_lba *e, bool is_block);
+static u64 z_lookup_cache(struct megazone *megaz, struct map_addr *sm);
+static u64 z_lookup(struct megazone *megaz, struct map_addr *sm);
+static int z_mapped_add_one(struct megazone *megaz, u64 dm_s, u64 lba);
+static int z_mapped_discard(struct megazone *megaz, u64 dm_s, u64 lba);
+static int z_mapped_addmany(struct megazone *megaz, u64 dm_s, u64 lba, u64);
+static int z_mapped_to_list(struct megazone *megaz, u64 dm_s, u64 lba);
+static int z_mapped_sync(struct megazone *megaz);
+static int z_mapped_init(struct megazone *megaz);
+static u64 z_acquire(struct megazone *megaz, u32 flags, u32 nblks, u32 *nfound);
+static u32 sb_crc32(struct zdm_superblock *sblock);
+static struct crc_pg *get_meta_pg_crc(struct megazone *, struct map_addr *,
+				      int, int);
+static int update_map_entry(struct megazone *, struct map_pg *,
+			    struct map_addr *, u64, int);
+static int read_block(struct dm_target *, enum dm_io_mem_type,
+		      void *, u64, unsigned int, int);
+static int write_block(struct dm_target *, enum dm_io_mem_type,
+		       void *, u64, unsigned int, int);
+static int zoned_init_disk(struct dm_target *ti, struct zoned *znd,
+			   int create, int check, int force);
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ *  generic-ish n-way alloc/free
+ *  Use kmalloc for small (< 4k) allocations.
+ *  Use vmalloc for multi-page alloctions
+ *  Except:
+ *  Use multipage allocations for dm_io'd pages that a frequently hit.
+ *
+ *  NOTE: ALL allocations are zero'd before returning.
+ *        alloc/free count is tracked for dynamic analysis.
+ */
+
+#define GET_PG_SYNC   0x010000
+#define GET_PG_CACHE  0x020000
+#define GET_ZPG       0x040000
+#define GET_KM        0x080000
+#define GET_VM        0x100000
+
+#define MP_SIO   (GET_PG_SYNC | 23)
+#define MP_CACHE (GET_PG_CACHE | 24)
+
+#define PG_01    (GET_ZPG |  1)
+#define PG_02    (GET_ZPG |  2)
+#define PG_05    (GET_ZPG |  5)
+#define PG_06    (GET_ZPG |  6)
+#define PG_08    (GET_ZPG |  8)
+#define PG_09    (GET_ZPG |  9)
+#define PG_10    (GET_ZPG | 10)
+#define PG_11    (GET_ZPG | 11)
+#define PG_13    (GET_ZPG | 13)
+#define PG_17    (GET_ZPG | 17)
+#define PG_27    (GET_ZPG | 27)
+
+#define KM_00    (GET_KM  |  0)
+#define KM_07    (GET_KM  |  7)
+#define KM_14    (GET_KM  | 14)
+#define KM_15    (GET_KM  | 15)
+#define KM_16    (GET_KM  | 16)
+#define KM_18    (GET_KM  | 18)
+#define KM_20    (GET_KM  | 20)
+#define KM_25    (GET_KM  | 25)
+#define KM_26    (GET_KM  | 26)
+#define KM_28    (GET_KM  | 28)
+#define KM_29    (GET_KM  | 29)
+#define KM_30    (GET_KM  | 30)
+
+#define VM_03    (GET_VM  |  3)
+#define VM_04    (GET_VM  |  4)
+#define VM_12    (GET_VM  | 12)
+#define VM_21    (GET_VM  | 21)
+#define VM_22    (GET_VM  | 22)
+
+#define ZDM_FREE(z, _p, sz, id) \
+	do { zdm_free((z), (_p), (sz), (id)); (_p) = 0; } while (0)
+
+#define ZDM_ALLOC(z, sz, id)       zdm_alloc((z), (sz), (id))
+#define ZDM_CALLOC(z, n, sz, id)   zdm_calloc((z), (n), (sz), (id))
+
+/**
+ * Unified free by 'code':
+ *
+ * This (ugly) unified scheme helps to find leaks and monitor usage
+ *   via ioctl tools.
+ */
+static void zdm_free(struct zoned *znd, void *p, size_t sz, u32 code)
+{
+	int id    = code & 0x00FFFF;
+	int flag  = code & 0xFF0000;
+
+	if (p) {
+		if (znd) {
+			spin_lock(&znd->stats_lock);
+			if (sz > znd->memstat)
+				Z_ERR(znd,
+				      "Free'd more mem than allocated? %d", id);
+
+			if (sz > znd->bins[id]) {
+				Z_ERR(znd,
+				      "Free'd more mem than allocated? %d", id);
+				dump_stack();
+			}
+			znd->memstat -= sz;
+			znd->bins[id] -= sz;
+			spin_unlock(&znd->stats_lock);
+		}
+
+		switch (flag) {
+		case GET_PG_SYNC:
+			free_pages((unsigned long)p, SYNC_IO_ORDER);
+			break;
+		case GET_PG_CACHE:
+			free_pages((unsigned long)p, SYNC_CACHE_ORDER);
+			break;
+		case GET_ZPG:
+			free_page((unsigned long)p);
+			break;
+		case GET_KM:
+			kfree(p);
+			break;
+		case GET_VM:
+			vfree(p);
+			break;
+		default:
+			Z_ERR(znd,
+			      "zdm_free %p scheme %x not mapped.", p, code);
+			break;
+		}
+
+	} else {
+		Z_ERR(znd, "double zdm_free %p [%d]", p, id);
+		BUG_ON(p);
+	}
+}
+
+/**
+ * Unified alloc by 'code':
+ *
+ * There a few things (like dm_io) that seem to need pages and not just
+ *   kmalloc'd memory.
+ *
+ * This (ugly) unified scheme helps to find leaks and monitor usage
+ *   via ioctl tools.
+ */
+static void *zdm_alloc(struct zoned *znd, size_t sz, int code)
+{
+	void *pmem = NULL;
+	int id    = code & 0x00FFFF;
+	int flag  = code & 0xFF0000;
+
+	switch (flag) {
+	case GET_PG_SYNC:
+		pmem = (void *)__get_free_pages(GFP_KERNEL, SYNC_IO_ORDER);
+		if (pmem)
+			memset(pmem, 0, sz);
+		break;
+	case GET_PG_CACHE:
+		pmem = (void *)__get_free_pages(GFP_KERNEL, SYNC_CACHE_ORDER);
+		if (pmem)
+			memset(pmem, 0, sz);
+		break;
+	case GET_ZPG:
+		pmem = (void *)get_zeroed_page(GFP_KERNEL);
+		break;
+	case GET_KM:
+		pmem = kzalloc(sz, GFP_KERNEL);
+		break;
+	case GET_VM:
+		pmem = vzalloc(sz);
+		break;
+	default:
+		Z_ERR(znd, "zdm alloc scheme for %u unknown.", code);
+		break;
+	}
+	if (!pmem) {
+		Z_ERR(znd, "Out of memory. %d", id);
+		dump_stack();
+	}
+	if (znd && pmem) {
+		spin_lock(&znd->stats_lock);
+		znd->memstat += sz;
+		znd->bins[id] += sz;
+		spin_unlock(&znd->stats_lock);
+	}
+	return pmem;
+}
+
+/**
+ * calloc is just an zeroed memory array alloc.
+ * all zdm_alloc schemes are for zeroed memory so no extra memset needed.
+ */
+static inline void *zdm_calloc(struct zoned *znd, size_t n, size_t sz, int code)
+{
+	return zdm_alloc(znd, sz * n, code);
+}
+
+/* -------------------------------------------------------------------------- */
+/**
+ *  Bump item to the top of the 'in-use' list.
+ *  The list is culled from the end to keep memory usage sane.
+ */
+static inline void incore_hint(struct megazone *megaz, struct list_head *head,
+			       struct list_head *item)
+{
+	spin_lock(&megaz->map_pool_lock);
+	if (head->next != item)
+		list_move(item, head);
+	spin_unlock(&megaz->map_pool_lock);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct io_4k_block *get_sync_cache(struct megazone *megaz)
+{
+	struct zoned *znd = megaz->znd;
+	struct io_4k_block *cache = NULL;
+	int avail;
+
+	for (avail = 0; avail < ARRAY_SIZE(znd->sync_cache); avail++) {
+		if (0 == test_and_set_bit(avail, &znd->sync_cache_flags)) {
+			cache = znd->sync_cache[avail];
+			if (!cache)
+				znd->sync_cache[avail] = cache =
+					ZDM_CALLOC(znd, SYNC_CACHE_PAGES,
+						sizeof(*cache), VM_12);
+			if (cache)
+				break;
+		}
+	}
+	return cache;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int put_sync_cache(struct megazone *megaz, struct io_4k_block *cache)
+{
+	struct zoned *znd = megaz->znd;
+	int err = -ENOENT;
+	int avail;
+
+	for (avail = 0; avail < ARRAY_SIZE(znd->sync_cache); avail++) {
+		if (cache == znd->sync_cache[avail]) {
+			BUG_ON(!test_and_clear_bit(avail,
+				&znd->sync_cache_flags));
+			err = 0;
+			break;
+		}
+	}
+	return err;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/**
+ * spin_lock wrapper for struct map_pg data
+ */
+static inline void mapped_lock(struct map_pg *mapped)
+{
+	mutex_lock(&mapped->md_lock);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * spin_unlock wrapper for struct map_pg data
+ */
+static inline void mapped_unlock(struct map_pg *mapped)
+{
+	mutex_unlock(&mapped->md_lock);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  translate a lookup table entry to a Sector #, or LBA
+ */
+static inline u64 map_value(struct megazone *megaz, u32 delta)
+{
+	u64 old_phy = 0;
+
+	if ((delta != MZTEV_UNUSED) && (delta != MZTEV_NF))
+		old_phy = (megaz->mega_nr * 1024 * Z_BLKSZ) + delta;
+
+	return old_phy;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  Warn if a give LBA is not valid (Esp if beyond a WP)
+ */
+static inline int warn_bad_lba(struct megazone *megaz, u64 lba48)
+{
+#define FMT_ERR "LBA %" PRIx64 " is not valid: MZ# %u, off:%x wp:%x"
+	int rcode = 0;
+	u64 zone  = lba48 / Z_BLKSZ;
+	u64 mz_nr = zone / 1024;
+
+	if (mz_nr == megaz->mega_nr) {
+		u32 wp_at;
+		u32 off = lba48 & 0xFFFF;
+
+		zone %= 1024;
+		if (zone < megaz->z_count) {
+			wp_at = megaz->z_ptrs[zone] & Z_WP_VALUE_MASK;
+			if (off >= wp_at) {
+				rcode = 1;
+				Z_ERR(megaz->znd, FMT_ERR,
+					lba48, megaz->mega_nr, off, wp_at);
+				dump_stack();
+			}
+		} else {
+			rcode = 1;
+			Z_ERR(megaz->znd, "LBA is not valid - Z: %" PRIu64
+				" count %u", zone, megaz->z_count);
+		}
+	} else {
+		rcode = 1;
+		Z_ERR(megaz->znd, "Lut %" PRIx64 " is not in MZ %u (got %"
+			PRIu64 ")!!!", lba48, megaz->mega_nr, mz_nr);
+	}
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  Encode a Sector # or LBA to a lookup table entry value.
+ */
+static int map_encode(struct megazone *megaz, u64 to_addr, u32 *value)
+{
+	int err = 0;
+	u64 mz_lba = (megaz->mega_nr * 1024 * Z_BLKSZ);
+
+	*value = (u32) MZTEV_UNUSED;
+	if (~0ul != to_addr) {
+		u64 physical = to_addr - mz_lba;
+		*value = (u32) (physical & MZTEV_MAX);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  Teardown a zoned device mapper instance.
+ */
+static void zoned_destroy(struct zoned *znd)
+{
+	int purge;
+	size_t sz = SYNC_CACHE_PAGES * sizeof(struct io_4k_block *);
+
+	if (znd->io_client)
+		dm_io_client_destroy(znd->io_client);
+
+	del_timer_sync(&znd->timer);
+
+	if (znd->io_wq) {
+		destroy_workqueue(znd->io_wq);
+		znd->io_wq = NULL;
+	}
+	if (znd->meta_wq) {
+		destroy_workqueue(znd->meta_wq);
+		znd->meta_wq = NULL;
+	}
+	if (znd->gc_wq) {
+		destroy_workqueue(znd->gc_wq);
+		znd->gc_wq = NULL;
+	}
+	if (znd->bg_wq) {
+		destroy_workqueue(znd->bg_wq);
+		znd->bg_wq = NULL;
+	}
+	if (znd->dev) {
+		dm_put_device(znd->ti, znd->dev);
+		znd->dev = NULL;
+	}
+	for (purge = 0; purge < ARRAY_SIZE(znd->sync_cache); purge++) {
+		if (znd->sync_cache[purge]) {
+			if (test_and_clear_bit(purge, &znd->sync_cache_flags))
+				Z_ERR(znd, "sync cache entry %d still in use!",
+				      purge);
+			ZDM_FREE(znd, znd->sync_cache[purge], sz, VM_12);
+		}
+	}
+	if (znd->z_superblock)
+		ZDM_FREE(znd, znd->z_superblock, Z_C4K, PG_05);
+	if (znd->gc_io_buf)
+		ZDM_FREE(znd, znd->gc_io_buf, GC_MAX_STRIPE * Z_C4K, VM_04);
+
+	if (znd->gc_postmap.jdata) {
+		size_t sz = Z_BLKSZ * sizeof(*znd->gc_postmap.jdata);
+
+		ZDM_FREE(znd, znd->gc_postmap.jdata, sz, VM_03);
+	}
+	ZDM_FREE(NULL, znd, sizeof(*znd), KM_00);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * Initialize a zoned device mapper instance
+ *
+ * Setup the zone pointer table and do a one time calculation of some
+ * basic limits
+ */
+static int zoned_init(struct dm_target *ti, struct zoned *znd)
+{
+	u64 size = i_size_read(get_bdev_bd_inode(znd));
+	u64 bdev_nr_sect4k = size / Z_C4K;
+	u64 device_zone_count = bdev_nr_sect4k / Z_BLKSZ;
+	u64 mzcount = dm_div_up(device_zone_count, MAX_ZONES_PER_MZ);
+	u64 remainder = device_zone_count % MAX_ZONES_PER_MZ;
+	u64 lut_resv;
+
+	if (0 < remainder && remainder < 5) {
+		Z_ERR(znd, "Final MZ contains too few zones!");
+		mzcount--;
+	}
+
+	znd->device_zone_count = device_zone_count;
+	znd->mega_zones_count = mzcount;
+	lut_resv = (znd->mega_zones_count * znd->mz_provision);
+
+	Z_DBG(znd, "%s: size:%" PRIu64 " zones: %" PRIu64 ", megas %" PRIu64
+		 ", resvd %" PRIu64 " %d", __func__, size,
+		 znd->device_zone_count, znd->mega_zones_count,
+		 lut_resv, __LINE__);
+
+	spin_lock_init(&znd->gc_lock);
+	spin_lock_init(&znd->stats_lock);
+
+	znd->gc_postmap.jdata =
+		ZDM_CALLOC(znd, Z_BLKSZ, sizeof(*znd->gc_postmap.jdata), VM_03);
+	if (!znd->gc_postmap.jdata) {
+		ti->error = "Could not create gc_postmap array";
+		return -ENOMEM;
+	}
+	znd->gc_postmap.jsize = Z_BLKSZ;
+	mutex_init(&znd->gc_postmap.cached_lock);
+
+	znd->io_client = dm_io_client_create();
+	if (!znd->io_client)
+		return -ENOMEM;
+
+	znd->meta_wq = create_singlethread_workqueue("znd_meta_wq");
+	if (!znd->meta_wq) {
+		ti->error = "couldn't start header metadata update thread";
+		return -ENOMEM;
+	}
+
+	znd->gc_wq = create_singlethread_workqueue("znd_gc_wq");
+	if (!znd->gc_wq) {
+		ti->error = "couldn't start GC workqueue.";
+		return -ENOMEM;
+	}
+	INIT_DELAYED_WORK(&znd->gc_work, gc_work_task);
+
+	znd->bg_wq = create_singlethread_workqueue("znd_bg_wq");
+	if (!znd->bg_wq) {
+		ti->error = "couldn't start background workqueue.";
+		return -ENOMEM;
+	}
+	INIT_WORK(&znd->bg_work, bg_work_task);
+
+	setup_timer(&znd->timer, activity_timeout, (unsigned long)znd);
+
+	znd->gc_io_buf = ZDM_CALLOC(znd, GC_MAX_STRIPE, Z_C4K, VM_04);
+	if (!znd->gc_io_buf) {
+		ti->error = "couldn't gc io buffer";
+		return -ENOMEM;
+	}
+
+	znd->io_wq = create_singlethread_workqueue("kzoned_dm_io_wq");
+	if (!znd->io_wq) {
+		ti->error = "couldn't start header metadata update thread";
+		return -ENOMEM;
+	}
+	znd->z_superblock = ZDM_ALLOC(znd, Z_C4K, PG_05);
+	if (!znd->z_superblock) {
+		ti->error = "couldn't allocate in-memory superblock";
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Metadata of zoned device mapper (for future backward compatibility)
+ */
+static int check_metadata_version(struct zdm_superblock *sblock)
+{
+	u32 metadata_version = le32_to_cpu(sblock->version);
+
+	if (metadata_version < MIN_ZONED_VERSION
+	    || metadata_version > MAX_ZONED_VERSION) {
+		DMERR("Unsupported metadata version %u found.",
+		      metadata_version);
+		DMERR("Only versions between %u and %u supported.",
+		      MIN_ZONED_VERSION, MAX_ZONED_VERSION);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * CRC check for superblock.
+ */
+static u32 sb_crc32(struct zdm_superblock *sblock)
+{
+	const u32 was = sblock->csum;
+	u32 crc;
+
+	sblock->csum = 0;
+	crc = crc32c(~(u32) 0u, sblock, sizeof(*sblock)) ^ SUPERBLOCK_CSUM_XOR;
+
+	sblock->csum = was;
+	return cpu_to_le32(crc);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Check the superblock to see if it is valid and not corrupt.
+ */
+static int sb_check(struct zdm_superblock *sblock)
+{
+	__le32 csum_le;
+
+	if (le64_to_cpu(sblock->magic) != SUPERBLOCK_MAGIC) {
+		DMERR("sb_check failed: magic %x: wanted %x",
+		      le32_to_cpu(sblock->magic), SUPERBLOCK_MAGIC);
+		return -EILSEQ;
+	}
+
+	csum_le = sb_crc32(sblock);
+	if (csum_le != sblock->csum) {
+		DMERR("sb_check failed: csum %u: wanted %u",
+		      csum_le, sblock->csum);
+		return -EILSEQ;
+	}
+
+	return check_metadata_version(sblock);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Initialize the on-disk format of a zoned device mapper.
+ */
+static int zoned_create_disk(struct dm_target *ti, struct zoned *znd)
+{
+	const int reset_non_empty = 1;
+	struct zdm_superblock *sblock = znd->super_block;
+	int iter;
+	int locations = znd->mega_zones_count * 3;
+	u64 incr = 512ul;
+	u64 lba = 0;
+	void *data;
+	int err;
+
+	memset(sblock, 0, sizeof(*sblock));
+	generate_random_uuid(sblock->uuid);
+	sblock->magic = cpu_to_le64(SUPERBLOCK_MAGIC);
+	sblock->version = cpu_to_le32(Z_VERSION);
+	sblock->first_zone = cpu_to_le64(znd->first_zone);
+
+	err = megazone_wp_sync(znd, reset_non_empty);
+	if (err)
+		return err;
+
+	data = ZDM_ALLOC(znd, Z_C4K, PG_06);
+	if (!data)
+		return -ENOMEM;
+
+	if (znd->mega_zones_count >= 32)
+		Z_ERR(znd, "TODO: Use lba of mz #32. FIXME!!");
+
+	for (iter = 0; iter < locations; iter++) {
+		int rc = write_block(ti, DM_IO_KMEM, data, lba, 1, 1);
+
+		if (rc)
+			Z_ERR(znd, "%s: clear sb @ %" PRIx64
+			       " failed:  %d", __func__, lba, rc);
+		lba += incr;
+	}
+	ZDM_FREE(znd, data, Z_C4K, PG_06);
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Repair an otherwise good device mapper instance that was not cleanly removed.
+ */
+static int zoned_repair(struct zoned *znd)
+{
+	Z_INFO(znd, "Is Dirty .. zoned_repair consistency fixer TODO!!!.");
+	return -ENOMEM;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Locate the existing SB on disk and re-load or create the device-mapper
+ * instance based on the existing disk state.
+ */
+static int zoned_init_disk(struct dm_target *ti, struct zoned *znd,
+			   int create, int check, int force)
+{
+	struct mz_superkey *key_blk = znd->z_superblock;
+	struct megazone *megaz = &znd->z_mega[0];
+	int jinit = 1;
+	int n4kblks = 1;
+	int use_wq = 1;
+	int rc = 0;
+	u64 first_zone = znd->first_zone;
+
+	memset(key_blk, 0, sizeof(*key_blk));
+
+	if (create && force) {
+		Z_ERR(znd, "Force Creating a clean instance.");
+	} else if (find_superblock(megaz, use_wq, 1)) {
+		u64 sb_lba = 0;
+		u64 generation;
+
+		Z_INFO(znd, "Found existing superblock");
+		if (force) {
+			if (first_zone != znd->first_zone) {
+				Z_ERR(znd, "  (force) override first zone: %"
+				      PRIu64  " with %" PRIu64 "",
+				      znd->first_zone, first_zone);
+				jinit = 0;
+				znd->first_zone = first_zone;
+			}
+		}
+		generation = mcache_greatest_gen(megaz, use_wq, &sb_lba, NULL);
+		Z_DBG(znd, "Generation: %" PRIu64 " @ %" PRIx64,
+			generation, sb_lba);
+
+		rc = read_block(ti, DM_IO_KMEM, key_blk, sb_lba,
+				n4kblks, use_wq);
+		if (rc) {
+			ti->error = "Superblock read error.";
+			return rc;
+		}
+	}
+	znd->super_block = &key_blk->sblock;
+	rc = sb_check(znd->super_block);
+	if (rc) {
+		jinit = 0;
+		if (create) {
+			DMWARN("Check failed .. creating superblock.");
+			zoned_create_disk(ti, znd);
+			znd->super_block->nr_zones =
+			    le64_to_cpu(znd->device_zone_count);
+			DMWARN("in-memory superblock created.");
+			znd->is_empty = 1;
+		} else {
+			ti->error = "Superblock check failed.";
+			return rc;
+		}
+	}
+
+	if (sb_test_flag(znd->super_block, SB_DIRTY)) {
+		int repair_check = zoned_repair(znd);
+
+		if (!force) {
+			/* if repair failed -- don't load from disk */
+			if (repair_check)
+				jinit = 0;
+		} else if (repair_check && jinit) {
+			Z_ERR(znd, "repair failed, force enabled loading ...");
+		}
+	}
+
+	if (jinit) {
+		u32 iter;
+
+		Z_ERR(znd, "INIT: Reloading DM Zoned metadata from DISK");
+
+		znd->first_zone = le64_to_cpu(znd->super_block->first_zone);
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			struct megazone *megaz = &znd->z_mega[iter];
+
+			set_bit(DO_JOURNAL_LOAD, &megaz->flags);
+			queue_work(znd->meta_wq, &megaz->meta_work);
+		}
+		Z_ERR(znd, "Waiting for load to complete.");
+		flush_workqueue(znd->meta_wq);
+	}
+
+	Z_ERR(znd, "ZONED: Build No %d marking superblock dirty.", BUILD_NO);
+
+	/* write the 'dirty' flag back to disk. */
+	sb_set_flag(znd->super_block, SB_DIRTY);
+	znd->super_block->csum = sb_crc32(znd->super_block);
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline sector_t jentry_value(struct map_sect_to_lba *e, bool is_block)
+{
+	sector_t value = 0;
+
+	if (is_block)
+		value = le64_to_lba48(e->physical, NULL);
+	else
+		value = le64_to_lba48(e->logical, NULL);
+
+	return value;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int compare_logical_sectors(const void *x1, const void *x2)
+{
+	const struct map_sect_to_lba *r1 = x1;
+	const struct map_sect_to_lba *r2 = x2;
+	const u64 v1 = le64_to_lba48(r1->logical, NULL);
+	const u64 v2 = le64_to_lba48(r2->logical, NULL);
+
+	return (v1 < v2) ? -1 : ((v1 > v2) ? 1 : 0);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int __find_sector_entry_chunk(struct map_sect_to_lba *data,
+				     s32 count, sector_t find, bool is_block)
+{
+	int at = -1;
+	s32 first = 0;
+	s32 last = count - 1;
+	s32 middle = (first + last) / 2;
+
+	while ((-1 == at) && (first <= last)) {
+		sector_t logical = ~0ul;
+
+		if (0 <= middle && middle < count)
+			logical = jentry_value(&data[middle], is_block);
+
+		if (logical < find)
+			first = middle + 1;
+		else if (logical > find)
+			last = middle - 1;
+		else
+			at = middle;
+
+		middle = (first + last) / 2;
+	}
+	return at;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 z_lookup_key_range(struct megazone *megaz, struct map_addr *maddr)
+{
+	u64 found = 0ul;
+	struct mzlam *lam = &megaz->logical_map;
+
+	if ((lam->sk_low <= maddr->dm_s) && (maddr->dm_s < lam->sk_high)) {
+		int off = maddr->dm_s - lam->sk_low;
+
+		mutex_lock(&megaz->mapkey_lock);
+		found = le64_to_cpu(megaz->bmkeys->stm_keys[off]);
+		mutex_unlock(&megaz->mapkey_lock);
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_add_one(struct megazone *megaz, u64 dm_s, u64 lba)
+{
+	int err;
+	struct mzlam *lam = &megaz->logical_map;
+	u16 zone = _calc_zone(lba);
+
+	/*
+	 * location of the SLT key sectors need to be
+	 * stashed into the sector lookup table block map
+	 * Does dm_s point in the sector lookup table block map
+	 */
+	if ((lam->sk_low <= dm_s) && (dm_s < lam->sk_high)) {
+		int off = dm_s - lam->sk_low;
+
+		mutex_lock(&megaz->mapkey_lock);
+		megaz->bmkeys->stm_keys[off] = cpu_to_le64(lba);
+		mutex_unlock(&megaz->mapkey_lock);
+	}
+	do {
+		err = z_mapped_to_list(megaz, dm_s, lba);
+	} while (-EBUSY == err);
+
+	megaz->z_commit[zone]++;
+	if (megaz->z_commit[zone] == Z_BLKSZ) {
+		mutex_lock(&megaz->zp_lock);
+		megaz->z_ptrs[zone] |= Z_WP_GC_READY;
+		mutex_unlock(&megaz->zp_lock);
+	}
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_discard(struct megazone *megaz, u64 dm_s, u64 lba)
+{
+	int err;
+
+	do {
+		err = z_mapped_to_list(megaz, dm_s, 0ul);
+	} while (-EBUSY == err);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct map_cache *jalloc(struct zoned *znd)
+{
+	struct map_cache *jrnl_first;
+
+	jrnl_first = ZDM_ALLOC(znd, sizeof(*jrnl_first), KM_07);
+	if (jrnl_first) {
+		mutex_init(&jrnl_first->cached_lock);
+		jrnl_first->jcount = 0;
+		jrnl_first->jsorted = 0;
+		jrnl_first->jdata = ZDM_CALLOC(znd, Z_UNSORTED,
+			sizeof(*jrnl_first->jdata), PG_08);
+
+		if (jrnl_first->jdata) {
+			u64 logical = Z_LOWER48;
+			u64 physical = Z_LOWER48;
+
+			jrnl_first->jdata[0].logical = cpu_to_le64(logical);
+			jrnl_first->jdata[0].physical = cpu_to_le64(physical);
+			jrnl_first->jsize = Z_UNSORTED - 1;
+
+		} else {
+			Z_ERR(znd, "%s: in memory journal is out of space.",
+			      __func__);
+			ZDM_FREE(znd, jrnl_first, sizeof(*jrnl_first), KM_07);
+			jrnl_first = NULL;
+		}
+	}
+	return jrnl_first;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline struct map_cache *jfirst_entry(struct megazone *megaz)
+{
+	unsigned long flags;
+	struct list_head *_jhead;
+	struct map_cache *jrnl;
+
+	spin_lock_irqsave(&megaz->jlock, flags);
+	_jhead = &(megaz->jlist);
+	jrnl = list_first_entry_or_null(_jhead, typeof(*jrnl), jlist);
+	if (jrnl && (&jrnl->jlist != _jhead))
+		atomic_inc(&jrnl->refcount);
+	else
+		jrnl = NULL;
+
+	spin_unlock_irqrestore(&megaz->jlock, flags);
+
+	return jrnl;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void jlist_add(struct megazone *megaz, struct map_cache *jrnl)
+{
+	unsigned long flags;
+	struct list_head *_jhead;
+
+	spin_lock_irqsave(&megaz->jlock, flags);
+	_jhead = &(megaz->jlist);
+	list_add(&(jrnl->jlist), _jhead);
+	spin_unlock_irqrestore(&megaz->jlock, flags);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void jderef(struct megazone *megaz, struct map_cache *jrnl)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&megaz->jlock, flags);
+	atomic_dec(&jrnl->refcount);
+	spin_unlock_irqrestore(&megaz->jlock, flags);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline struct map_cache *jnext_entry(struct megazone *megaz,
+					    struct map_cache *jrnl)
+{
+	unsigned long flags;
+	struct list_head *_jhead;
+	struct map_cache *next;
+
+	spin_lock_irqsave(&megaz->jlock, flags);
+	_jhead = &(megaz->jlist);
+	next = list_next_entry(jrnl, jlist);
+	if (next && (&next->jlist != _jhead))
+		atomic_inc(&next->refcount);
+	else
+		next = NULL;
+	atomic_dec(&jrnl->refcount);
+	spin_unlock_irqrestore(&megaz->jlock, flags);
+
+	return next;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void jcache_sort_no_lock(struct map_cache *jrnl)
+{
+	if (jrnl->jsorted < jrnl->jcount) {
+		sort(&jrnl->jdata[1], jrnl->jcount,
+		     sizeof(*jrnl->jdata),
+		     compare_logical_sectors, NULL);
+		jrnl->jsorted = jrnl->jcount;
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void jcache_sort_locked(struct map_cache *jrnl)
+{
+	if (jrnl->jsorted < jrnl->jcount) {
+		mutex_lock_nested(&jrnl->cached_lock, SINGLE_DEPTH_NESTING);
+		jcache_sort_no_lock(jrnl);
+		mutex_unlock(&jrnl->cached_lock);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int jlinear_find(struct map_cache *jrnl, u64 dm_s)
+{
+	int at = -1;
+	int jentry;
+
+	for (jentry = jrnl->jcount; jentry > 0; jentry--) {
+		u64 logi = le64_to_lba48(jrnl->jdata[jentry].logical, NULL);
+
+		if (logi == dm_s) {
+			at = jentry - 1;
+			goto out;
+		}
+	}
+
+out:
+	return at;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 z_lookup_cache(struct megazone *megaz, struct map_addr *maddr)
+{
+	struct map_cache *jrnl;
+	u64 found = 0ul;
+
+	jrnl = jfirst_entry(megaz);
+	while (jrnl) {
+		int at;
+
+		if (jrnl->no_sort_flag) {
+			at = jlinear_find(jrnl, maddr->dm_s);
+		} else {
+			/* Possible dead-lock if unsorted: */
+			jcache_sort_locked(jrnl);
+			at = __find_sector_entry_chunk(
+				&jrnl->jdata[1], jrnl->jcount, maddr->dm_s, 0);
+		}
+		if (at != -1) {
+			struct map_sect_to_lba *data = &jrnl->jdata[at + 1];
+
+			found = le64_to_lba48(data->physical, NULL);
+		}
+		if (found) {
+			jderef(megaz, jrnl);
+			goto out;
+		}
+
+		jrnl = jnext_entry(megaz, jrnl);
+	}
+out:
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int lba_in_zone(struct map_cache *jrnl, u16 zone)
+{
+	int jentry;
+
+	if (zone >= MAX_ZONES_PER_MZ)
+		goto out;
+
+	for (jentry = jrnl->jcount; jentry > 0; jentry--) {
+		u64 lba = le64_to_lba48(jrnl->jdata[jentry].physical, NULL);
+
+		if (_calc_zone(lba) == zone)
+			return 1;
+	}
+out:
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_verify_cache(struct megazone *megaz, u16 zone)
+{
+	struct map_cache *jrnl = NULL;
+	int err = 0;
+
+	jrnl = jfirst_entry(megaz);
+	while (jrnl) {
+		mutex_lock(&jrnl->cached_lock);
+		if (lba_in_zone(jrnl, zone)) {
+			Z_ERR(megaz->znd,
+			      "GC: **ERR** %x LBA in cache <= Corrupt",
+			      zone);
+			err = 1;
+			megaz->meta_result = -ENOSPC;
+		}
+		mutex_unlock(&jrnl->cached_lock);
+		jrnl = jnext_entry(megaz, jrnl);
+	}
+	return err;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int _journal_blocks_to_table(struct megazone *megaz, u16 zone)
+{
+	struct map_cache *jrnl = NULL;
+	int err = 0;
+
+	jrnl = jfirst_entry(megaz);
+	while (jrnl) {
+		int try_free = 0;
+		struct map_cache *jskip;
+
+		mutex_lock(&jrnl->cached_lock);
+		if (jrnl->jcount == jrnl->jsize) {
+			jcache_sort_no_lock(jrnl);
+			err = move_to_map_tables(megaz, jrnl);
+			if (!err && 0 == jrnl->jcount)
+				try_free = 1;
+		} else {
+			if (lba_in_zone(jrnl, zone)) {
+				Z_ERR(megaz->znd,
+					"Moving %d Runts because z: %u",
+					jrnl->jcount, zone);
+
+				jcache_sort_no_lock(jrnl);
+				err = move_to_map_tables(megaz, jrnl);
+			}
+		}
+		mutex_unlock(&jrnl->cached_lock);
+
+		if (err) {
+			jderef(megaz, jrnl);
+			Z_ERR(megaz->znd, "%s: Sector map failed.", __func__);
+			goto out;
+		}
+
+		jskip = jnext_entry(megaz, jrnl);
+		if (try_free) {
+			if (0 == jrnl->refcount.counter) {
+				unsigned long flags;
+				size_t sz = Z_UNSORTED * sizeof(*jrnl->jdata);
+
+				spin_lock_irqsave(&megaz->jlock, flags);
+				list_del(&jrnl->jlist);
+				spin_unlock_irqrestore(&megaz->jlock, flags);
+
+				ZDM_FREE(megaz->znd, jrnl->jdata, sz, PG_08);
+				ZDM_FREE(megaz->znd, jrnl,
+					 sizeof(*jrnl), KM_07);
+				jrnl = NULL;
+
+				megaz->mc_entries--;
+			} else {
+				Z_ERR(megaz->znd,
+					"Journal still in use. Not freed");
+			}
+		}
+		jrnl = jskip;
+	}
+out:
+	return err;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_flush_bdev(struct zoned *znd)
+{
+	int err;
+	sector_t bi_done;
+
+	Z_ERR(znd, "%s: flush to backing device!", __func__);
+	err = blkdev_issue_flush(znd->dev->bdev, GFP_KERNEL, &bi_done);
+	if (err)
+		Z_ERR(znd, "%s: flush failing sector %lu!", __func__, bi_done);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_SYNC(struct megazone *megaz, int do_tables)
+{
+	int err = 0;
+
+	err = do_sync_tables(megaz, do_tables);
+	if (err) {
+		Z_ERR(megaz->znd, "Uh oh: do_sync_tables -> %d", err);
+		goto out;
+	}
+
+	err = sync_crc_pages(megaz);
+	if (err) {
+		Z_ERR(megaz->znd, "Uh oh. sync_crc_pages -> %d", err);
+		goto out;
+	}
+
+	err = z_mapped_sync(megaz);
+	if (err) {
+		Z_ERR(megaz->znd, "Uh oh. z_mapped_sync -> %d", err);
+		goto out;
+	}
+
+out:
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_init_from_journal(struct megazone *megaz)
+{
+	int err = 0;
+
+	if (test_and_clear_bit(DO_JOURNAL_LOAD, &megaz->flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = z_mapped_init(megaz);
+		mutex_unlock(&megaz->mz_io_mutex);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int mdtest(struct megazone *megaz, struct map_addr *maddr, u32 *data,
+		  int z_id, int is_to, int use_wq, u32 pg_no, u16 crc_pgv)
+{
+	int rcode = 0;
+	int crc_okay = 0;
+	u32 unused = 0;
+	int off;
+	u64 lba = z_lookup(megaz, maddr);
+
+	if (lba) {
+		const int count = 1;
+		int err;
+		u16 crc;
+
+		err = read_block(megaz->znd->ti, DM_IO_KMEM, data, lba,
+				 count, use_wq);
+		if (err) {
+			Z_ERR(megaz->znd, "Integrity ERR: %" PRIx64
+			      " on disk %" PRIx64 " read err %d",
+			      maddr->dm_s, lba, err);
+			rcode = err;
+			goto out;
+		}
+		crc = crc_md_le16(data, Z_CRC_4K);
+
+		if (crc == crc_pgv) {
+			crc_okay = 1;
+		} else {
+			Z_ERR(megaz->znd,
+			      "Integrity ERR: %04x != %04x at lba %"
+			      PRIx64 " lmap %" PRIx64,
+			      crc, crc_pgv, lba, maddr->dm_s);
+			rcode = -EIO;
+			goto out;
+		}
+	} else {
+		Z_ERR(megaz->znd,
+		      "MZ# %u LBA Not found for: 0x%" PRIx64 " is_to %d",
+		      megaz->mega_nr, maddr->dm_s, is_to);
+	}
+
+	if (is_to || (!crc_okay))
+		goto out;
+
+	for (off = 0; off < 1024; off++) {
+		u32 enc = data[off];
+		u64 ORlba = (megaz->mega_nr * Z_BLKSZ * 1024)
+			  + (z_id * Z_BLKSZ) + pg_no + off;
+
+		if (enc == MZTEV_UNUSED) {
+			unused++;
+		} else {
+			u64 dm_s = map_value(megaz, enc);
+
+			if (dm_s < megaz->znd->nr_blocks) {
+				struct map_pg *Smap;
+				struct map_addr Saddr;
+
+				map_addr_calc(dm_s, &Saddr);
+				Smap = sector_map_entry(megaz, &Saddr);
+				if (Smap && Smap->mdata)
+					Z_DBG(megaz->znd,
+					      "lba: %" PRIx64 " okay",
+					      ORlba);
+				else
+					Z_ERR(megaz->znd,
+					      "lba: %" PRIx64" ERROR",
+					      ORlba);
+
+			} else {
+				Z_ERR(megaz->znd,
+				      "Invalid rmap entry: %x.",
+				      enc);
+			}
+			BUG_ON(dm_s >= megaz->znd->nr_blocks);
+		}
+	}
+	rcode = unused;
+
+out:
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int meta_integrity_test(struct megazone *megaz)
+{
+	u64 entry;
+	int rc = 0;
+	u32 z_used = Z_BLKSZ;
+	u64 s_base = 0x20000 + (megaz->mega_nr * Z_BLKSZ * 1024);
+	u32 *data = ZDM_ALLOC(megaz->znd, Z_C4K, PG_09);
+
+	if (!data)
+		return -ENOMEM;
+
+	/* NOTE: R_lut followed by S_lut */
+	for (entry = 0; entry < 0x20000; entry++) {
+		const int use_wq = 0;
+		int z_id = entry / 64;
+		int is_to;
+		int crce;
+		u16 crc_pgv;
+		struct map_addr maddr;
+		struct crc_pg *pblock;
+
+		map_addr_calc(s_base + entry, &maddr);
+		crce = (maddr.mz_off & 0xFFFF) % 2048;
+		is_to = !is_reverse_table_zone(megaz, &maddr);
+		pblock = get_meta_pg_crc(megaz, &maddr, is_to, use_wq);
+		if (!pblock) {
+			Z_ERR(megaz->znd, "%s: Out of space for metadata?",
+				__func__);
+			return -ENOSPC;
+		}
+
+		if (!is_to && (0 == (entry % 64)))
+			z_used = Z_BLKSZ;
+
+		atomic_inc(&pblock->refcount);
+
+		mutex_lock(&pblock->lock_pg);
+		crc_pgv = pblock->cdata[crce];
+		mutex_unlock(&pblock->lock_pg);
+
+		if (crc_pgv) {
+			u32 pg_no = is_to ? 0 : ((entry % 64) * 1024);
+			int rcode;
+
+			rcode = mdtest(megaz, &maddr, data, z_id, is_to, use_wq,
+				       pg_no, crc_pgv);
+
+			if (rcode < 0)
+				rc = rcode;
+			else
+				z_used -= rcode;
+		}
+
+		atomic_dec(&pblock->refcount);
+
+		if (!is_to && 63 == (entry % 64)) {
+			if (0 == (megaz->z_ptrs[z_id] & Z_WP_FLAGS_MASK))
+				megaz->zfree_count[z_id] = Z_BLKSZ - z_used;
+		}
+	}
+
+
+	ZDM_FREE(megaz->znd, data, Z_C4K, PG_09);
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_meta_check(struct megazone *megaz)
+{
+	int err = 0;
+
+	if (test_and_clear_bit(DO_META_CHECK, &megaz->flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = meta_integrity_test(megaz);
+		mutex_unlock(&megaz->mz_io_mutex);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_journal_to_table(struct megazone *megaz)
+{
+	int err = 0;
+
+	if (test_and_clear_bit(DO_JOURNAL_MOVE, &megaz->flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = _journal_blocks_to_table(megaz, MAX_ZONES_PER_MZ);
+		mutex_unlock(&megaz->mz_io_mutex);
+	}
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_free_pages(struct megazone *megaz)
+{
+	int err = 0;
+
+	if (test_and_clear_bit(DO_MEMPOOL, &megaz->flags)) {
+		int pool_size = MZ_MEMPOOL_SZ * 4;
+		u64 tnow = jiffies_64;
+		u64 mem_time;
+
+		mem_time = msecs_to_jiffies(1500);
+		mem_time = (mem_time < tnow) ? tnow - mem_time : 0;
+		if (time_before64(megaz->age, mem_time))
+			pool_size = MZ_MEMPOOL_SZ;
+
+		mem_time = msecs_to_jiffies(5000);
+		mem_time = (mem_time < tnow) ? tnow - mem_time : 0;
+		if (time_before64(megaz->age, mem_time))
+			pool_size = 3;
+
+		mutex_lock(&megaz->mz_io_mutex);
+		err = fpages(megaz, pool_size);
+		mutex_unlock(&megaz->mz_io_mutex);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_sync_to_disk(struct megazone *megaz)
+{
+	int err = 0;
+
+	if (test_and_clear_bit(DO_SYNC, &megaz->flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = do_SYNC(megaz, 1);
+		mutex_unlock(&megaz->mz_io_mutex);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void meta_work_task(struct work_struct *work)
+{
+	int err = 0;
+	struct megazone *megaz;
+
+	if (!work)
+		return;
+
+	megaz = container_of(work, struct megazone, meta_work);
+	if (!megaz)
+		return;
+
+	err = do_init_from_journal(megaz);
+
+	/* reduce memory pressure on journal list of arrays
+	   by pushing them into the sector map lookup tables */
+	if (!err)
+		err = do_journal_to_table(megaz);
+
+	/* reduce memory pressure on sector map lookup tables
+	   by pushing them onto disc */
+	if (!err)
+		err = do_free_pages(megaz);
+
+	/* force a consistent set of meta data out to disk */
+	if (!err)
+		err = do_sync_to_disk(megaz);
+
+	if (!err)
+		err = do_meta_check(megaz);
+
+	megaz->age = jiffies_64;
+	if (err)
+		megaz->meta_result = err;
+
+	clear_bit(DO_METAWORK_QD, &megaz->flags);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_to_list(struct megazone *megaz, u64 dm_s, u64 lba)
+{
+	struct map_cache *jrnl = NULL;
+	struct map_cache *jrnl_first = NULL;
+	int handled = 0;
+	int list_count = 0;
+	int err = 0;
+
+	jrnl = jfirst_entry(megaz);
+	while (jrnl) {
+		int at;
+
+		mutex_lock(&jrnl->cached_lock);
+		jcache_sort_no_lock(jrnl);
+		at = __find_sector_entry_chunk(&jrnl->jdata[1], jrnl->jcount,
+					       dm_s, 0);
+		if (at != -1) {
+			struct map_sect_to_lba *data = &jrnl->jdata[at + 1];
+			u64 lba_was = le64_to_lba48(data->physical, NULL);
+			u64 physical = lba & Z_LOWER48;
+
+			if (lba != lba_was) {
+				Z_DBG(megaz->znd,
+					 "Remap %" PRIx64 " -> %" PRIx64
+					 " (was %" PRIx64 "->%" PRIx64 ")",
+					 dm_s, lba,
+					 le64_to_lba48(data->logical, NULL),
+					 le64_to_lba48(data->physical, NULL));
+				err = unused_phy(megaz, lba_was, 0);
+				if (1 == err)
+					err = 0;
+			}
+			data->physical = cpu_to_le64(physical);
+			handled = 1;
+		} else if (!jrnl_first) {
+			if (jrnl->jcount < jrnl->jsize)
+				jrnl_first = jrnl;
+		}
+		mutex_unlock(&jrnl->cached_lock);
+		if (handled) {
+			jderef(megaz, jrnl);
+			goto out;
+		}
+		jrnl = jnext_entry(megaz, jrnl);
+		list_count++;
+	}
+
+	/* ------------------------------------------------------------------ */
+	/* ------------------------------------------------------------------ */
+	if (jrnl_first) {
+		atomic_inc(&jrnl_first->refcount);
+	} else {
+		jrnl_first = jalloc(megaz->znd);
+		if (jrnl_first) {
+			atomic_inc(&jrnl_first->refcount);
+			jlist_add(megaz, jrnl_first);
+		} else {
+			Z_ERR(megaz->znd,
+			      "%s: in memory journal is out of space.",
+			      __func__);
+			err = -ENOMEM;
+			goto out;
+		}
+
+		if (list_count > JOURNAL_MEMCACHE_BLOCKS)
+			set_bit(DO_JOURNAL_MOVE, &megaz->flags);
+		megaz->mc_entries = list_count + 1;
+	}
+
+	/* ------------------------------------------------------------------ */
+	/* ------------------------------------------------------------------ */
+
+	if (jrnl_first) {
+		mutex_lock(&jrnl_first->cached_lock);
+
+		if (jrnl_first->jcount < jrnl_first->jsize) {
+			u16 idx = ++jrnl_first->jcount;
+
+			jrnl_first->jdata[idx].logical = lba48_to_le64(0, dm_s);
+			jrnl_first->jdata[idx].physical = lba48_to_le64(0, lba);
+		} else {
+			Z_ERR(megaz->znd, "%s: cached bin out of space!",
+			      __func__);
+			err = -EBUSY;
+		}
+		mutex_unlock(&jrnl_first->cached_lock);
+		jderef(megaz, jrnl_first);
+	}
+out:
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void sb_merge(struct megazone *megaz)
+{
+	struct zdm_superblock *sblk = &megaz->bmkeys->sblock;
+
+	memcpy(sblk, megaz->znd->super_block, sizeof(*sblk));
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u64 next_generation(struct megazone *megaz)
+{
+	u64 generation = le64_to_cpu(megaz->bmkeys->generation);
+
+	if (0 == generation)
+		generation = 2;
+
+	generation++;
+	if (0 == generation)
+		generation++;
+
+	return generation;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_sync(struct megazone *megaz)
+{
+	struct dm_target *ti = megaz->znd->ti;
+	struct map_cache *jrnl;
+	int nblks = 1;
+	int use_wq = 0;
+	int rc = 1;
+	int jwrote = 0;
+	int cached = 0;
+	int idx = 0;
+	int do_reset_wp = 0;
+	int z_id = 0;
+	int need_sync_io = 1;
+	int sync_io_blocks = sizeof(*megaz->sync_io) / Z_C4K;
+	u64 lba = 0;
+	u64 generation = next_generation(megaz);
+	u64 modulo = 3;
+	u64 incr = 512;
+	struct megazone *mz0;
+	struct io_4k_block *sync_cache = get_sync_cache(megaz);
+
+	if (!sync_cache) {
+		Z_ERR(megaz->znd, "FAILED to get SYNC CACHE.");
+		rc = -ENOMEM;
+		goto out;
+	}
+
+	if (megaz->mega_nr >= 32)
+		Z_ERR(megaz->znd, "TODO: Use lba of mz #32.");
+	else
+		lba += (modulo * incr) * megaz->mega_nr;
+
+	lba += (generation % modulo) * incr;
+	if (0 == lba)
+		lba++;
+
+	megaz->bmkeys->generation = cpu_to_le64(generation);
+	megaz->bmkeys->gc_resv = megaz->z_gc_resv;
+	megaz->bmkeys->meta_resv = megaz->z_meta_resv;
+
+	sb_merge(megaz);
+
+	jrnl = jfirst_entry(megaz);
+	while (jrnl) {
+		u64 phy = le64_to_lba48(jrnl->jdata[0].physical, NULL);
+		u16 jcount = jrnl->jcount & 0xFFFF;
+
+		jrnl->jdata[0].physical = lba48_to_le64(jcount, phy);
+		megaz->bmkeys->crcs[idx] = crc_md_le16(jrnl->jdata, Z_CRC_4K);
+		idx++;
+
+		memcpy(sync_cache[cached].data, jrnl->jdata, Z_C4K);
+		cached++;
+
+		if (cached == SYNC_CACHE_PAGES) {
+			rc = write_block(ti, DM_IO_VMA,
+					 sync_cache, lba, cached, use_wq);
+			if (rc) {
+				Z_ERR(megaz->znd, "%s: cache-> %" PRIu64
+				      " [%d blks] %p -> %d",
+				      __func__, lba, nblks, jrnl->jdata, rc);
+				jderef(megaz, jrnl);
+				goto out;
+			}
+			lba    += cached;
+			jwrote += cached;
+			cached  = 0;
+		}
+		jrnl = jnext_entry(megaz, jrnl);
+	}
+
+	jwrote += cached;
+	if (jwrote > 20)
+		Z_ERR(megaz->znd, "MZ#%u **WARNING** large map cache %d",
+		       megaz->mega_nr, jwrote);
+
+	mz0 = &megaz->znd->z_mega[0];
+	mutex_lock(&mz0->zp_lock);
+	mz0->z_ptrs[z_id] |= lba + 2;
+	mz0->zfree_count[z_id] = 0;
+	mutex_unlock(&mz0->zp_lock);
+
+	mutex_lock(&megaz->zp_lock);
+	megaz->bmkeys->n_crcs = cpu_to_le16(jwrote);
+	megaz->bmkeys->zp_crc = crc_md_le16(megaz->z_ptrs, Z_CRC_4K);
+	megaz->bmkeys->free_crc = crc_md_le16(megaz->zfree_count, Z_CRC_4K);
+	megaz->bmkeys->key_crc = 0;
+	megaz->bmkeys->key_crc = crc_md_le16(megaz->bmkeys, Z_CRC_4K);
+
+	if (cached < (SYNC_CACHE_PAGES - 3)) {
+		memcpy(sync_cache[cached].data, megaz->bmkeys, Z_C4K);
+		cached++;
+		memcpy(sync_cache[cached].data, megaz->z_ptrs, Z_C4K);
+		cached++;
+		memcpy(sync_cache[cached].data, megaz->zfree_count, Z_C4K);
+		cached++;
+		need_sync_io = 0;
+	}
+
+	if (cached > 0) {
+		rc = write_block(ti, DM_IO_VMA, sync_cache, lba, cached,
+				 use_wq);
+		if (rc) {
+			Z_ERR(megaz->znd, "%s: Jrnl-> %" PRIu64
+			      " [%d blks] %p -> %d",
+			      __func__, lba, cached, sync_cache, rc);
+			mutex_unlock(&megaz->zp_lock);
+			goto out;
+		}
+		lba += cached;
+	}
+
+	if (need_sync_io) {
+		void *data = megaz->sync_io;
+
+		nblks = sync_io_blocks;
+		rc = write_block(ti, DM_IO_KMEM, data, lba, nblks, use_wq);
+		if (rc) {
+			Z_ERR(megaz->znd,
+			      "%s: WPs -> %" PRIu64 " [%d blks] %p -> %d",
+			       __func__, lba, nblks, data, rc);
+			mutex_unlock(&megaz->zp_lock);
+			goto out;
+		}
+	}
+	mutex_unlock(&megaz->zp_lock);
+
+	if (do_reset_wp)
+		dmz_reset_wp(megaz, z_id);
+
+out:
+	put_sync_cache(megaz, sync_cache);
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_key_page(void *_data)
+{
+	int is_key = 0;
+	struct mz_superkey *data = _data;
+
+	/* Starts with Z_KEY_SIG and ends with magic */
+
+	if (Z_KEY_SIG == le64_to_cpu(data->sig[1])) {
+		if (Z_TABLE_MAGIC == le64_to_cpu(data->magic)) {
+			u16 crc_value = data->key_crc;
+			u16 crc_check;
+
+			data->key_crc = 0;
+			crc_check = crc_md_le16(data, Z_CRC_4K);
+			data->key_crc = crc_value;
+			if (crc_check == crc_value)
+				is_key = 1;
+		}
+	}
+	return is_key;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void zoned_personality(struct zoned *znd,
+				     struct zdm_superblock *sblock)
+{
+	znd->first_zone = le64_to_cpu(sblock->first_zone);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int find_superblock_at(struct megazone *megaz, u64 lba, int use_wq,
+			      int do_init)
+{
+	struct dm_target *ti = megaz->znd->ti;
+	int found = 0;
+	int nblks = 1;
+	int rc = -ENOMEM;
+	u32 count = 0;
+	u64 *data = ZDM_ALLOC(megaz->znd, Z_C4K, PG_10);
+
+	if (!data) {
+		Z_ERR(megaz->znd, "No memory for finding generation ..");
+		return 0;
+	}
+	if (0 == lba)
+		lba++;
+	do {
+		rc = read_block(ti, DM_IO_KMEM, data, lba, nblks, use_wq);
+		if (rc) {
+			Z_ERR(megaz->znd,
+				"%s: read @%" PRIu64 " [%d blks] %p -> %d",
+			       __func__, lba, nblks, data, rc);
+			goto out;
+		}
+		if (is_key_page(data)) {
+			struct mz_superkey *kblk = (struct mz_superkey *) data;
+			struct zdm_superblock *sblock = &kblk->sblock;
+			int err = sb_check(sblock);
+
+			if (!err) {
+				found = 1;
+				if (do_init)
+					zoned_personality(megaz->znd, sblock);
+			}
+			goto out;
+		}
+		if (0 == data[0] && 0 == data[1]) {
+			/* No SB here. */
+			Z_ERR(megaz->znd, "FGen: Invalid block %" PRIx64 "?",
+				lba);
+			goto out;
+		}
+		lba++;
+		count++;
+		if (count > MAX_CACHE_SYNC) {
+			Z_ERR(megaz->znd, "FSB: Too deep to be useful.");
+			goto out;
+		}
+	} while (!found);
+
+out:
+	ZDM_FREE(megaz->znd, data, Z_C4K, PG_10);
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int find_superblock(struct megazone *megaz, int use_wq, int do_init)
+{
+	int found = 0;
+	u64 lba;
+
+	for (lba = 0; lba < 0x30000; lba += Z_BLKSZ) {
+		found = find_superblock_at(megaz, lba, use_wq, do_init);
+		if (found)
+			break;
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 mcache_find_gen(struct megazone *megaz, u64 lba, int use_wq,
+				    u64 *sb_lba)
+{
+	struct dm_target *ti = megaz->znd->ti;
+	u64 generation = 0;
+	int nblks = 1;
+	int rc = 1;
+	int done = 0;
+	u32 count = 0;
+	u64 *data = ZDM_ALLOC(megaz->znd, Z_C4K, PG_11);
+
+	if (!data) {
+		Z_ERR(megaz->znd, "No memory for finding generation ..");
+		return 0;
+	}
+	do {
+		rc = read_block(ti, DM_IO_KMEM, data, lba, nblks, use_wq);
+
+		if (rc) {
+			Z_ERR(megaz->znd,
+				"%s: Jrnl-> %" PRIu64 " [%d blks] %p -> %d",
+			       __func__, lba, nblks, data, rc);
+			goto out;
+		}
+		if (is_key_page(data)) {
+			struct mz_superkey *kblk = (struct mz_superkey *) data;
+
+			generation = le64_to_cpu(kblk->generation);
+			done = 1;
+			if (sb_lba)
+				*sb_lba = lba;
+			goto out;
+		}
+		if (0 == data[0] && 0 == data[1]) {
+			/* No SB here... */
+			Z_DBG(megaz->znd,
+				"FGen: Invalid block %" PRIx64 "?", lba);
+			goto out;
+		}
+		lba++;
+		count++;
+		if (count > MAX_CACHE_SYNC) {
+			Z_ERR(megaz->znd, "FGen: Too deep to be useful.");
+			goto out;
+		}
+	} while (!done);
+
+out:
+	ZDM_FREE(megaz->znd, data, Z_C4K, PG_11);
+	return generation;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int cmp_gen(u64 left, u64 right)
+{
+	int result = 0;
+
+	if (left != right) {
+		u64 delta = (left > right) ? left - right : right - left;
+
+		result = -1;
+		if (delta > 1) {
+			if (left == ~0ul)
+				result = 1;
+		} else {
+			if (right > left)
+				result = 1;
+		}
+	}
+
+	return result;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 mcache_greatest_gen(struct megazone *megaz, int use_wq, u64 *sb,
+				u64 *at_lba)
+{
+	u64 lba = 0;
+	u64 gen_no[3] = { 0ul, 0ul, 0ul };
+	u64 gen_lba[3] = { 0ul, 0ul, 0ul };
+	u64 gen_sb[3] = { 0ul, 0ul, 0ul };
+	u64 incr = Z_BLKSZ;
+	int locations = 2;
+	int pick = 0;
+	int idx;
+
+	locations = ARRAY_SIZE(gen_lba);
+	incr = 512ul;
+	if (megaz->mega_nr >= 32)
+		Z_ERR(megaz->znd, "TODO: Use lba of mz #32.");
+	else
+		lba += (locations * incr) * megaz->mega_nr;
+
+	for (idx = 0; idx < locations; idx++) {
+		u64 *pAt = &gen_sb[idx];
+
+		gen_lba[idx] = lba;
+		gen_no[idx] = mcache_find_gen(megaz, lba, use_wq, pAt);
+		if (gen_no[idx])
+			pick = idx;
+		lba += incr;
+	}
+
+	for (idx = 0; idx < locations; idx++) {
+		if (cmp_gen(gen_no[pick], gen_no[idx]) > 0)
+			pick = idx;
+	}
+
+	if (gen_no[pick]) {
+		if (at_lba)
+			*at_lba = gen_lba[pick];
+		if (sb)
+			*sb = gen_sb[pick];
+	}
+
+	return gen_no[pick];
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_init(struct megazone *megaz)
+{
+	struct dm_target *ti = megaz->znd->ti;
+	int nblks = 1;
+	int use_wq = 0;
+	int rc = 1;
+	int done = 0;
+	int jfound = 0;
+	int idx = 0;
+	struct list_head hjload;
+	u64 lba = 0;
+	u64 generation;
+	u16 crc_chk;
+	struct io_4k_block *sync_cache = get_sync_cache(megaz);
+
+	if (!sync_cache) {
+		Z_ERR(megaz->znd, "FAILED to get SYNC CACHE.");
+		rc = -ENOMEM;
+		goto out;
+	}
+
+	INIT_LIST_HEAD(&hjload);
+
+	generation = mcache_greatest_gen(megaz, use_wq, NULL, &lba);
+	if (0 == generation) {
+		rc = -ENODATA;
+		goto out;
+	}
+
+	if (0 == lba)
+		lba++;
+
+	do {
+		struct map_cache *jrnl = jalloc(megaz->znd);
+
+		if (!jrnl) {
+			rc = -ENOMEM;
+			goto out;
+		}
+
+		rc = read_block(ti, DM_IO_KMEM,
+				jrnl->jdata, lba, nblks, use_wq);
+		if (rc) {
+			Z_ERR(megaz->znd,
+				"%s: Jrnl-> %" PRIu64 " [%d blks] %p -> %d",
+			       __func__, lba, nblks, jrnl->jdata, rc);
+
+			goto out;
+		}
+		lba++;
+
+		if (is_key_page(jrnl->jdata)) {
+			size_t sz = Z_UNSORTED * sizeof(*jrnl->jdata);
+
+			memcpy(megaz->bmkeys, jrnl->jdata, Z_C4K);
+			jrnl->jcount = 0;
+			done = 1;
+			ZDM_FREE(megaz->znd, jrnl->jdata, sz, PG_08);
+			ZDM_FREE(megaz->znd, jrnl, sizeof(*jrnl), KM_07);
+			jrnl = NULL;
+		} else {
+			u16 jcount;
+
+			(void)le64_to_lba48(jrnl->jdata[0].physical, &jcount);
+			jrnl->jcount = jcount;
+			list_add(&(jrnl->jlist), &hjload);
+			jfound++;
+		}
+
+		if (jfound > MAX_CACHE_SYNC) {
+			rc = -EIO;
+			goto out;
+		}
+	} while (!done);
+
+	crc_chk = megaz->bmkeys->key_crc;
+	megaz->bmkeys->key_crc = 0;
+	megaz->bmkeys->key_crc = crc_md_le16(megaz->bmkeys, Z_CRC_4K);
+
+	if (crc_chk != megaz->bmkeys->key_crc) {
+		Z_ERR(megaz->znd, "Bad Block Map KEYS!");
+		Z_ERR(megaz->znd,
+		      "MZ#%u Key CRC: Ex: %04x vs %04x <- calculated",
+		      megaz->mega_nr, le16_to_cpu(crc_chk),
+		      le16_to_cpu(megaz->bmkeys->key_crc));
+		rc = -EIO;
+	}
+
+	if (jfound != le16_to_cpu(megaz->bmkeys->n_crcs)) {
+		Z_ERR(megaz->znd,
+			"MZ#%u mcache entries: found = %u, expected = %u",
+			megaz->mega_nr, jfound,
+			le16_to_cpu(megaz->bmkeys->n_crcs));
+		rc = -EIO;
+	}
+
+	if ((crc_chk == megaz->bmkeys->key_crc) && !list_empty(&hjload)) {
+		struct map_cache *jrnl;
+		struct map_cache *jsafe;
+
+		list_for_each_entry_safe(jrnl, jsafe, &hjload, jlist) {
+			u16 crc = crc_md_le16(jrnl->jdata, Z_CRC_4K);
+
+			Z_DBG(megaz->znd,
+				"MZ#%u JRNL CRC: %u: %04x [vs %04x] (c:%d)",
+			      megaz->mega_nr, idx, le16_to_cpu(crc),
+			      le16_to_cpu(megaz->bmkeys->crcs[idx]),
+			      jrnl->jcount);
+
+			if (crc == megaz->bmkeys->crcs[idx]) {
+				jlist_add(megaz, jrnl);
+			} else {
+				Z_ERR(megaz->znd,
+				      "MZ#%u %04x [vs %04x] (c:%d)",
+				      megaz->mega_nr, le16_to_cpu(crc),
+				      le16_to_cpu(megaz->bmkeys->crcs[idx]),
+				      jrnl->jcount);
+				rc = -EIO;
+			}
+			idx++;
+		}
+	}
+
+	mutex_lock(&megaz->zp_lock);
+
+	/*
+	 * Read last know write printers
+	 */
+	rc = read_block(ti, DM_IO_VMA, sync_cache, lba, nblks, use_wq);
+	if (rc)
+		Z_DBG(megaz->znd,
+		      "%s: WPs -> %" PRIu64 " [%d blks] %p -> %d",
+		      __func__, lba, nblks, megaz->z_ptrs, rc);
+
+	memcpy(megaz->z_ptrs, sync_cache[0].data, Z_C4K);
+	crc_chk = crc_md_le16(megaz->z_ptrs, Z_CRC_4K);
+	if (crc_chk != megaz->bmkeys->zp_crc) {
+		Z_ERR(megaz->znd,
+		      "MZ#%u WPs CRC: Ex %04x vs %04x  <- calculated",
+		      megaz->mega_nr, le16_to_cpu(megaz->bmkeys->zp_crc),
+		      le16_to_cpu(crc_chk));
+		Z_ERR(megaz->znd, "Bad zone pointers!!");
+		set_bit(DO_META_CHECK, &megaz->flags);
+	}
+	memcpy(megaz->z_commit, megaz->z_ptrs, Z_C4K);
+	lba++;
+
+	/*
+	 * Read last calculated free counters
+	 */
+	rc = read_block(ti, DM_IO_VMA, sync_cache, lba, nblks, use_wq);
+	if (rc)
+		Z_DBG(megaz->znd,
+		      "%s: WPs -> %" PRIu64 " [%d blks] %p -> %d",
+		      __func__, lba, nblks, megaz->zfree_count, rc);
+
+	memcpy(megaz->zfree_count, sync_cache[0].data, Z_C4K);
+
+	crc_chk = crc_md_le16(megaz->zfree_count, Z_CRC_4K);
+	if (crc_chk != megaz->bmkeys->free_crc) {
+		Z_ERR(megaz->znd, "Bad zone free counters!!");
+		Z_ERR(megaz->znd,
+		      "MZ#%u FreeCount CRC: Ex %04x vs %04x  <- calculated",
+		      megaz->mega_nr, le16_to_cpu(megaz->bmkeys->free_crc),
+		      le16_to_cpu(crc_chk));
+		set_bit(DO_META_CHECK, &megaz->flags);
+	} else {
+		int entry = 0;
+
+		megaz->discard_count = 0;
+		if (0 == megaz->mega_nr)
+			entry++;
+		for (; entry < megaz->z_count; entry++) {
+			u32 used = megaz->z_ptrs[entry] & Z_WP_VALUE_MASK;
+
+			if (used == Z_BLKSZ) {
+				u32 nfree = megaz->zfree_count[entry];
+
+				megaz->discard_count += nfree;
+			}
+		}
+	}
+	megaz->z_gc_resv = megaz->bmkeys->gc_resv;
+	megaz->z_meta_resv = megaz->bmkeys->meta_resv;
+	mutex_unlock(&megaz->zp_lock);
+
+out:
+	put_sync_cache(megaz, sync_cache);
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_addmany(struct megazone *megaz, u64 dm_s, u64 lba,
+			    u64 count)
+{
+	int rc = 0;
+	sector_t blk;
+
+	for (blk = 0; blk < count; blk++) {
+		rc = z_mapped_add_one(megaz, dm_s + blk, lba + blk);
+		if (rc)
+			goto out;
+	}
+out:
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/**
+ * Lookup a logical sector address to find the disk LBA
+ *
+ *   z_lookup		 (u64)
+ *     locate_sector	  *, u64
+ *       sector_map_entry     (int)
+ *	 get_map_entry      *, u64, *, int
+ *	   get_map_table_entry *, *, u64, u64
+ *	   load_page	u64, int, int, int, int, *, u16, int
+ *	     z_lookup
+ *
+ *
+ *
+ *
+ */
+static u64 z_lookup(struct megazone *megaz, struct map_addr *maddr)
+{
+	u64 found = z_lookup_key_range(megaz, maddr);
+
+	if (!found)
+		found = z_lookup_cache(megaz, maddr);
+	if (!found)
+		found = locate_sector(megaz, maddr);
+
+	return found;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct map_pg *load_map_entry(struct megazone *megaz, u64 lba, int is_to)
+{
+	struct map_pg *mapped = get_map_table_entry(megaz, lba, is_to);
+
+	if (mapped) {
+		if (!mapped->mdata) {
+			int rc = map_entry_page(megaz, mapped, lba, is_to);
+
+			if (rc < 0)
+				megaz->meta_result = rc;
+		}
+	} else {
+		Z_ERR(megaz->znd, "%s: No table for page# %" PRIx64 ".",
+		      __func__, lba);
+	}
+	return mapped;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int metadata_dirty_fling(struct megazone *megaz, u64 dm_s)
+{
+	struct zoned *znd = megaz->znd;
+	struct mzlam *lam = &megaz->logical_map;
+	struct map_pg *Smap = NULL;
+	int is_flung = 0;
+
+	Smap = NULL;
+	if ((lam->r_base <= dm_s) && dm_s < (lam->r_base + Z_BLKSZ)) {
+		Smap = load_map_entry(megaz, dm_s, 0);
+		if (!Smap)
+			Z_ERR(znd, "Failed to fling: %" PRIx64, dm_s);
+	} else if ((lam->s_base <= dm_s) && dm_s < (lam->s_base + Z_BLKSZ)) {
+		Smap = load_map_entry(megaz, dm_s, 1);
+		if (!Smap)
+			Z_ERR(znd, "Failed to fling: %" PRIx64, dm_s);
+	}
+	if (Smap) {
+		atomic_inc(&Smap->refcount);
+		mapped_lock(Smap);
+		Smap->age = jiffies_64;
+		set_bit(IS_DIRTY, &Smap->flags);
+		is_flung = 1;
+		mapped_unlock(Smap);
+		atomic_dec(&Smap->refcount);
+
+		if (Smap->lba != dm_s)
+			Z_ERR(znd, "Excess churn? lba %"PRIx64
+				   " [last: %"PRIx64"]", dm_s, Smap->lba);
+	}
+
+	if ((lam->crc_low <= dm_s) && (dm_s < lam->crc_hi)) {
+		int off = dm_s - lam->crc_low;
+		struct crc_pg *pblock = NULL;
+		u64 lba;
+		u16 crc;
+
+		if (off < MZKY_NCRC) {
+			pblock = &megaz->rtm_crc[off];
+			mutex_lock(&megaz->mapkey_lock);
+			lba = le64_to_cpu(megaz->bmkeys->rtm_crc_lba[off]);
+			crc = megaz->bmkeys->rtm_crc_pg[off];
+			mutex_unlock(&megaz->mapkey_lock);
+		} else {
+			off -= MZKY_NCRC;
+			pblock = &megaz->stm_crc[off];
+			mutex_lock(&megaz->mapkey_lock);
+			lba = le64_to_cpu(megaz->bmkeys->stm_crc_lba[off]);
+			crc = megaz->bmkeys->stm_crc_pg[off];
+			mutex_unlock(&megaz->mapkey_lock);
+		}
+		if (pblock && lba) {
+			const int wqueue = 0;
+
+			atomic_inc(&pblock->refcount);
+			load_crc_meta_pg(megaz, pblock, lba, crc, wqueue);
+			mutex_lock(&pblock->lock_pg);
+			set_bit(IS_DIRTY, &pblock->flags);
+			pblock->age = jiffies_64;
+			is_flung = 1;
+			mutex_unlock(&pblock->lock_pg);
+			if (pblock->lba != dm_s)
+				Z_ERR(znd, "Excess churn? lba %"PRIx64
+				      " [last: %"PRIx64"]", dm_s, pblock->lba);
+			atomic_dec(&pblock->refcount);
+		}
+	}
+	return is_flung;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void z_do_copy_more(struct gc_state *gc_entry)
+{
+	unsigned long flags;
+	struct zoned *znd = gc_entry->megaz->znd;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	set_bit(DO_GC_PREPARE, &gc_entry->gc_flags);
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_post(struct megazone *megaz, u64 dm_s, u64 lba)
+{
+	struct zoned *znd = megaz->znd;
+	struct map_cache *post = &znd->gc_postmap;
+	int handled = 0;
+
+	if (post->jcount < post->jsize) {
+		u16 idx = ++post->jcount;
+
+		post->jdata[idx].logical = lba48_to_le64(0, dm_s);
+		post->jdata[idx].physical = lba48_to_le64(0, lba);
+		handled = 1;
+	} else {
+		Z_ERR(znd, "*CRIT* post overflow L:%llx -> S:%llx", lba, dm_s);
+	}
+	return handled;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_metadata_to_ram(struct gc_state *gc_entry)
+{
+	struct megazone *megaz = gc_entry->megaz;
+	struct zoned *znd = megaz->znd;
+	struct map_pg *ORmap;
+	struct map_pg *Smap;
+	struct mzlam *lam = &megaz->logical_map;
+	u64 from_lba;
+	struct map_addr ORaddr;
+	struct map_addr Saddr;
+	int rcode = 0;
+	int count;
+
+	from_lba = (lam->mz_base + gc_entry->z_gc) * Z_BLKSZ;
+	mutex_lock(&megaz->mz_io_mutex);
+	for (count = 0; count < MZKY_NCRC; count++) {
+		struct crc_pg *pblock;
+		u16 crc;
+		u64 lba;
+		int wqueue = 0;
+
+		mutex_lock(&megaz->mapkey_lock);
+		lba = le64_to_cpu(megaz->bmkeys->rtm_crc_lba[count]);
+		mutex_unlock(&megaz->mapkey_lock);
+		if (lba && (_calc_zone(lba) == gc_entry->z_gc)) {
+			pblock = &megaz->rtm_crc[count];
+			crc = megaz->bmkeys->rtm_crc_pg[count];
+			atomic_inc(&pblock->refcount);
+			load_crc_meta_pg(megaz, pblock, lba, crc, wqueue);
+			mutex_lock(&pblock->lock_pg);
+			set_bit(IS_DIRTY, &pblock->flags);
+			pblock->age = jiffies_64;
+			mutex_unlock(&pblock->lock_pg);
+			atomic_dec(&pblock->refcount);
+		}
+		mutex_lock(&megaz->mapkey_lock);
+		lba = le64_to_cpu(megaz->bmkeys->stm_crc_lba[count]);
+		mutex_unlock(&megaz->mapkey_lock);
+		if (lba && (_calc_zone(lba) == gc_entry->z_gc)) {
+			pblock = &megaz->stm_crc[count];
+			crc = megaz->bmkeys->stm_crc_pg[count];
+			atomic_inc(&pblock->refcount);
+			load_crc_meta_pg(megaz, pblock, lba, crc, wqueue);
+			mutex_lock(&pblock->lock_pg);
+			set_bit(IS_DIRTY, &pblock->flags);
+			pblock->age = jiffies_64;
+			mutex_unlock(&pblock->lock_pg);
+			atomic_dec(&pblock->refcount);
+		}
+	}
+
+	/* pull all of the affect struct map_pg and crc pages into memory: */
+	for (count = 0; count < Z_BLKSZ; count++) {
+		u32 ORencoded;
+		u64 ORlba = from_lba + count;
+
+		map_addr_calc(ORlba, &ORaddr);
+		ORmap = reverse_map_entry(megaz, &ORaddr);
+		if (ORmap && ORmap->mdata) {
+			atomic_inc(&ORmap->refcount);
+			mapped_lock(ORmap);
+			ORencoded = ORmap->mdata[ORaddr.offentry];
+			mapped_unlock(ORmap);
+			if (ORencoded != MZTEV_UNUSED) {
+				u64 dm_s = map_value(megaz, ORencoded);
+				int flung = 1;
+
+				if (dm_s < znd->nr_blocks) {
+					map_addr_calc(dm_s, &Saddr);
+					Smap = sector_map_entry(megaz, &Saddr);
+					if (!Smap)
+						rcode = -ENOMEM;
+				} else {
+					Z_ERR(znd, "Invalid rmap entry: %x.",
+					      ORencoded);
+				}
+
+				if (!metadata_dirty_fling(megaz, dm_s)) {
+					flung = 0;
+					gc_post(megaz, dm_s, ORlba);
+				}
+			}
+			atomic_dec(&ORmap->refcount);
+		}
+	}
+	mutex_unlock(&megaz->mz_io_mutex);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int append_blks(struct megazone *megaz, u64 lba,
+		       struct io_4k_block *io_buf, int count)
+{
+	int rcode = 0;
+	int rc;
+	u32 chunk;
+	struct zoned *znd = megaz->znd;
+	struct io_4k_block *sync_cache = get_sync_cache(megaz);
+
+	if (!sync_cache) {
+		Z_ERR(megaz->znd, "FAILED to get SYNC CACHE.");
+		rc = -ENOMEM;
+		goto out;
+	}
+
+	for (chunk = 0; chunk < count; chunk += SYNC_CACHE_PAGES) {
+		u32 nblks = count - chunk;
+
+		if (nblks > SYNC_CACHE_PAGES)
+			nblks = SYNC_CACHE_PAGES;
+
+		rc = read_block(znd->ti, DM_IO_VMA, sync_cache, lba, nblks, 0);
+		if (rc) {
+			Z_ERR(znd, "Reading error ... disable zone: %u",
+				(u32)(lba >> 16));
+			rcode = -EIO;
+			goto out;
+		}
+		memcpy(&io_buf[chunk], sync_cache, nblks * Z_C4K);
+		lba += nblks;
+	}
+out:
+	put_sync_cache(megaz, sync_cache);
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_read(struct gc_state *gc_entry)
+{
+	struct megazone *megaz = gc_entry->megaz;
+	struct zoned *znd = megaz->znd;
+	struct io_4k_block *io_buf = megaz->znd->gc_io_buf;
+	struct map_cache *post = &znd->gc_postmap;
+	unsigned long flags;
+	u64 start_lba;
+	int nblks;
+	int rcode = 0;
+	int fill = 0;
+	int jstart;
+	int jentry;
+
+	spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+	jstart = gc_entry->r_ptr;
+	spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+
+	if (0 == jstart)
+		jstart++;
+
+	mutex_lock(&post->cached_lock);
+
+	/* A discard may have puched holes in the postmap. re-sync lba */
+	jentry = jstart;
+	while (jentry <= post->jcount && (Z_LOWER48 ==
+			le64_to_lba48(post->jdata[jentry].physical, NULL))) {
+		jentry++;
+	}
+	/* nothing left to move */
+	if (jentry > post->jcount)
+		goto out_finished;
+
+	/* skip over any discarded blocks */
+	if (jstart != jentry)
+		jstart = jentry;
+
+	start_lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+	post->jdata[jentry].physical = lba48_to_le64(GC_READ, start_lba);
+	nblks = 1;
+	jentry++;
+
+	while (jentry <= post->jcount && (nblks+fill) < GC_MAX_STRIPE) {
+		u64 dm_s = le64_to_lba48(post->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+
+		if (Z_LOWER48 == dm_s || Z_LOWER48 == lba) {
+			jentry++;
+			continue;
+		}
+
+		post->jdata[jentry].physical = lba48_to_le64(GC_READ, lba);
+
+		/* if the block is contiguous add it to the read */
+		if (lba == (start_lba + nblks)) {
+			nblks++;
+		} else {
+			if (nblks) {
+				int err;
+
+				err = append_blks(megaz, start_lba,
+						  &io_buf[fill], nblks);
+				if (err) {
+					rcode = err;
+					goto out;
+				}
+				fill += nblks;
+			}
+			start_lba = lba;
+			nblks = 1;
+		}
+		jentry++;
+	}
+
+	/* Issue a copy of 'nblks' blocks */
+	if (nblks > 0) {
+		int err;
+
+		err = append_blks(megaz, start_lba, &io_buf[fill], nblks);
+		if (err) {
+			rcode = err;
+			goto out;
+		}
+		fill += nblks;
+	}
+
+out_finished:
+	Z_DBG(znd, "Read %d blocks from %d", fill, gc_entry->r_ptr);
+
+	spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+	gc_entry->nblks = fill;
+	gc_entry->r_ptr = jentry;
+	if (fill > 0)
+		set_bit(DO_GC_WRITE, &gc_entry->gc_flags);
+	else
+		set_bit(DO_GC_COMPLETE, &gc_entry->gc_flags);
+
+	spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+
+out:
+	mutex_unlock(&post->cached_lock);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u64 current_mapping(struct megazone *megaz, u64 dm_s)
+{
+	struct map_addr maddr;
+
+	map_addr_calc(dm_s, &maddr);
+	return z_lookup(megaz, &maddr);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_write(struct gc_state *gc_entry)
+{
+	struct megazone *megaz = gc_entry->megaz;
+	struct zoned *znd = megaz->znd;
+	struct dm_target *ti = znd->ti;
+	struct io_4k_block *io_buf = megaz->znd->gc_io_buf;
+	struct map_cache *post = &znd->gc_postmap;
+	unsigned long flags;
+	u64 lba;
+	u32 nblks;
+	u32 out = 0;
+	int err = 0;
+	int jentry;
+
+	spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+	jentry = gc_entry->w_ptr;
+	nblks = gc_entry->nblks;
+	spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+
+	if (0 == jentry)
+		jentry++;
+
+	mutex_lock(&post->cached_lock);
+
+	while (nblks > 0) {
+		u32 nfound = 0;
+		u32 added = 0;
+
+		/* when lba is zero blocks were not allocated. retry with the
+		   smaller request */
+		lba = z_acquire(megaz, Z_AQ_GC, nblks, &nfound);
+		if (!lba) {
+			if (nfound) {
+				u32 avail = nfound;
+
+				nfound = 0;
+				lba = z_acquire(megaz, Z_AQ_GC, avail, &nfound);
+			}
+		}
+
+		if (!lba) {
+			err = -ENOSPC;
+			goto out;
+		}
+
+		err = write_block(ti, DM_IO_VMA, &io_buf[out], lba, nfound, 0);
+		if (err) {
+			Z_ERR(znd, "Write %d blocks to %"PRIx64". ERROR: %d",
+			      nfound, lba, err);
+			goto out;
+		}
+		out += nfound;
+
+		while ((jentry <= post->jcount) && (added < nfound)) {
+			u16 rflg;
+			u64 orig = le64_to_lba48(
+					post->jdata[jentry].physical, &rflg);
+			u64 dm_s = le64_to_lba48(
+					post->jdata[jentry].logical, NULL);
+
+			if ((Z_LOWER48 == dm_s || Z_LOWER48 == orig)) {
+				jentry++;
+
+				if (rflg & GC_READ) {
+					Z_ERR(znd, "ERROR: %" PRIx64
+					      " read and not written %" PRIx64,
+					      orig, dm_s);
+					lba++;
+					added++;
+				}
+				continue;
+			}
+			rflg &= ~GC_READ;
+			post->jdata[jentry].physical = lba48_to_le64(rflg, lba);
+			lba++;
+			added++;
+			jentry++;
+		}
+		nblks -= nfound;
+	}
+	Z_DBG(znd, "Write %d blocks from %d", gc_entry->nblks, gc_entry->w_ptr);
+	set_bit(DO_GC_META, &gc_entry->gc_flags);
+
+out:
+	spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+	gc_entry->nblks = 0;
+	gc_entry->w_ptr = jentry;
+	spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+	mutex_unlock(&post->cached_lock);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_finalize(struct gc_state *gc_entry)
+{
+	int err = 0;
+	struct megazone *megaz = gc_entry->megaz;
+	struct map_cache *post = &megaz->znd->gc_postmap;
+	int jentry;
+
+	mutex_lock(&post->cached_lock);
+	for (jentry = post->jcount; jentry > 0; jentry--) {
+		u64 dm_s = le64_to_lba48(post->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+
+		if (dm_s != Z_LOWER48 || lba != Z_LOWER48) {
+			Z_ERR(megaz->znd,
+			      "GC: Failed to move MZ# %u %" PRIx64
+			      " from %"PRIx64" [%d]",
+			      megaz->mega_nr, dm_s, lba, jentry);
+			err = -EIO;
+		}
+	}
+	mutex_unlock(&post->cached_lock);
+	post->jcount = jentry;
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void clear_gc_target(struct megazone *megaz)
+{
+	int z_id;
+
+	for (z_id = megaz->z_data; z_id < megaz->z_count; z_id++) {
+		if (megaz->z_ptrs[z_id] & Z_WP_GC_TARGET) {
+			mutex_lock(&megaz->zp_lock);
+			megaz->z_ptrs[z_id] &= ~Z_WP_GC_TARGET;
+			mutex_unlock(&megaz->zp_lock);
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_metadata_update(struct gc_state *gc_entry)
+{
+	struct megazone *megaz = gc_entry->megaz;
+	struct zoned *znd = megaz->znd;
+	struct map_cache *post = &znd->gc_postmap;
+	u32 used = post->jcount;
+	int err = 0;
+	int jentry;
+	struct mzlam *lam = &megaz->logical_map;
+
+	mutex_lock(&megaz->mz_io_mutex);
+	for (jentry = post->jcount; jentry > 0; jentry--) {
+		int discard = 0;
+		int mapping = 0;
+		struct map_pg *mapped = NULL;
+		u64 dm_s = le64_to_lba48(post->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+
+		if ((lam->r_base <= dm_s) && dm_s < (lam->r_base + Z_BLKSZ)) {
+			u64 off = dm_s - lam->r_base;
+
+			mapped = megaz->reversetm[off];
+			mapping = 1;
+		} else if ((lam->s_base <= dm_s) &&
+			   (dm_s < (lam->s_base + Z_BLKSZ))) {
+			u64 off = dm_s - lam->s_base;
+
+			mapped = megaz->sectortm[off];
+			mapping = 1;
+		}
+
+		if (mapping && !mapped)
+			Z_ERR(megaz->znd,
+				 "MD: dm_s: %" PRIx64 " -> lba: %" PRIx64
+				 " no mapping in ram.", dm_s, lba);
+
+		if (mapped) {
+			atomic_inc(&mapped->refcount);
+			mapped_lock(mapped);
+			if (_calc_zone(mapped->last_write) != gc_entry->z_gc) {
+				Z_ERR(znd, "MD: %" PRIx64
+				      " Discarded - %" PRIx64
+				      " already flown to: %x",
+				      dm_s, mapped->last_write,
+				      _calc_zone(mapped->last_write));
+				discard = 1;
+			} else if (mapped->mdata &&
+				   test_bit(IS_DIRTY, &mapped->flags)) {
+				Z_ERR(znd,
+				      "MD: %" PRIx64 " Discarded - %"PRIx64
+				      " is in-flight",
+				      dm_s, mapped->last_write);
+				discard = 2;
+			}
+			if (!discard)
+				mapped->last_write = lba;
+			mapped_unlock(mapped);
+			atomic_dec(&mapped->refcount);
+		}
+
+		/*
+		 * location of the SLT key sectors need to be
+		 * stashed into the sector lookup table block map
+		 * Does dm_s point in the sector lookup table block map ?
+		 */
+		if ((lam->sk_low <= dm_s) && (dm_s < lam->sk_high)) {
+			if (!discard) {
+				int off = dm_s - lam->sk_low;
+
+				mutex_lock(&megaz->mapkey_lock);
+				megaz->bmkeys->stm_keys[off] = cpu_to_le64(lba);
+				mutex_unlock(&megaz->mapkey_lock);
+			}
+		} else if (lam->crc_low <= dm_s && dm_s < lam->crc_hi) {
+			const u16 z_gc = gc_entry->z_gc;
+			int off = dm_s - lam->crc_low;
+			struct crc_pg *pblock = NULL;
+
+			mutex_lock(&megaz->mapkey_lock);
+			if (off < 32)
+				pblock = &megaz->rtm_crc[off];
+			else
+				pblock = &megaz->stm_crc[off - 32];
+			if (_calc_zone(pblock->last_write) != z_gc) {
+				Z_ERR(znd,
+				      "MD: %" PRIx64 " Discarded - %"PRIx64
+				      " already flown to: %x [CRC]",
+				      dm_s, pblock->last_write,
+				      _calc_zone(pblock->last_write));
+				discard = 1;
+			} else if (pblock->cdata &&
+				   test_bit(IS_DIRTY, &pblock->flags)) {
+
+				Z_ERR(znd, "MD: %" PRIx64 " Discarded - %"PRIx64
+				      " is in-flight [CRC]",
+				      dm_s, pblock->last_write);
+				discard = 2;
+			}
+			/* update current lba for 'moved' and 'in-flight' */
+			if (1 != discard) {
+				mutex_lock(&pblock->lock_pg);
+
+				Z_ERR(znd, "MD: %" PRIx64 " mv from  %"PRIx64
+					   " to: %" PRIu64 " [CRC]",
+				      dm_s, pblock->last_write, lba);
+
+				pblock->last_write = lba;
+				if (off < 32)
+					megaz->bmkeys->rtm_crc_lba[off] =
+					    cpu_to_le64(lba);
+				else
+					megaz->bmkeys->stm_crc_lba[off - 32] =
+					    cpu_to_le64(lba);
+
+				mutex_unlock(&pblock->lock_pg);
+			}
+			mutex_unlock(&megaz->mapkey_lock);
+		}
+
+		mutex_lock(&post->cached_lock);
+		if (1 == discard) {
+			Z_ERR(znd, "Dropped: %" PRIx64 " ->  %"PRIx64,
+			      post->jdata[jentry].logical,
+			      post->jdata[jentry].physical);
+
+			post->jdata[jentry].logical = ~0ul;
+			post->jdata[jentry].physical = ~0ul;
+		}
+		if (~0ul == post->jdata[jentry].logical &&
+		    ~0ul == post->jdata[jentry].physical) {
+			used--;
+		} else {
+			u16 zone = _calc_zone(lba);
+
+			megaz->z_commit[zone]++;
+			if (megaz->z_commit[zone] == Z_BLKSZ) {
+				mutex_lock(&megaz->zp_lock);
+				megaz->z_ptrs[zone] |= Z_WP_GC_READY;
+				mutex_unlock(&megaz->zp_lock);
+			}
+		}
+		mutex_unlock(&post->cached_lock);
+	}
+	err = move_to_map_tables(megaz, post);
+	if (err)
+		Z_ERR(znd, "Move to tables post GC failure");
+
+	clear_gc_target(megaz);
+	mutex_unlock(&megaz->mz_io_mutex);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * Attempt allocation of @param nblks within fron the current WP of z_at
+ * When nblks are not available 0 is returned and @param nfound is the
+ * contains the number of blocks *available* but not *allocated*.
+ * When nblks are available the starting LBA in 4k space is returned and
+ * nblks are allocated *allocated* and *nfound is the number of blocks
+ * remaining in zone z_at from the LBA returned.
+ */
+static sector_t _blkalloc(struct megazone *megaz, u32 z_at, u32 flags,
+			  u32 nblks,
+			  u32 *nfound)
+{
+	u32 wptr = megaz->z_ptrs[z_at];
+	u32 gc_tflg = wptr & Z_WP_GC_TARGET;
+	sector_t found = 0;
+	u32 avail = 0;
+	int do_open_zone = 0;
+
+	wptr &= ~Z_WP_GC_TARGET;
+	if (wptr < Z_BLKSZ)
+		avail = Z_BLKSZ - wptr;
+
+#if 0 /* DEBUG START: Testing zm_write_pages() */
+	if (avail > 7)
+		avail = 7;
+#endif /* DEBUG END: Testing zm_write_pages() */
+
+	*nfound = avail;
+	if (nblks <= avail) {
+		u64 disk_zone = ((megaz->mega_nr * 1024) + z_at);
+
+		mutex_lock(&megaz->zp_lock);
+		found = (disk_zone * Z_BLKSZ) + wptr;
+		*nfound = nblks;
+		if (0 == megaz->z_ptrs[z_at])
+			do_open_zone = 1;
+
+		wptr += nblks;
+		megaz->zfree_count[z_at] -= nblks;
+		if (wptr == Z_BLKSZ)
+			megaz->discard_count += megaz->zfree_count[z_at];
+
+		wptr |= gc_tflg;
+		if (flags & Z_AQ_GC)
+			wptr |= Z_WP_GC_TARGET;
+
+		megaz->z_ptrs[z_at] = wptr;
+		mutex_unlock(&megaz->zp_lock);
+		if (do_open_zone)
+			dmz_open_zone(megaz, z_at);
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u16 _gc_tag = 1;
+
+/**
+ * Add a zone onto the GC queue -- and kick start the compact operation if
+ *  not already in progress.
+ */
+static int z_zone_compact_queue(struct megazone *megaz, u64 z_gc, int delay)
+{
+	unsigned long flags;
+	struct zoned *znd = megaz->znd;
+	int do_queue = 0;
+	int err = 0;
+	struct gc_state *gc_entry = ZDM_ALLOC(znd, sizeof(*gc_entry), KM_16);
+
+	if (!gc_entry) {
+		Z_ERR(znd, "No Memory for compact!!");
+		return -ENOMEM;
+	}
+	gc_entry->megaz = megaz;
+	gc_entry->z_gc = z_gc;
+	gc_entry->tag = _gc_tag++;
+	set_bit(DO_GC_NEW, &gc_entry->gc_flags);
+	znd->gc_backlog++;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	if (!znd->gc_active) {
+		znd->gc_active = gc_entry;
+		do_queue = 1;
+	}
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	if (do_queue) {
+		unsigned long tval = msecs_to_jiffies(delay);
+
+		queue_delayed_work(znd->gc_wq, &znd->gc_work, tval);
+
+		Z_DBG(znd, "%s: Queue GC: MZ# %d Z# %" PRIx64
+		      ", wp: %x, free %x - tag %u",
+		      __func__, megaz->mega_nr, z_gc, megaz->z_ptrs[z_gc],
+		      megaz->zfree_count[z_gc], gc_entry->tag);
+	}
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * Called periodically to see if GC needs to be done. ...
+ *
+ * FIXME: this is really too aggressive ...
+ *
+ */
+static int gc_compact_check(struct megazone *megaz, int delay)
+{
+	unsigned long flags;
+	int queued = 0;
+	int n_filled = 0;
+	int n_empty = 0;
+	int z_gc = megaz->z_data;
+	u16 top_roi = 0xFFFF;
+	u32 stale = 0;
+
+	Z_DBG(megaz->znd, "MZ# %u Checking zone range [%u,%u)",
+		megaz->mega_nr, z_gc, megaz->z_count);
+
+	if (megaz->meta_result)
+		goto out;
+
+	if (test_bit(ZF_FREEZE, &megaz->znd->flags))
+		goto out;
+
+	spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+	if (megaz->znd->gc_active)
+		queued = 1; /* alread queued */
+	spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+
+	if (queued)
+		goto out;
+
+	/* scan for most stale zone in MZ [top_roi] */
+	for (; z_gc < megaz->z_count; z_gc++) {
+		u32 wp = megaz->z_ptrs[z_gc] & Z_WP_VALUE_MASK;
+
+		if (wp == Z_BLKSZ)
+			stale += megaz->zfree_count[z_gc];
+
+		if (0 != (megaz->z_ptrs[z_gc] & Z_WP_GC_PENDING)) {
+			n_filled++;
+			continue;
+		}
+		if (is_ready_for_gc(megaz, z_gc)) {
+			u32 nfree = megaz->zfree_count[z_gc];
+
+			n_filled++;
+			if (0xFFFF == top_roi)
+				top_roi = z_gc;
+			else if (nfree > megaz->zfree_count[top_roi])
+				top_roi = z_gc;
+
+		} else if ((Z_BLKSZ - wp) < 0xff) {
+			n_filled++;
+		}
+		if (0 == megaz->z_ptrs[z_gc])
+			n_empty++;
+	}
+	megaz->z_gc_free = n_empty;
+
+	if (megaz->discard_count != stale) {
+		Z_ERR(megaz->znd, "repair running discard count %d <- %d",
+		      megaz->discard_count, stale);
+	}
+
+	/* determine the cut-off for GC based on MZ overall staleness */
+	if (top_roi != 0xFFFF) {
+		u32 state_metric = GC_PRIO_DEFAULT;
+		int pctfree = n_empty * 100 / megaz->z_count;
+
+		/*
+		 * -> at less than 5 zones free switch to critical
+		 * -> at less than 5% zones free switch to HIGH
+		 * -> at less than 25% free switch to LOW
+		 * -> high level is 'cherry picking' near empty zones
+		 * */
+		if (megaz->z_gc_free < 5)
+			state_metric = GC_PRIO_CRIT;
+		else if (pctfree < 5)
+			state_metric = GC_PRIO_HIGH;
+		else if (pctfree < 25)
+			state_metric = GC_PRIO_LOW;
+
+		if (megaz->zfree_count[top_roi] > state_metric) {
+			if (z_zone_compact_queue(megaz, top_roi, delay))
+				goto out;
+			queued = 1;
+		}
+	}
+out:
+	return queued;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_compact(struct gc_state *gc_entry)
+{
+	unsigned long flags;
+	int err = 0;
+	int do_meta_flush = 0;
+	struct megazone *megaz = gc_entry->megaz;
+
+	megaz->age = jiffies_64;
+	if (test_and_clear_bit(DO_GC_NEW, &gc_entry->gc_flags)) {
+		err = z_flush_bdev(megaz->znd);
+		if (err) {
+			gc_entry->result = err;
+			goto out;
+		}
+
+		mutex_lock(&megaz->zp_lock);
+		megaz->z_ptrs[gc_entry->z_gc] |= Z_WP_GC_FULL;
+		mutex_unlock(&megaz->zp_lock);
+
+		mutex_lock(&megaz->mz_io_mutex);
+		err = _journal_blocks_to_table(megaz, gc_entry->z_gc);
+		set_bit(DO_GC_NO_PURGE, &megaz->flags);
+		mutex_unlock(&megaz->mz_io_mutex);
+		if (err) {
+			Z_ERR(megaz->znd, "Failed to purge journal: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+
+		if (megaz->znd->gc_postmap.jcount > 0) {
+			Z_ERR(megaz->znd, "*** Unexpected data in postmap!!");
+			megaz->znd->gc_postmap.jcount = 0;
+		}
+
+		err = z_zone_gc_metadata_to_ram(gc_entry);
+		if (err) {
+			Z_ERR(megaz->znd,
+			      "Pre-load metadata to memory failed!! %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+		set_bit(DO_GC_PREPARE, &gc_entry->gc_flags);
+
+		if (!megaz->znd->gc_throttle)
+			return -EAGAIN;
+	}
+
+next_in_queue:
+	megaz->age = jiffies_64;
+	if (test_and_clear_bit(DO_GC_PREPARE, &gc_entry->gc_flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = z_zone_gc_read(gc_entry);
+		mutex_unlock(&megaz->mz_io_mutex);
+		if (err < 0) {
+			Z_ERR(megaz->znd,
+			      "z_zone_gc_chunk issue failure: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+		if (!megaz->znd->gc_throttle)
+			return -EAGAIN;
+	}
+
+	if (test_and_clear_bit(DO_GC_WRITE, &gc_entry->gc_flags)) {
+		mutex_lock(&megaz->mz_io_mutex);
+		err = z_zone_gc_write(gc_entry);
+		mutex_unlock(&megaz->mz_io_mutex);
+		if (err) {
+			Z_ERR(megaz->znd,
+			      "z_zone_gc_chunk issue failure: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+		if (!megaz->znd->gc_throttle)
+			return -EAGAIN;
+	}
+
+	if (test_and_clear_bit(DO_GC_META, &gc_entry->gc_flags)) {
+		z_do_copy_more(gc_entry);
+		goto next_in_queue;
+	}
+
+	megaz->age = jiffies_64;
+	if (test_and_clear_bit(DO_GC_COMPLETE, &gc_entry->gc_flags)) {
+		err = z_zone_gc_metadata_update(gc_entry);
+		gc_entry->result = err;
+		if (err) {
+			Z_ERR(megaz->znd, "Metadata error ... disable zone: %u",
+			      gc_entry->z_gc);
+		}
+		err = gc_finalize(gc_entry);
+		if (err) {
+			Z_ERR(megaz->znd, "GC: Failed to finalize: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+
+		gc_verify_cache(megaz, gc_entry->z_gc);
+
+		mutex_lock(&megaz->mz_io_mutex);
+		err = _journal_blocks_to_table(megaz, gc_entry->z_gc);
+		mutex_unlock(&megaz->mz_io_mutex);
+		if (err) {
+			Z_ERR(megaz->znd, "Failed to purge journal: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+
+		/* Release the zones for writing */
+		dmz_reset_wp(megaz, gc_entry->z_gc);
+
+		mutex_lock(&megaz->zp_lock);
+		megaz->z_ptrs[gc_entry->z_gc] = 0;
+		megaz->z_commit[gc_entry->z_gc] = 0;
+		megaz->discard_count -= megaz->zfree_count[gc_entry->z_gc];
+		megaz->zfree_count[gc_entry->z_gc] = Z_BLKSZ;
+
+		megaz->z_gc_free++;
+		if (megaz->z_gc_resv & Z_WP_GC_ACTIVE)
+			megaz->z_gc_resv = gc_entry->z_gc;
+		else if (megaz->z_meta_resv & Z_WP_GC_ACTIVE)
+			megaz->z_meta_resv = gc_entry->z_gc;
+
+		mutex_unlock(&megaz->zp_lock);
+
+		Z_ERR(megaz->znd,
+		      "GC %d: MZ# %d, z#0x%x, wp:%08x, free:%x finished.",
+		      gc_entry->tag, megaz->mega_nr, gc_entry->z_gc,
+		      megaz->z_ptrs[gc_entry->z_gc],
+		      megaz->zfree_count[gc_entry->z_gc]);
+
+		spin_lock_irqsave(&megaz->znd->gc_lock, flags);
+		megaz->znd->gc_backlog--;
+		megaz->znd->gc_active = NULL;
+		spin_unlock_irqrestore(&megaz->znd->gc_lock, flags);
+
+		ZDM_FREE(megaz->znd, gc_entry, sizeof(*gc_entry), KM_16);
+
+		set_bit(DO_JOURNAL_MOVE, &megaz->flags);
+		set_bit(DO_MEMPOOL, &megaz->flags);
+		set_bit(DO_SYNC, &megaz->flags);
+		do_meta_flush = 1;
+	}
+out:
+	clear_bit(DO_GC_NO_PURGE, &megaz->flags);
+	if (do_meta_flush) {
+		struct zoned *znd = megaz->znd;
+
+		if (!work_pending(&megaz->meta_work))
+			queue_work(znd->meta_wq, &megaz->meta_work);
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void gc_work_task(struct work_struct *work)
+{
+	struct gc_state *gc_entry = NULL;
+	unsigned long flags;
+	struct zoned *znd;
+	int err;
+
+	if (!work)
+		return;
+
+	znd = container_of(to_delayed_work(work), struct zoned, gc_work);
+	if (!znd)
+		return;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	if (znd->gc_active)
+		gc_entry = znd->gc_active;
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	if (!gc_entry) {
+		Z_ERR(znd, "ERROR: gc_active not set!");
+		return;
+	}
+
+	err = z_zone_gc_compact(gc_entry);
+	if (-EAGAIN == err) {
+		unsigned long tval = msecs_to_jiffies(3);
+
+		queue_delayed_work(znd->gc_wq, &znd->gc_work, tval);
+	} else {
+		on_timeout_activity(znd);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_reserved(struct megazone *megaz, const u32 z_pref)
+{
+	const u32 gc   = megaz->z_gc_resv & Z_WP_VALUE_MASK;
+	const u32 meta = megaz->z_meta_resv & Z_WP_VALUE_MASK;
+
+	return (gc == z_pref || meta == z_pref) ? 1 : 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_immediate(struct megazone *megaz)
+{
+	int can_retry = 0;
+	struct zoned *znd = megaz->znd;
+	int queued;
+
+	megaz->znd->gc_throttle = 1;
+
+	if (delayed_work_pending(&znd->gc_work)) {
+		Z_ERR(znd, " ... pre-flush GC queue.");
+		mod_delayed_work(znd->gc_wq, &znd->gc_work, 0);
+		mutex_unlock(&megaz->mz_io_mutex);
+		flush_delayed_work(&znd->gc_work);
+		mutex_lock(&megaz->mz_io_mutex);
+	}
+
+	Z_ERR(znd, " ... try to queue GC.");
+
+	queued = gc_compact_check(megaz, 0);
+	if (!queued)
+		goto out;
+
+	Z_ERR(znd, " ... GC queued.");
+
+	if (delayed_work_pending(&znd->gc_work)) {
+		can_retry = 1;
+		mod_delayed_work(znd->gc_wq, &znd->gc_work, 0);
+		mutex_unlock(&megaz->mz_io_mutex);
+		flush_delayed_work(&znd->gc_work);
+		mutex_lock(&megaz->mz_io_mutex);
+		Z_ERR(znd, " ... GC flushed.");
+	}
+
+out:
+	megaz->znd->gc_throttle = 0;
+
+	return can_retry;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 z_acquire(struct megazone *megaz, u32 flags, u32 nblks, u32 *nfound)
+{
+	sector_t found = 0;
+	u32 z_pref = megaz->z_current;
+
+	found = _blkalloc(megaz, z_pref, flags, nblks, nfound);
+	if (found || *nfound)
+		goto out;
+
+	/* no space left in zone .. explicitly close it */
+	dmz_close_zone(megaz, z_pref);
+
+	if (megaz->z_gc_free < 5) {
+		Z_ERR(megaz->znd, "... alloc - gc low on free space.");
+		gc_immediate(megaz);
+	}
+
+retry:
+	/* continue down the round-robin path */
+	for (z_pref = megaz->z_current + 1; z_pref < megaz->z_count; z_pref++) {
+		if (is_reserved(megaz, z_pref))
+			continue;
+
+		found = _blkalloc(megaz, z_pref, flags, nblks, nfound);
+		if (found || *nfound) {
+			megaz->z_current = z_pref;
+			megaz->z_gc_free--;
+			goto out;
+		}
+	}
+
+	/* If we didn't get a new pref zone ... walk from the start */
+	for (z_pref = megaz->z_data; z_pref < megaz->z_current; z_pref++) {
+		if (is_reserved(megaz, z_pref))
+			continue;
+
+		found = _blkalloc(megaz, z_pref, flags, nblks, nfound);
+		if (found || *nfound) {
+			megaz->z_current = z_pref;
+			megaz->z_gc_free--;
+			goto out;
+		}
+	}
+
+	if (flags & Z_AQ_GC) {
+		u32 gresv = megaz->z_gc_resv & Z_WP_VALUE_MASK;
+
+		Z_ERR(megaz->znd, "MZ# %u: Using GC Reserve (%u)",
+			megaz->mega_nr, gresv);
+		found = _blkalloc(megaz, gresv, flags, nblks, nfound);
+		megaz->z_gc_resv |= Z_WP_GC_ACTIVE;
+	}
+
+	if (flags & Z_AQ_META) {
+		int can_retry;
+		u32 mresv;
+
+		mresv = megaz->z_meta_resv & Z_WP_VALUE_MASK;
+
+		Z_ERR(megaz->znd, "MZ# %u: Using META Reserve (%u)",
+			megaz->mega_nr, megaz->z_meta_resv);
+
+		can_retry = gc_immediate(megaz);
+		if (can_retry)
+			goto retry;
+
+		found = _blkalloc(megaz, mresv, flags, nblks, nfound);
+	}
+
+out:
+	if (!found && (0 == *nfound)) {
+		Z_ERR(megaz->znd, "%s: -> MZ# %u: Out of space.",
+		       __func__, megaz->mega_nr);
+
+		if (gc_immediate(megaz))
+			goto retry;
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void do_free_mapped(struct megazone *megaz, struct map_pg *mapped)
+{
+	if (!test_bit(DO_GC_NO_PURGE, &megaz->flags)) {
+		int entry;
+
+		mapped_lock(mapped);
+		if (test_bit(IS_DIRTY, &mapped->flags) ||
+		    (mapped->refcount.counter > 0)) {
+			Z_ERR(megaz->znd, "%s: Cannot free: %"PRIx64,
+				__func__, mapped->lba);
+			mapped_unlock(mapped);
+			return;
+		}
+
+		spin_lock(&megaz->map_pool_lock);
+		list_del(&(mapped->inpool));
+		mapped->inpool.prev = mapped->inpool.next = NULL;
+		spin_unlock(&megaz->map_pool_lock);
+
+		if (mapped->mdata) {
+			ZDM_FREE(megaz->znd, mapped->mdata, Z_C4K, PG_27);
+			megaz->incore_count--;
+		}
+		mapped_unlock(mapped);
+
+		entry = mapped->lba & 0xFFFF;
+		if (2 == (mapped->lba / Z_BLKSZ) % 1024) {
+			struct map_pg *mentry = megaz->reversetm[entry];
+
+			ZDM_FREE(megaz->znd, mentry, sizeof(*mentry), KM_20);
+			megaz->reversetm[entry] = NULL;
+		} else {
+			struct map_pg *mentry = megaz->sectortm[entry];
+
+			ZDM_FREE(megaz->znd, mentry, sizeof(*mentry), KM_20);
+			megaz->sectortm[entry] = NULL;
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int expunge_oldest_pages(struct megazone *megaz, int count)
+{
+	struct list_head *_pages = &(megaz->smtpool);
+	struct map_pg *_oldpg;
+	struct map_pg *t_pg;
+	struct map_pg *oldest = NULL;
+	u64 tpurge = msecs_to_jiffies(MEM_PURGE_MSECS);
+	u64 tnow = jiffies_64;
+
+	if (list_empty(_pages))
+		return 0;
+	tpurge = (tpurge < tnow) ? tnow - tpurge : 0;
+	list_for_each_entry_safe_reverse(_oldpg, t_pg, _pages, inpool) {
+		if (count-- > 0) {
+			oldest = _oldpg;
+
+			if (test_bit(IS_DIRTY, &oldest->flags)) {
+				write_if_dirty(megaz, oldest, 0);
+			} else {
+				if (tpurge &&
+				    time_before64(oldest->age, tpurge) &&
+				    (0 == oldest->refcount.counter))
+					do_free_mapped(megaz, oldest);
+			}
+		}
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int load_crc_meta_pg(struct megazone *megaz, struct crc_pg *pblock,
+			    u64 pg_lba, u16 crc16, int use_wq)
+{
+	int err = 0;
+
+	if (pblock && !pblock->cdata) {
+		u16 *data = ZDM_ALLOC(megaz->znd, Z_C4K, PG_17);
+
+		if (!data) {
+			Z_ERR(megaz->znd, "Out of memory");
+			err = -ENOMEM;
+			goto out;
+		}
+		if (pg_lba) {
+			u16 check;
+			int count = 1;
+
+			Z_DBG(megaz->znd, "get_meta_pg_crc:  %" PRIx64
+					  " from lba: %" PRIx64,
+					  pblock->lba, pg_lba);
+
+			if (warn_bad_lba(megaz, pg_lba))
+				Z_ERR(megaz->znd,
+				      "Bad CRC pg %" PRIx64, pg_lba);
+
+			err = read_block(megaz->znd->ti, DM_IO_KMEM, data,
+					pg_lba, count, use_wq);
+			if (err) {
+				ZDM_FREE(megaz->znd, data, Z_C4K, PG_17);
+
+				Z_ERR(megaz->znd, "Read of CRC page @%" PRIx64
+				      " failed: %d", pg_lba, err);
+				goto out;
+			}
+			check = crc_md_le16(data, Z_CRC_4K);
+			if (check == crc16)
+				pblock->last_write = pg_lba;
+			else
+				Z_ERR(megaz->znd,
+				      "CRC PG: %" PRIx64 " from %" PRIx64
+				      " [%04x != %04x] last write: %" PRIx64,
+				      pblock->lba, pg_lba, le16_to_cpu(check),
+				      le16_to_cpu(crc16), pblock->last_write);
+		} else {
+			memset(data, 0, Z_C4K);
+		}
+		pblock->cdata = data;
+	}
+	if (pblock)
+		pblock->age = jiffies_64;
+
+out:
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct crc_pg *get_meta_pg_crc(struct megazone *megaz,
+				       struct map_addr *madr,
+				       int is_to, int use_wq)
+{
+	struct crc_pg *pblock = NULL;
+	int pg_no = (madr->mz_off & 0xFFFF) / 2048;
+	u16 crc16 = 0;
+	u64 pg_lba = 0ul;
+	int is_rtz = is_reverse_table_zone(megaz, madr);
+	int err;
+
+	if (is_rtz == is_to)
+		Z_ERR(megaz->znd, "CRC: is_rtz == is_to [%d != %d] lba: %"
+		      PRIx64, is_rtz, is_to, madr->dm_s);
+
+	if (0 == is_to) {
+		pblock = &megaz->rtm_crc[pg_no];
+		mutex_lock(&megaz->mapkey_lock);
+		crc16 = megaz->bmkeys->rtm_crc_pg[pg_no];
+		pg_lba = le64_to_cpu(megaz->bmkeys->rtm_crc_lba[pg_no]);
+		mutex_unlock(&megaz->mapkey_lock);
+	} else {
+		pblock = &megaz->stm_crc[pg_no];
+		mutex_lock(&megaz->mapkey_lock);
+		crc16 = megaz->bmkeys->stm_crc_pg[pg_no];
+		pg_lba = le64_to_cpu(megaz->bmkeys->stm_crc_lba[pg_no]);
+		mutex_unlock(&megaz->mapkey_lock);
+	}
+
+	if (pblock->last_write && pg_lba != pblock->last_write)
+		Z_ERR(megaz->znd, "load1 %"PRIx64", last %"PRIx64,
+		      pg_lba, pblock->last_write);
+
+	err = load_crc_meta_pg(megaz, pblock, pg_lba, crc16, use_wq);
+	if (err)
+		pblock = NULL;
+
+	return pblock;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int write_if_dirty(struct megazone *megaz, struct map_pg *oldest,
+			  int use_wq)
+{
+	int rcode = 0;
+	struct map_addr maddr;
+
+	mapped_lock(oldest);
+
+	if (test_bit(IS_DIRTY, &oldest->flags)
+	    && (0 == oldest->refcount.counter)) {
+
+		u32 nblks = 1;
+		u32 nfound = 0;
+		u64 dm_s = oldest->lba;
+		u64 lba;
+
+		/* we need the maddr to acquire a free block in
+		 * the correct megazone */
+		map_addr_calc(dm_s, &maddr);
+		lba = z_acquire(megaz, Z_AQ_META, nblks, &nfound);
+		if (lba && nfound) {
+			int rcwrt;
+			int count = 1;
+			void *data = oldest->mdata;
+			int crce = (maddr.mz_off & 0xFFFF) % 2048;
+			struct crc_pg *pblock;
+			int is_to = !is_reverse_table_zone(megaz, &maddr);
+			u32 crcb4;
+
+			Z_DBG(megaz->znd, "%" PRIx64 " -> is map to %d",
+			      dm_s, is_to);
+
+			pblock = get_meta_pg_crc(megaz, &maddr, is_to, use_wq);
+			if (!pblock) {
+				Z_ERR(megaz->znd,
+				      "%s: Out of space for metadata?",
+				      __func__);
+				rcode = -ENOSPC;
+				goto out;
+			}
+
+			atomic_inc(&pblock->refcount);
+			mutex_lock(&pblock->lock_pg);
+			pblock->cdata[crce] = crc_md_le16(data, Z_CRC_4K);
+			crcb4 = crcpg(data);
+			set_bit(IS_DIRTY, &pblock->flags);
+			pblock->age = jiffies_64;
+			mutex_unlock(&pblock->lock_pg);
+			atomic_dec(&pblock->refcount);
+
+			Z_DBG(megaz->znd,
+			      "write_page: %" PRIx64 " -> %" PRIx64,
+			       dm_s, lba);
+
+			rcwrt = write_block(megaz->znd->ti, DM_IO_KMEM, data,
+					    lba, count, use_wq);
+
+			oldest->age = jiffies_64;
+			oldest->last_write = lba;
+
+			if (rcwrt) {
+				Z_ERR(megaz->znd,
+				      "write_page: %" PRIx64 " -> %" PRIx64
+				      " ERR: %d", oldest->lba, lba, rcwrt);
+				rcode = rcwrt;
+				goto out;
+			}
+
+			Z_DBG(megaz->znd, "meta: %" PRIx64 " -> %" PRIx64
+			      " (table entry)", dm_s, lba);
+
+			mapped_unlock(oldest);
+			rcwrt = z_mapped_addmany(megaz, dm_s, lba, nfound);
+			if (!rcwrt && crcpg(data) == crcb4)
+				clear_bit(IS_DIRTY, &oldest->flags);
+
+			mapped_lock(oldest);
+			if (rcwrt) {
+				Z_ERR(megaz->znd, "%s: Journal MANY failed.",
+				      __func__);
+				rcode = rcwrt;
+				goto out;
+			}
+
+		} else {
+			Z_ERR(megaz->znd, "%s: Out of space for metadata?",
+			      __func__);
+			rcode = -ENOSPC;
+			goto out;
+		}
+	}
+out:
+	mapped_unlock(oldest);
+
+	if (rcode)
+		megaz->meta_result = rcode;
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int fpages(struct megazone *megaz, int allowed_pages)
+{
+	int rc = 0;
+
+	if (megaz->incore_count > allowed_pages) {
+		int count = megaz->incore_count - allowed_pages;
+
+		rc = expunge_oldest_pages(megaz, count);
+		if (rc)
+			Z_ERR(megaz->znd, "%s: Failed to remove oldest pages!",
+				__func__);
+	}
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int write_if_pg_blk_dirty(struct megazone *megaz,
+				 struct crc_pg *pblock,
+				 u64 *paddrs, u16 *crc16, int pg_no)
+{
+	int rcode = 0;
+
+	if (!pblock->cdata)
+		goto out;	/* nothing to write. */
+
+	if (test_bit(IS_DIRTY, &pblock->flags)) {
+		u32 nblks = 1;
+		u32 nfound = 0;
+		u64 dm_s = pblock->lba;
+		u64 lba;
+
+		lba = z_acquire(megaz, Z_AQ_META, nblks, &nfound);
+		if (lba && nfound) {
+			int count = 1;
+			int use_wq = 0;
+			void *data = pblock->cdata;
+			u32 crcb4 = crcpg(data);
+
+			Z_DBG(megaz->znd,
+			      "write_crc_page: %d: %" PRIx64 " -> %"
+			      PRIx64, pg_no, pblock->lba, lba);
+
+			rcode = write_block(megaz->znd->ti, DM_IO_KMEM,
+					    data, lba, count, use_wq);
+			if (rcode) {
+				Z_ERR(megaz->znd,
+				      "%s: %d: %" PRIx64 " -> %" PRIx64
+				      " ERR: %d", __func__, pg_no,
+				      pblock->lba, lba, rcode);
+				goto out;
+			}
+
+			mutex_lock(&pblock->lock_pg);
+			mutex_lock(&megaz->mapkey_lock);
+			paddrs[pg_no] = cpu_to_le64(lba);
+			crc16[pg_no] = crc_md_le16(data, Z_CRC_4K);
+			mutex_unlock(&megaz->mapkey_lock);
+			pblock->last_write = lba;
+			pblock->age = jiffies_64;
+			if (crcpg(data) == crcb4)
+				clear_bit(IS_DIRTY, &pblock->flags);
+			mutex_unlock(&pblock->lock_pg);
+
+			Z_DBG(megaz->znd, "meta: %" PRIx64 " -> %" PRIx64
+			      " (crc entry)", dm_s, lba);
+
+			rcode = z_mapped_addmany(megaz, dm_s, lba, nfound);
+			if (rcode) {
+				Z_ERR(megaz->znd, "%s: Journal MANY failed.",
+				      __func__);
+				goto out;
+			}
+
+		} else {
+			Z_ERR(megaz->znd, "%s: Out of space for metadata?",
+			      __func__);
+			rcode = -ENOSPC;
+			goto out;
+		}
+	}
+out:
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int write_purge_crc_pages(struct megazone *megaz, struct crc_pg *pgs,
+				 u64 *lbas, u16 *crcs)
+{
+	int err = 0;
+	int pg_no = 0;
+	u64 tpurge = msecs_to_jiffies(MEM_PURGE_MSECS);
+	u64 tnow = jiffies_64;
+
+	tpurge = (tpurge < tnow) ? tnow - tpurge : 0;
+	for (pg_no = 0; pg_no < 32; pg_no++) {
+		int sync_err;
+		struct crc_pg *pblock = &pgs[pg_no];
+
+		atomic_inc(&pblock->refcount);
+		sync_err = write_if_pg_blk_dirty(megaz, pblock, lbas,
+						 crcs, pg_no);
+		atomic_dec(&pblock->refcount);
+		if (sync_err)
+			err = sync_err;
+		if ((pblock->cdata)
+		    && (0 != tpurge)
+		    && (0 == pblock->refcount.counter)
+		    && time_before64(pblock->age, tpurge)
+		    && !test_bit(DO_GC_NO_PURGE, &megaz->flags))
+			ZDM_FREE(megaz->znd, pblock->cdata, Z_C4K, PG_17);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int sync_crc_pages(struct megazone *megaz)
+{
+	int err = 0;
+	int rcode;
+
+	rcode = write_purge_crc_pages(megaz, megaz->stm_crc,
+				      megaz->bmkeys->stm_crc_lba,
+				      megaz->bmkeys->stm_crc_pg);
+	if (rcode)
+		err = rcode;
+	rcode = write_purge_crc_pages(megaz, megaz->rtm_crc,
+				      megaz->bmkeys->rtm_crc_lba,
+				      megaz->bmkeys->rtm_crc_pg);
+	if (rcode)
+		err = rcode;
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct map_pg *get_map_table_entry(struct megazone *megaz, u64 lba,
+				       int is_to)
+{
+	struct map_pg *found = NULL;
+	struct map_pg **table = is_to ? megaz->sectortm : megaz->reversetm;
+	u64 entry = lba & 0xFFFF;
+
+	found = table[entry];
+	if (!found) {
+		/* if we didn't find one .. create it */
+		found = ZDM_ALLOC(megaz->znd, sizeof(*found), KM_20);
+		if (found) {
+			found->lba = lba;
+			found->mdata = NULL;
+
+			mutex_init(&found->md_lock);
+			table[entry] = found;
+		} else {
+			Z_ERR(megaz->znd, "NO MEM for mapped_t !!!");
+		}
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void megazone_free_one(struct megazone *megaz)
+{
+	if (megaz) {
+		struct zoned *znd = megaz->znd;
+		size_t mapsz = Z_BLKSZ * sizeof(struct map_pg *);
+
+		if (megaz->sectortm)
+			ZDM_FREE(megaz->znd, megaz->sectortm, mapsz, VM_21);
+		if (megaz->reversetm)
+			ZDM_FREE(megaz->znd, megaz->reversetm, mapsz, VM_22);
+
+		if (megaz->sync_io) {
+			size_t ssz = sizeof(*megaz->sync_io);
+
+			ZDM_FREE(megaz->znd, megaz->sync_io, ssz, MP_SIO);
+			megaz->z_ptrs = NULL;
+			megaz->zfree_count = NULL;
+			megaz->bmkeys = NULL;
+		}
+		if (megaz->cow_block)
+			ZDM_FREE(znd, megaz->cow_block, Z_C4K, PG_02);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int megazone_init_one_b(struct megazone *megaz)
+{
+	int rcode = 0;
+	struct zoned *znd = megaz->znd;
+
+	megaz->sync_io = ZDM_ALLOC(znd, sizeof(*megaz->sync_io), MP_SIO);
+	megaz->sectortm = ZDM_CALLOC(znd, Z_BLKSZ, sizeof(struct map_pg *),
+				     VM_21);
+	megaz->reversetm = ZDM_CALLOC(znd, Z_BLKSZ, sizeof(struct map_pg *),
+				      VM_22);
+	if (!megaz->sync_io || !megaz->sectortm || !megaz->reversetm) {
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	megaz->bmkeys = &megaz->sync_io->bmkeys;
+	megaz->z_ptrs = megaz->sync_io->z_ptrs;
+	megaz->zfree_count = megaz->sync_io->zfree;
+	megaz->incore_count = 0;
+	megaz->last_w = ~0ul;
+	megaz->bmkeys->sig[0] = Z_KEY_SIG;
+	megaz->bmkeys->sig[1] = cpu_to_le64(Z_KEY_SIG);
+	megaz->bmkeys->magic  = cpu_to_le64(Z_TABLE_MAGIC);
+	megazone_fill_lam(megaz, &megaz->logical_map);
+	if (megaz->z_count > znd->mz_provision) {
+		u64 lba = megaz->logical_map.crc_low;
+		int znr;
+
+		for (znr = 0; znr < megaz->z_count; znr++)
+			megaz->zfree_count[znr] = Z_BLKSZ;
+
+		for (; znr < 1024; znr++) {
+			megaz->z_ptrs[znr] = 0xffffffff;
+			megaz->zfree_count[znr] = 0;
+		}
+
+		megaz->z_gc_free = megaz->z_count - 2;
+		megaz->z_meta_resv = megaz->z_count - 2;
+		megaz->z_gc_resv = megaz->z_count - 1;
+		megaz->z_gc_free -= 2;
+		megaz->z_data = 0;
+		if (megaz->mega_nr == 0) {
+			megaz->z_data++;
+			megaz->z_gc_free--;
+		}
+		megaz->z_current = megaz->z_data;
+
+		for (znr = 0; znr < 32; znr++) {
+			megaz->rtm_crc[znr].lba = lba + znr;
+			megaz->stm_crc[znr].lba = lba + znr + MZKY_NCRC;
+
+			mutex_init(&megaz->rtm_crc[znr].lock_pg);
+			mutex_init(&megaz->stm_crc[znr].lock_pg);
+		}
+	}
+
+out:
+	if (rcode)
+		megazone_free_one(megaz);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int megazone_init_one_a(struct megazone *megaz)
+{
+	int rcode = 0;
+
+	megaz->flags = 0;
+
+	INIT_LIST_HEAD(&megaz->jlist);
+	INIT_LIST_HEAD(&megaz->smtpool);
+
+	spin_lock_init(&megaz->map_pool_lock);
+	spin_lock_init(&megaz->jlock);
+
+	mutex_init(&megaz->zp_lock);
+	mutex_init(&megaz->discard_lock);
+	mutex_init(&megaz->mz_io_mutex);
+	mutex_init(&megaz->mapkey_lock);
+	INIT_WORK(&megaz->meta_work, meta_work_task);
+
+	megaz->incore_count = 0;
+	megaz->last_w = ~0ul;
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int megazone_init(struct zoned *znd)
+{
+	int rcode = 0;
+	struct megazone *megazones;
+	u32 iter;
+
+	megazones = ZDM_CALLOC(znd, znd->mega_zones_count,
+				sizeof(*megazones), KM_25);
+	if (!megazones) {
+		Z_ERR(znd, "No memory for megazone array.");
+		rcode = -ENOMEM;
+		goto out;
+	}
+	znd->z_mega = megazones;
+
+	for (iter = 0; iter < znd->mega_zones_count; iter++) {
+		u64 remaining = znd->device_zone_count - (1024 * iter);
+		struct megazone *megaz = &megazones[iter];
+
+		megaz->mega_nr = iter;
+		megaz->znd = znd;
+		megaz->z_count = remaining < 1024 ? remaining : 1024;
+
+		rcode = megazone_init_one_a(megaz);
+		if (rcode) {
+			Z_ERR(megaz->znd,
+			      "Catastrophic ERR megazone init A failed!!");
+			goto out;
+		}
+	}
+
+	for (iter = 0; iter < znd->mega_zones_count; iter++) {
+		struct megazone *megaz = &megazones[iter];
+
+		rcode = megazone_init_one_b(megaz);
+		if (rcode) {
+			Z_ERR(megaz->znd,
+			      "Catastrophic ERR megazone init B failed!!");
+			goto out;
+		}
+	}
+
+out:
+	if (rcode)
+		megazone_free_all(znd);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int _sync_page(struct megazone *megaz, struct map_pg *mapped,
+		      int *is_dirty)
+{
+	int err = 0;
+
+	if (mapped) {
+		if (test_bit(IS_DIRTY, &mapped->flags)) {
+			BUG_ON(!mapped->mdata);
+			*is_dirty |= 1;
+		}
+		err = write_if_dirty(megaz, mapped, 0);
+		if (err)
+			Z_ERR(megaz->znd, "%s: lut write failed", __func__);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int _sync_mapped_pages(struct megazone *megaz)
+{
+	struct list_head *_pages = &(megaz->smtpool);
+	struct map_pg *_oldpg;
+	struct map_pg *t_pg;
+	struct map_pg *oldest = NULL;
+
+	if (list_empty(_pages))
+		return 0;
+
+	list_for_each_entry_safe_reverse(_oldpg, t_pg, _pages, inpool) {
+		oldest = _oldpg;
+		write_if_dirty(megaz, oldest, 0);
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_sync_tables(struct megazone *megaz, int need_table_push)
+{
+	int err = 0;
+	int is_dirty;
+
+	if (test_and_clear_bit(DO_JOURNAL_MOVE, &megaz->flags) ||
+	    need_table_push) {
+		err = _journal_blocks_to_table(megaz, MAX_ZONES_PER_MZ);
+		if (err)
+			goto out;
+	}
+	_sync_mapped_pages(megaz);
+	sync_crc_pages(megaz);
+	do {
+		int entry;
+
+		is_dirty = 0;
+		for (entry = 0; entry < Z_BLKSZ; entry++) {
+			struct map_pg *mentry = megaz->sectortm[entry];
+
+			if (mentry && mentry->mdata) {
+				err = _sync_page(megaz, mentry, &is_dirty);
+				if (err)
+					goto out;
+			}
+			mentry = megaz->reversetm[entry];
+			if (mentry && mentry->mdata) {
+				err = _sync_page(megaz, mentry, &is_dirty);
+				if (err)
+					goto out;
+			}
+		}
+	} while (is_dirty);
+out:
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * Release resources as ZDM is being removed
+ */
+static void mapped_free(struct megazone *megaz, struct map_pg *mapped)
+{
+	if (mapped) {
+		mapped_lock(mapped);
+		BUG_ON(test_bit(IS_DIRTY, &mapped->flags));
+		if (mapped->mdata) {
+			ZDM_FREE(megaz->znd, mapped->mdata, Z_C4K, PG_27);
+			megaz->incore_count--;
+		}
+		mapped_unlock(mapped);
+		ZDM_FREE(megaz->znd, mapped, sizeof(*mapped), KM_20);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int flush_sector_map(struct megazone *megaz)
+{
+	int ii;
+	int err = 0;
+
+	if (!megaz->sectortm)
+		return err;
+
+	for (ii = 0; ii < Z_BLKSZ; ii++) {
+		if (megaz->sectortm[ii] && megaz->sectortm[ii]->mdata)
+			err |= write_if_dirty(megaz, megaz->sectortm[ii], 1);
+	}
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int flush_reverse_map(struct megazone *megaz)
+{
+	int ii;
+	int err = 0;
+
+	if (!megaz->reversetm)
+		return err;
+
+	for (ii = 0; ii < Z_BLKSZ; ii++) {
+		if (megaz->reversetm[ii] && megaz->reversetm[ii]->mdata)
+			err |= write_if_dirty(megaz, megaz->reversetm[ii], 1);
+	}
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int release_table_pages(struct megazone *megaz)
+{
+	int entry;
+	int err;
+
+	INIT_LIST_HEAD(&megaz->smtpool);
+
+	err = flush_sector_map(megaz);
+	if (err)
+		goto out;
+
+	err = flush_reverse_map(megaz);
+	if (err)
+		goto out;
+
+	if (megaz->sectortm) {
+		for (entry = 0; entry < Z_BLKSZ; entry++) {
+			mapped_free(megaz, megaz->sectortm[entry]);
+			megaz->sectortm[entry] = NULL;
+		}
+	}
+
+	if (megaz->reversetm) {
+		for (entry = 0; entry < Z_BLKSZ; entry++) {
+			mapped_free(megaz, megaz->reversetm[entry]);
+			megaz->reversetm[entry] = NULL;
+		}
+	}
+
+out:
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int release_journal_pages(struct megazone *megaz)
+{
+	struct list_head *_jhead = &(megaz->jlist);
+	struct map_cache *jrnl;
+	struct map_cache *jtmp;
+
+	if (list_empty(_jhead))
+		return 0;
+
+	list_for_each_entry_safe(jrnl, jtmp, _jhead, jlist) {
+		/** move all the journal entries into the SLT */
+		spin_lock(&megaz->jlock);
+		list_del(&jrnl->jlist);
+		ZDM_FREE(megaz->znd, jrnl->jdata, Z_C4K, PG_08);
+		ZDM_FREE(megaz->znd, jrnl, sizeof(*jrnl), KM_07);
+		spin_unlock(&megaz->jlock);
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void megazone_free_all(struct zoned *znd)
+{
+	if (znd->z_mega) {
+		u32 iter;
+
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			struct megazone *megaz = &znd->z_mega[iter];
+
+			if (megaz) {
+				release_table_pages(megaz);
+				release_journal_pages(megaz);
+				megazone_free_one(megaz);
+			}
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void megazone_flush_all(struct zoned *znd)
+{
+	set_bit(ZF_FREEZE, &znd->flags);
+	if (znd->z_mega) {
+		u32 iter;
+
+		znd->gc_throttle = 1;
+		mod_delayed_work(znd->gc_wq, &znd->gc_work, 0);
+		flush_delayed_work(&znd->gc_work);
+
+		for (iter = 0; iter < znd->mega_zones_count; iter++) {
+			struct megazone *megaz = &znd->z_mega[iter];
+
+			if (megaz) {
+				clear_bit(DO_GC_NO_PURGE, &megaz->flags);
+				set_bit(DO_JOURNAL_MOVE, &megaz->flags);
+				set_bit(DO_MEMPOOL, &megaz->flags);
+				set_bit(DO_SYNC, &megaz->flags);
+				queue_work(znd->meta_wq, &megaz->meta_work);
+			}
+		}
+		flush_workqueue(znd->meta_wq);
+		mod_delayed_work(znd->gc_wq, &znd->gc_work, 0);
+		flush_delayed_work(&znd->gc_work);
+		znd->gc_throttle = 0;
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void megazone_destroy(struct zoned *znd)
+{
+	set_bit(ZF_FREEZE, &znd->flags);
+	if (znd->z_mega) {
+		size_t zmsz = znd->mega_zones_count * sizeof(*znd->z_mega);
+
+		megazone_flush_all(znd);
+		megazone_free_all(znd);
+		ZDM_FREE(znd, znd->z_mega, zmsz, KM_25);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * dm_s is a logical sector that maps 1:1 to the whole disk in 4k blocks
+ * Here the logical LBA and field are calculated for the lookup table
+ * where the physical LBA can be read from disk.
+ */
+static int map_addr_calc(u64 dm_s, struct map_addr *out)
+{
+	u64 zone_nr = dm_s / Z_BLKSZ;
+	u64 mz_nr = zone_nr / 1024;
+
+	/* offset: blocks from base of mega zone */
+	u64 offset = dm_s - (mz_nr * Z_BLKSZ * 1024);
+
+	/* In SLT which block will hold the u32 mapping?
+	 *  -> dm_tbl_s set [0-65536) */
+	u64 dm_tbl_s = offset / 1024;
+
+	/* In that block, which index (as an array of 1024 entries?
+	 *  -> entry in the set [0-1024) */
+	u64 entry = offset - (dm_tbl_s * 1024);
+
+	/* Logically what LBA is the table block? -> lut_s */
+	u64 b_addr = dm_tbl_s + (mz_nr * Z_BLKSZ * 1024);
+
+	/* NOTE: CRC pages start at b_addr+0 */
+	/*       REVERSE map table starts at b_addr + (1 * zone size) */
+	/*       FORWARD map table starts at b_addr + (2 * zone size) */
+	b_addr += Z_BLKSZ;	/* zone 0 is not mapped */
+
+	out->dm_s = dm_s;
+	out->z_id = zone_nr;
+	out->mz_id = mz_nr;
+
+	/* lookup table mapping: */
+	out->mz_off = offset;	   /* (0->65536*1024] */
+	out->offentry = entry;          /* (0->1024] */
+	out->lut_r = b_addr + Z_BLKSZ;
+	out->lut_s = b_addr + (2 * Z_BLKSZ);
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/* data sector (fs layer) -> mapper sector */
+/**
+ * s_nr is a logical sector that maps 1:1 to
+ * to the subset of the disk presented as a block device to
+ * an upper layer (typically a file-system or raid).
+ *
+ * read/write to this level are mapped onto dm_s
+ */
+static int map_addr_to_zdm(struct zoned *znd, u64 s_nr, struct map_addr *out)
+{
+#define FIRST_MZSZ      ((0x400ul - (znd->mz_provision + 1)) << 16)
+#define FIRST_MZ_OFF               ((znd->mz_provision + 1)  << 16)
+#define LOW_CHUNK       ((0x400ul -  znd->mz_provision)      << 16)
+#define MAP_BUMP                    (znd->mz_provision       << 16)
+#define EXPAND           (0x400ul                            << 16)
+
+	u64 dm_s = 0;
+	u64 s = s_nr;
+	u64 jno;
+	u64 joff;
+	u64 mz0_Size = LOW_CHUNK;
+	u64 mz0_Offset = MAP_BUMP;
+
+	if (s < mz0_Size) {
+		dm_s = s + mz0_Offset;
+	} else {
+		s -= mz0_Size;
+		jno = s / LOW_CHUNK;
+		joff = s - (jno * LOW_CHUNK);
+		jno++;
+		dm_s = (jno * EXPAND) + joff + MAP_BUMP;
+	}
+	out->dm_s = dm_s;
+	map_addr_calc(dm_s, out);
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * ---------------------------------------------------------------------
+ *
+ *   Load a page of the sector lookup table that maps to sm->lut_s.
+ *
+ *   This sector is normally stored within the zone data sectors
+ *    co-located with the upper level data.
+ *
+ *   When no sector has been written fill a new block of the memory
+ *   pool with 0xff
+ */
+static int load_page(struct megazone *megaz, struct map_pg *mapped, u64 lba,
+		     int is_to)
+{
+	struct map_addr maddr;
+	u64 lba48;
+	int rcode = 0;
+
+	/**
+	 * This table entry may be on-disk, if so it needs to
+	 * be loaded.
+	 * If not it needs to be initialized to 0xFF
+	 */
+
+	/* may be recursive to load_page */
+	map_addr_calc(lba, &maddr);
+	lba48 = z_lookup(megaz, &maddr);
+	if (lba48) {
+		const int count = 1;
+		const int use_wq = 1;
+		const int crce = (maddr.mz_off & 0xFFFF) % 2048;
+		int rd;
+		struct crc_pg *pblock;
+		u16 check;
+
+		if (mapped->last_write == lba48)
+			Z_DBG(megaz->znd,
+			      "Page RE-LOAD %" PRIx64 " from %" PRIx64,
+			      mapped->lba, lba48);
+
+		atomic_inc(&mapped->refcount);
+		mapped_lock(mapped);
+		if (warn_bad_lba(megaz, lba48))
+			Z_ERR(megaz->znd, "Bad PAGE %" PRIx64, lba48);
+
+		rd = read_block(megaz->znd->ti, DM_IO_KMEM, mapped->mdata,
+				lba48, count, use_wq);
+
+		check = crc_md_le16(mapped->mdata, Z_CRC_4K);
+		mapped_unlock(mapped);
+		atomic_dec(&mapped->refcount);
+		if (rd) {
+			Z_ERR(megaz->znd, "%s: read_block: ERROR: %d",
+				__func__, rd);
+			rcode = -EIO;
+			goto out;
+		}
+		pblock = get_meta_pg_crc(megaz, &maddr, is_to, use_wq);
+		if (!pblock) {
+			Z_ERR(megaz->znd, "%s: Out of space for metadata?",
+				__func__);
+			rcode = -ENOSPC;
+			goto out;
+		}
+		atomic_inc(&pblock->refcount);
+		mutex_lock(&pblock->lock_pg);
+		if (pblock->cdata[crce] == check)
+			mapped->last_write = lba48;
+		mutex_unlock(&pblock->lock_pg);
+		atomic_dec(&pblock->refcount);
+		if (pblock->cdata[crce] != check) {
+			/* FIXME:!! */
+			int count;
+			u64 lba_recheck;
+
+			map_addr_calc(lba, &maddr);
+			lba_recheck = z_lookup(megaz, &maddr);
+
+			Z_ERR(megaz->znd,
+			      "Sanity: %" PRIx64 " mapped to %" PRIx64 " vs %"
+			      PRIx64 "", lba, lba_recheck, lba48);
+
+			Z_ERR(megaz->znd,
+			      "Corrupt metadata: %" PRIx64 " from %" PRIx64
+			      " [%04x != %04x] crc lba: %" PRIx64 " flags:%lx",
+			      lba, lba48, le16_to_cpu(check),
+			      le16_to_cpu(pblock->cdata[crce]),
+			      pblock->lba, mapped->flags);
+
+			Z_ERR(megaz->znd,
+			      "load_page: %" PRIx64 " from lba:%" PRIx64
+			      " last written to: %" PRIx64 " - map_to? %d",
+			      mapped->lba, lba48, mapped->last_write,
+			      is_to);
+
+			mapped_lock(mapped);
+			for (count = 0; count < 1024; count++) {
+				Z_ERR(megaz->znd, "mapped->mdata[%d] -> %08x",
+				      count, mapped->mdata[count]);
+			}
+			mapped_unlock(mapped);
+
+			megaz->meta_result = -ENOSPC;
+			rcode = -EIO;
+			goto out;
+		}
+		rcode = 1;
+	}
+
+out:
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int map_entry_page(struct megazone *megaz, struct map_pg *mapped,
+			  u64 lba, int is_to)
+{
+	struct list_head *mapped_pool = &(megaz->smtpool);
+	int rc = -ENOMEM;
+
+	atomic_inc(&mapped->refcount);
+	mapped_lock(mapped);
+	mapped->mdata = ZDM_ALLOC(megaz->znd, Z_C4K, PG_27);
+	if (mapped->mdata)
+		memset(mapped->mdata, 0xFF, Z_C4K);
+	mapped_unlock(mapped);
+	if (!mapped->mdata) {
+		Z_ERR(megaz->znd, "%s: Out of memory.", __func__);
+		atomic_dec(&mapped->refcount);
+		mapped = NULL;
+		goto out;
+	}
+	rc = load_page(megaz, mapped, lba, is_to);
+	if (rc < 0) {
+		Z_ERR(megaz->znd, "%s: load_page from %" PRIx64
+		      " [to? %d] error: %d", __func__, lba,
+		      is_to, rc);
+		atomic_dec(&mapped->refcount);
+		ZDM_FREE(megaz->znd, mapped->mdata, Z_C4K, PG_27);
+		mapped = NULL;
+		goto out;
+	}
+	mapped->age = jiffies_64;
+	spin_lock(&megaz->map_pool_lock);
+	list_add(&mapped->inpool, mapped_pool);
+	megaz->incore_count++;
+	spin_unlock(&megaz->map_pool_lock);
+	atomic_dec(&mapped->refcount);
+
+	Z_DBG(megaz->znd, "Page loaded: lba: %" PRIx64, lba);
+out:
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct map_pg *get_map_entry(struct megazone *megaz,
+				    struct map_addr *maddr, int is_to)
+{
+	u64 lba = is_to ? maddr->lut_s : maddr->lut_r;
+	struct map_pg *mapped = get_map_table_entry(megaz, lba, is_to);
+
+	if (mapped) {
+		if (!mapped->mdata) {
+			int rc = map_entry_page(megaz, mapped, lba, is_to);
+
+			if (rc < 0) {
+				megaz->meta_result = rc;
+				mapped = NULL;
+			}
+		}
+	} else {
+		Z_ERR(megaz->znd,
+		      "%s: No table for %" PRIx64 " page# %" PRIx64 ".",
+		      __func__, maddr->dm_s, lba);
+	}
+	return mapped;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/* load a page of the (sector -> lba) sector map table into core memory */
+static struct map_pg *sector_map_entry(struct megazone *megaz,
+				       struct map_addr *maddr)
+{
+	const int is_to = 1;
+
+	return get_map_entry(megaz, maddr, is_to);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/* load a page of the (lba -> sector) reverse map table into core memory */
+static struct map_pg *reverse_map_entry(struct megazone *megaz,
+					struct map_addr *maddr)
+{
+	const int is_to = 0;
+
+	return get_map_entry(megaz, maddr, is_to);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/* resolve a sector mapping to the on-block-device lba via the lookup table */
+static u64 locate_sector(struct megazone *megaz, struct map_addr *maddr)
+{
+	struct map_pg *mapped;
+	u64 old_phy = 0;
+
+	mapped = sector_map_entry(megaz, maddr);
+	if (mapped) {
+		if (mapped->mdata) {
+			struct list_head *pool_pages = &(megaz->smtpool);
+			struct list_head *m_item = &(mapped->inpool);
+			u32 delta;
+
+			atomic_inc(&mapped->refcount);
+			mapped_lock(mapped);
+			delta = mapped->mdata[maddr->offentry];
+			mapped_unlock(mapped);
+			atomic_dec(&mapped->refcount);
+			old_phy = map_value(megaz, delta);
+
+			BUG_ON(old_phy >= megaz->znd->nr_blocks);
+
+			mapped->age = jiffies_64;
+			if (m_item->next)
+				incore_hint(megaz, pool_pages, m_item);
+		}
+	}
+	return old_phy;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * when is_fwd is 0:
+ *  - maddr->dm_s is a sector -> lba.
+ *         in this case the old lba is discarded and scheduled for cleanup
+ *         by updating the reverse map lba tables noting that this location
+ *         is now unused.
+ * when is_fwd is 0:
+ *  - maddr->dm_s is an lba, lba -> dm_s
+ */
+
+static int update_map_entry(struct megazone *megaz, struct map_pg *mapped,
+			    struct map_addr *maddr, u64 to_addr, int is_fwd)
+{
+	int err = -ENOMEM;
+
+	if (mapped && mapped->mdata) {
+		u64 index = maddr->offentry;
+		u32 delta;
+		u32 value;
+		int was_updated = 0;
+
+		atomic_inc(&mapped->refcount);
+		mapped_lock(mapped);
+		delta = mapped->mdata[index];
+		mapped_unlock(mapped);
+		err = map_encode(megaz, to_addr, &value);
+		if (!err) {
+			mapped_lock(mapped);
+			/*
+			 * if the value is modified update the table and
+			 * place it on top of the active [inpool] list
+			 * this will keep the chunk of lookup table in
+			 * memory.
+			 */
+			if (mapped->mdata[index] != value) {
+				struct list_head *pool_pages =
+				    &(megaz->smtpool);
+				struct list_head *m_item = &(mapped->inpool);
+
+				mapped->mdata[index] = value;
+				mapped->age = jiffies_64;
+				set_bit(IS_DIRTY, &mapped->flags);
+				was_updated = 1;
+				if (m_item->next)
+					incore_hint(megaz, pool_pages, m_item);
+
+			} else {
+				Z_ERR(megaz->znd,
+					"*ERR* %" PRIx64
+					" -> mdata[index] (%x) == (%x)",
+					maddr->dm_s,
+					mapped->mdata[index], value);
+				dump_stack();
+			}
+			mapped_unlock(mapped);
+		} else {
+			Z_ERR(megaz->znd,
+				"*ERR* Mapping: %" PRIx64 " to %" PRIx64,
+				to_addr, maddr->dm_s);
+		}
+
+		if (was_updated && is_fwd && (delta != MZTEV_UNUSED)) {
+			u64 old_phy = map_value(megaz, delta);
+
+			/*
+			 * add to discard list of the controlling mzone
+			 * for the 'delta' physical block
+			 * unlikly, but they may be different megazones
+			 */
+			Z_DBG(megaz->znd, "%s: unused_phy: %" PRIu64
+			      " (new lba: %" PRIu64 ")",
+			      __func__, old_phy, to_addr);
+
+			BUG_ON(old_phy >= megaz->znd->nr_blocks);
+
+			err = unused_phy(megaz, old_phy, 0);
+			if (err)
+				err = -ENOSPC;
+		}
+		atomic_dec(&mapped->refcount);
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  Copy a journal block into the sector map [SLT] for this megazone.
+ */
+static int move_to_map_tables(struct megazone *megaz, struct map_cache *jrnl)
+{
+	struct map_pg *smtbl = NULL;
+	struct map_pg *rmtbl = NULL;
+	struct map_addr maddr = { .dm_s = 0ul };
+	struct map_addr reverse = { .dm_s = 0ul };
+	u64 lut_s = ~0ul;
+	u64 lut_r = ~0ul;
+	int jentry;
+	int err = 0;
+	int is_fwd = 1;
+
+	/* the journal being move must remain stable so sorting
+	 * is disabled. If a sort is desired due to an unsorted
+	 * page the search devolves to a linear lookup.
+	 */
+	jrnl->no_sort_flag = 1;
+
+	for (jentry = jrnl->jcount; jentry > 0;) {
+		u64 dm_s = le64_to_lba48(jrnl->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(jrnl->jdata[jentry].physical, NULL);
+
+		if (dm_s == Z_LOWER48 || lba == Z_LOWER48) {
+			jrnl->jcount = --jentry;
+			continue;
+		}
+
+		map_addr_calc(dm_s, &maddr);
+		if (lut_s != maddr.lut_s) {
+			if (smtbl)
+				atomic_dec(&smtbl->refcount);
+			smtbl = sector_map_entry(megaz, &maddr);
+			if (!smtbl) {
+				err = -ENOMEM;
+				goto out;
+			}
+			atomic_inc(&smtbl->refcount);
+			lut_s = smtbl->lba;
+		}
+
+		is_fwd = 1;
+		if (0ul == lba)
+			lba = ~0ul;
+		err = update_map_entry(megaz, smtbl, &maddr, lba, is_fwd);
+		if (err < 0)
+			goto out;
+
+		/*
+		 * In this case the reverse was handled as part of
+		 * discarding the forward map entry -- if it was in use.
+		 */
+		if (~0ul != lba) {
+			map_addr_calc(lba, &reverse);
+			if (lut_r != reverse.lut_r) {
+				if (rmtbl)
+					atomic_dec(&rmtbl->refcount);
+				rmtbl = reverse_map_entry(megaz, &reverse);
+				if (!rmtbl) {
+					err = -ENOMEM;
+					goto out;
+				}
+				atomic_inc(&rmtbl->refcount);
+				lut_r = rmtbl->lba;
+			}
+			is_fwd = 0;
+			err = update_map_entry(megaz, rmtbl, &reverse,
+						dm_s, is_fwd);
+			if (err == 1)
+				err = 0;
+#if EXTRA_DEBUG
+		} else {
+			Z_DBG(megaz->znd, "DISCARD: entry %d -> dm_s:%" PRIx64,
+				 jentry, dm_s);
+
+#endif /* EXTRA_DEBUG */
+		}
+
+		if (err < 0)
+			goto out;
+
+		jrnl->jdata[jentry].logical = ~0ul;
+		jrnl->jdata[jentry].physical = ~0ul;
+		jrnl->jcount = --jentry;
+	}
+out:
+	if (smtbl)
+		atomic_dec(&smtbl->refcount);
+	if (rmtbl)
+		atomic_dec(&rmtbl->refcount);
+
+	set_bit(DO_MEMPOOL, &megaz->flags);
+
+	jrnl->no_sort_flag = 0;
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * Add an unused block to the list of blocks to be
+ *  discarded during garbage collection.
+ */
+static int unused_phy(struct megazone *megaz, u64 lba, u64 orig_s)
+{
+	int err = 0;
+	struct map_pg *mapped;
+	struct map_addr reverse;
+	int z_off;
+
+	/* Any reason to *NOT* immediately stuff UNUSED into
+	   the reverse table entry? */
+	mutex_lock(&megaz->discard_lock);
+
+	map_addr_calc(lba, &reverse);
+	z_off = reverse.z_id % 1024;
+	mapped = reverse_map_entry(megaz, &reverse);
+	if (!mapped) {
+		err = -EIO;
+		Z_ERR(megaz->znd, "unused_phy: Reverse Map Entry not found.");
+		goto out;
+	}
+
+	BUG_ON(!mapped->mdata);
+
+	atomic_inc(&mapped->refcount);
+	mapped_lock(mapped);
+
+	/*
+	 * if the value is modified update the table and
+	 * place it on top of the active [inpool] list
+	 */
+	if (mapped->mdata[reverse.offentry] != MZTEV_UNUSED) {
+		struct list_head *_pool = &(megaz->smtpool);
+		struct list_head *m_item = &(mapped->inpool);
+		u32 wp = megaz->z_ptrs[z_off] & Z_WP_VALUE_MASK;
+
+		if (orig_s) {
+			u32 enc = mapped->mdata[reverse.offentry];
+			u64 dm_s = map_value(megaz, enc);
+			int drop_discard = 0;
+
+			if (_calc_zone(dm_s) < 4) {
+				drop_discard = 1;
+				Z_ERR(megaz->znd, "Discard invalid target %"
+				      PRIx64" - Is ZDM Meta %"PRIx64" vs %"
+				      PRIx64, lba, orig_s, dm_s);
+			}
+			if (orig_s != dm_s) {
+				drop_discard = 1;
+				Z_ERR(megaz->znd,
+				      "Discard %" PRIx64
+				      " mismatched src: %"PRIx64 " vs %" PRIx64,
+				      lba, orig_s, dm_s);
+			}
+			if (drop_discard)
+				goto out_unlock;
+		}
+		mapped->mdata[reverse.offentry] = MZTEV_UNUSED;
+		mapped->age = jiffies_64;
+		set_bit(IS_DIRTY, &mapped->flags);
+		if (m_item->next)
+			incore_hint(megaz, _pool, m_item);
+		if (wp && megaz->zfree_count[z_off] < Z_BLKSZ)
+			megaz->zfree_count[z_off]++;
+		if (wp == Z_BLKSZ)
+			megaz->discard_count++;
+	} else {
+		Z_DBG(megaz->znd,
+		      "lba: %" PRIx64 " alread reported as free?", lba);
+	}
+
+out_unlock:
+	mapped_unlock(mapped);
+	atomic_dec(&mapped->refcount);
+
+out:
+	mutex_unlock(&megaz->discard_lock);
+
+	return err;
+}
-- 
1.9.1

