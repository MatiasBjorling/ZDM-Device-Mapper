From f08161251e2656bdb3bc7f505f377ffd7197d69f Mon Sep 17 00:00:00 2001
From: Shaun Tancheff <shaun@tancheff.com>
Date: Thu, 4 Feb 2016 14:05:19 -0600
Subject: [PATCH 5/7] ZDM: New DM target 'zoned'

    ZDM presents a traditional block device for ZBC/ZAC zone based
    devices.
    User space utilities in zdm-tools for creating, repairing and restore
    DM instances at: https://github.com/Seagate

Signed-off-by: Shaun Tancheff <shaun.tancheff@seagate.com>
---
 Documentation/device-mapper/zoned.txt |   72 +
 MAINTAINERS                           |    7 +
 drivers/md/Kconfig                    |   16 +-
 drivers/md/Makefile                   |    1 +
 drivers/md/dm-zoned.c                 | 2436 +++++++++++++++
 drivers/md/dm-zoned.h                 |  665 ++++
 drivers/md/libzoned.c                 | 5372 +++++++++++++++++++++++++++++++++
 7 files changed, 8567 insertions(+), 2 deletions(-)
 create mode 100644 Documentation/device-mapper/zoned.txt
 create mode 100644 drivers/md/dm-zoned.c
 create mode 100644 drivers/md/dm-zoned.h
 create mode 100644 drivers/md/libzoned.c

diff --git a/Documentation/device-mapper/zoned.txt b/Documentation/device-mapper/zoned.txt
new file mode 100644
index 0000000..59a700e
--- /dev/null
+++ b/Documentation/device-mapper/zoned.txt
@@ -0,0 +1,72 @@
+ZDM: A Host Aware ZBC/ZAC Device Mapper
+  - Expected zone size 256MiB
+  - ZAC/ZBC support Reset WP, Open Zone, Close Zone, Report Zones
+
+ZDM allows Host Aware and Host Managed drives to appear as
+normal block devices to devices and filesystesm above it.
+
+Currently the ZDM must be the bottom most DM layer. THis will
+be addressed in a future release.
+
+The initial implementation focuses on drives with same sized zones of
+256MB which is 65536 4k blocks. In future the zone size of 256MB will
+be relaxed to allow any size of zone as long as they are all,
+or mostly, the same size.
+
+Internally all addressing is on 4k boundaries. Currently a 4k PAGE_SIZE is
+assumed. Architectures with 8k (or other) PAGE_SIZE have not been tested
+and are likly broken at the moment.
+
+Host Managed drives are expected to work however they have not yet
+been tested.
+
+Required Conventional/Host Aware space: 0.1 percent is required and
+0.2 percent is recommened. A future release my address using much
+fewer conventional zones (1 or 2 zones per partition/drive).
+
+
+Stream Id Support
+	ZDM supports 255 stream ids for the purpose of co-locating
+	data that is expected to have similar life-times. On
+	ingress to a zone a stream id affinity is created. All
+	futher writes to the same stream id will write to the same
+	zone, until it is filled.
+
+	The device mapper internally is a COW device with a 4k per block
+	addressing scheme. There are some fix-ups to handle non-4k aligned
+	requests to support applications which read and write in 512 byte
+	blocks, however it is still desirable to submit patches for these
+	subsystems assuming the respective maintainers are willing to
+	accept 4k alignment patches.
+
+Address space:
+	The zoned device mapper presents a smaller block device than
+	the amount of data available on the physical media. The extra
+	space is used to hold the meta data needed for managing the
+	data being stored on the drive performing COW block [re]mapping.
+	The 'shrink' is done by appropriately sizing the device via
+	dmsetup.
+	See the zdmadm utility will detect and size the device appropriaty.
+
+Map Strategy:
+	Map incoming sector onto device mapper sector space.
+
+Read Strategy:
+	Check each block requested in the bio to determine if the data
+	blocks are consecutively stored on disk. Pass as much per-bio
+	as possible through to the backing block device.
+
+Write Strategy:
+	Allocate space for entire bio on the backing block device
+	redirecting all incoming write requests to the most recently
+	written zone until the zone is filled or the bio is too large
+	to fit and a new zone is opened. Note that if the zone is not
+	filled this zone will likely become used by meta data writes
+	which are typically single blocks.
+
+Sync Strategy:
+	On SYNC bios all the meta data need to restore the zoned device
+	mapper for disk is written to one of the well known zones at
+	the beginning of the mega zone. Data consistency is only
+	'guaranteed' to be on-disk and consistent following sync
+	events [same as ext4].
diff --git a/MAINTAINERS b/MAINTAINERS
index 45bf7e9..f56d448 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -11651,6 +11651,13 @@ L:	zd1211-devs@lists.sourceforge.net (subscribers-only)
 S:	Maintained
 F:	drivers/net/wireless/zd1211rw/
 
+ZDM ZONED DEVICE MAPPER TARGET
+M:	Shaun Tancheff <shaun.tancheff@seagate.com>
+L:	dm-devel@redhat.com
+S:	Maintained
+F:	drivers/md/dm-zoned.*
+F:	drivers/md/libzoned.c
+
 ZONED BLOCK DEVICE CONTROL SUPPORT
 M:	Shaun Tancheff <shaun.tancheff@seagate.com>
 L:	linux-fsdevel@vger.kernel.org
diff --git a/drivers/md/Kconfig b/drivers/md/Kconfig
index 3e01e6f..52b73d8 100644
--- a/drivers/md/Kconfig
+++ b/drivers/md/Kconfig
@@ -37,9 +37,9 @@ config MD_AUTODETECT
 	default y
 	---help---
 	  If you say Y here, then the kernel will try to autodetect raid
-	  arrays as part of its boot process. 
+	  arrays as part of its boot process.
 
-	  If you don't use raid and say Y, this autodetection can cause 
+	  If you don't use raid and say Y, this autodetection can cause
 	  a several-second delay in the boot time due to various
 	  synchronisation steps that are part of this step.
 
@@ -335,6 +335,18 @@ config DM_ERA
          over time.  Useful for maintaining cache coherency when using
          vendor snapshots.
 
+config DM_ZONED
+       tristate "ZDM: Zoned based device target (EXPERIMENTAL)"
+       depends on BLK_DEV_DM
+       default n
+       select LIBCRC32C
+       select BLK_ZONED_CTRL
+       ---help---
+         dm-zoned provides a rand access block device on top of a
+         ZBC/ZAC block device.
+         Forward writing within zones, garbage collection within zones.
+         Use zdmadm to create, repair and/or restore ZDM instances.
+
 config DM_MIRROR
        tristate "Mirror target"
        depends on BLK_DEV_DM
diff --git a/drivers/md/Makefile b/drivers/md/Makefile
index 462f443..a692a79 100644
--- a/drivers/md/Makefile
+++ b/drivers/md/Makefile
@@ -59,6 +59,7 @@ obj-$(CONFIG_DM_CACHE_SMQ)	+= dm-cache-smq.o
 obj-$(CONFIG_DM_CACHE_CLEANER)	+= dm-cache-cleaner.o
 obj-$(CONFIG_DM_ERA)		+= dm-era.o
 obj-$(CONFIG_DM_LOG_WRITES)	+= dm-log-writes.o
+obj-$(CONFIG_DM_ZONED)		+= dm-zoned.o
 
 ifeq ($(CONFIG_DM_UEVENT),y)
 dm-mod-objs			+= dm-uevent.o
diff --git a/drivers/md/dm-zoned.c b/drivers/md/dm-zoned.c
new file mode 100644
index 0000000..73cba32
--- /dev/null
+++ b/drivers/md/dm-zoned.c
@@ -0,0 +1,2436 @@
+/*
+ * Kernel Device Mapper for abstracting ZAC/ZBC devices as normal
+ * block devices for linux file systems.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include "dm.h"
+#include <linux/dm-io.h>
+#include <linux/init.h>
+#include <linux/mempool.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/random.h>
+#include <linux/crc32c.h>
+#include <linux/crc16.h>
+#include <linux/sort.h>
+#include <linux/ctype.h>
+#include <linux/types.h>
+#include <linux/blk-zoned-ctrl.h>
+#include <linux/timer.h>
+#include <linux/delay.h>
+#include <linux/kthread.h>
+#include <linux/freezer.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include "dm-zoned.h"
+
+/*
+ * FUTURE FIXME:
+ * Current sd.c does not swizzle on report zones and no
+ * scsi native drives exists so ... therefore all results are
+ * little endian ...
+ * When sd.c translates the output of report zones
+ * then remove the 'everything is little endian' assumption.
+ */
+#define REPORT_ZONES_LE_ONLY 1
+
+#define PRIu64 "llu"
+#define PRIx64 "llx"
+#define PRId32 "d"
+#define PRIx32 "x"
+#define PRIu32 "u"
+
+static inline char *_zdisk(struct zoned *znd)
+{
+	return znd->bdev_name;
+}
+
+#define Z_ERR(znd, fmt, arg...) \
+	pr_err("dm-zoned(%s): " fmt "\n", _zdisk(znd), ## arg)
+
+#define Z_INFO(znd, fmt, arg...) \
+	pr_info("dm-zoned(%s): " fmt "\n", _zdisk(znd), ## arg)
+
+#define Z_DBG(znd, fmt, arg...) \
+	pr_debug("dm-zoned(%s): " fmt "\n", _zdisk(znd), ## arg)
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static int dev_is_congested(struct dm_dev *dev, int bdi_bits);
+static int zoned_is_congested(struct dm_target_callbacks *cb, int bdi_bits);
+static int zoned_ctr(struct dm_target *ti, unsigned argc, char **argv);
+static void do_io_work(struct work_struct *work);
+static int block_io(struct zoned *, enum dm_io_mem_type, void *, sector_t,
+		    unsigned int, int, int);
+static int znd_async_io(struct zoned *znd,
+			enum dm_io_mem_type dtype,
+			void *data,
+			sector_t block, unsigned int nDMsect, int rw, int queue,
+			io_notify_fn callback, void *context);
+static int zoned_bio(struct zoned *znd, struct bio *bio);
+static int zoned_map_write(struct zoned *znd, struct bio*, u64 s_zdm);
+static int zoned_map_read(struct zoned *znd, struct bio *bio);
+static int zoned_map(struct dm_target *ti, struct bio *bio);
+static sector_t get_dev_size(struct dm_target *ti);
+static int zoned_iterate_devices(struct dm_target *ti,
+				 iterate_devices_callout_fn fn, void *data);
+static void zoned_io_hints(struct dm_target *ti, struct queue_limits *limits);
+static int is_zoned_inquiry(struct zoned *znd, int trim, int ata);
+static int dmz_reset_wp(struct zoned *znd, u64 z_id);
+static int dmz_open_zone(struct zoned *znd, u64 z_id);
+static int dmz_close_zone(struct zoned *znd, u64 z_id);
+static u32 dmz_report_count(struct zoned *znd, void *report, size_t bufsz);
+static int dmz_report_zones(struct zoned *znd, u64 z_id,
+			    struct bdev_zone_report *report, size_t bufsz);
+static void activity_timeout(unsigned long data);
+static void zoned_destroy(struct zoned *);
+static int gc_can_cherrypick(struct zoned *znd, u32 sid, int delay, int gfp);
+static void bg_work_task(struct work_struct *work);
+static void on_timeout_activity(struct zoned *znd, int mempurge, int delay);
+static int zdm_create_proc_entries(struct zoned *znd);
+static void zdm_remove_proc_entries(struct zoned *znd);
+
+/**
+ * bio_stream() - Decode stream id from BIO.
+ * @znd: ZDM Instance
+ *
+ * Return: stream_id
+ */
+static inline u32 bio_stream(struct bio *bio)
+{
+	/*
+	 * Since adding stream id to a BIO is not yet in mainline we just
+	 * assign some defaults: use stream_id 0xff for upper level meta data
+	 * and 0x40 for everything else ...
+	 */
+	return (bio->bi_rw & REQ_META) ? 0xff : 0x40;
+}
+
+/**
+ * get_bdev_bd_inode() - Get primary backing device inode
+ * @znd: ZDM Instance
+ *
+ * Return: backing device inode
+ */
+static inline struct inode *get_bdev_bd_inode(struct zoned *znd)
+{
+	return znd->dev->bdev->bd_inode;
+}
+
+#include "libzoned.c"
+
+static int dev_is_congested(struct dm_dev *dev, int bdi_bits)
+{
+	struct request_queue *q = bdev_get_queue(dev->bdev);
+
+	return bdi_congested(&q->backing_dev_info, bdi_bits);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_is_congested(struct dm_target_callbacks *cb, int bdi_bits)
+{
+	struct zoned *znd = container_of(cb, struct zoned, callbacks);
+	int backing = dev_is_congested(znd->dev, bdi_bits);
+
+	if (znd->gc_backlog > 1) {
+		/*
+		 * Was BDI_async_congested;
+		 * Was BDI_sync_congested;
+		 */
+		backing |= 1 << WB_async_congested;
+		backing |= 1 << WB_sync_congested;
+	}
+	return backing;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void set_discard_support(struct zoned *znd, int trim)
+{
+	struct mapped_device *md = dm_table_get_md(znd->ti->table);
+	struct queue_limits *limits = dm_get_queue_limits(md);
+	struct gendisk *disk = znd->dev->bdev->bd_disk;
+
+	Z_INFO(znd, "Discard Support: %s", trim ? "on" : "off");
+	if (limits) {
+		limits->logical_block_size =
+			limits->physical_block_size =
+			limits->io_min = Z_C4K;
+		if (trim) {
+			limits->discard_alignment = Z_C4K;
+			limits->discard_granularity = Z_C4K;
+			limits->max_discard_sectors = 1 << 16;
+			limits->max_hw_discard_sectors = 1 << 16;
+			limits->discard_zeroes_data = 1;
+		}
+	}
+	/* fixup stacked queue limits so discard zero's data is honored */
+	if (trim && disk->queue) {
+		limits = &disk->queue->limits;
+		limits->discard_alignment = Z_C4K;
+		limits->discard_granularity = Z_C4K;
+		limits->max_discard_sectors = 1 << 16;
+		limits->max_hw_discard_sectors = 1 << 16;
+		limits->discard_zeroes_data = 1;
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int is_zoned_inquiry(struct zoned *znd, int trim, int ata)
+{
+	struct gendisk *disk = znd->dev->bdev->bd_disk;
+
+	if (disk->queue) {
+		u8 extended = 1;
+		u8 page_op = 0xb1;
+		u8 *buf = NULL;
+		u16 sz = 64;
+		int wp_err;
+
+		set_discard_support(znd, trim);
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+		if (ata) {
+			struct zoned_identity ident;
+
+			wp_err = blk_zoned_identify_ata(disk, &ident);
+			if (!wp_err) {
+				if (ident.type_id == HOST_AWARE) {
+					znd->zinqtype = Z_TYPE_SMR_HA;
+					znd->ata_passthrough = 1;
+				}
+			}
+			return 0;
+		}
+
+		buf = ZDM_ALLOC(znd, Z_C4K, PG_01, NORMAL); /* zoned inq */
+		if (!buf)
+			return -ENOMEM;
+
+		wp_err = blk_zoned_inquiry(disk, extended, page_op, sz, buf);
+		if (!wp_err) {
+			znd->zinqtype = buf[Z_VPD_INFO_BYTE] >> 4 & 0x03;
+			if (znd->zinqtype != Z_TYPE_SMR_HA &&
+			    buf[4] == 0x17 && buf[5] == 0x5c) {
+				Z_ERR(znd, "Forcing ResetWP capability ... ");
+				znd->zinqtype = Z_TYPE_SMR_HA;
+				znd->ata_passthrough = 0;
+			}
+		}
+#else
+	#warning "CONFIG_BLK_ZONED_CTRL required."
+#endif
+		if (buf)
+			ZDM_FREE(znd, buf, Z_C4K, PG_01);
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_map_discard(struct zoned *znd, struct bio *bio)
+{
+	u64 lba     = 0;
+	int rcode   = DM_MAPIO_SUBMITTED;
+	u64 s_zdm   = (bio->bi_iter.bi_sector >> Z_SHFT4K) + znd->md_end;
+	u64 blks    = bio->bi_iter.bi_size / Z_C4K;
+	u64 count;
+
+	if (znd->is_empty)
+		goto cleanup_out;
+
+	for (count = 0; count < blks; count++) {
+		int err = 0;
+		u64 addr = s_zdm + count;
+
+		MutexLock(&znd->mz_io_mutex);
+		lba = current_mapping(znd, addr, CRIT);
+		if (lba)
+			err = z_mapped_discard(znd, addr, lba);
+		mutex_unlock(&znd->mz_io_mutex);
+		if (err) {
+			rcode = err;
+			goto out;
+		}
+		if ((count & 0xFFFFF) == 0 && blks > 0x80000) {
+			if (test_bit(DO_JOURNAL_MOVE, &znd->flags) ||
+			    test_bit(DO_MEMPOOL, &znd->flags)) {
+				if (!test_bit(DO_METAWORK_QD, &znd->flags) &&
+				    !work_pending(&znd->meta_work)) {
+
+					Z_ERR(znd, "Large discard @ %"
+					      PRIx64 " %" PRIu64 " blocks  ...",
+					      s_zdm, blks);
+
+					set_bit(DO_METAWORK_QD, &znd->flags);
+					queue_work(znd->meta_wq,
+						  &znd->meta_work);
+					flush_workqueue(znd->meta_wq);
+				}
+			}
+		}
+	}
+
+cleanup_out:
+	bio_endio(bio);
+out:
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int is_non_wp_zone(struct zoned *znd, u64 z_id)
+{
+	u32 gzoff = z_id % 1024;
+	struct meta_pg *wpg = &znd->wp[z_id >> 10];
+	u32 wp = le32_to_cpu(wpg->wp_alloc[gzoff]);
+
+	return (wp & Z_WP_NON_SEQ) ? 1 : 0;
+}
+
+
+static int dmz_reset_wp(struct zoned *znd, u64 z_id)
+{
+	int wp_err = 0;
+
+	if (is_non_wp_zone(znd, z_id))
+		return wp_err;
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = znd->dev->bdev->bd_disk;
+		u64 z_offset = z_id + znd->zdstart;
+		u64 s_addr = zone_to_sector(z_offset);
+		int retry = 5;
+
+		wp_err = -1;
+		while (wp_err && --retry > 0) {
+			if (znd->ata_passthrough)
+				wp_err = blk_zoned_reset_wp_ata(disk, s_addr);
+			else
+				wp_err = blk_zoned_reset_wp(disk, s_addr);
+		}
+
+		if (wp_err) {
+			Z_ERR(znd, "Reset WP: %" PRIx64 " [Z:%" PRIu64
+			      "] -> %d failed.", s_addr, z_id, wp_err);
+			Z_ERR(znd, "ZAC/ZBC support disabled.");
+			znd->zinqtype = 0;
+			wp_err = -ENOTSUPP;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int dmz_open_zone(struct zoned *znd, u64 z_id)
+{
+	int wp_err = 0;
+
+	if (is_non_wp_zone(znd, z_id))
+		return wp_err;
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = znd->dev->bdev->bd_disk;
+		u64 z_offset = z_id + znd->zdstart;
+		u64 s_addr = zone_to_sector(z_offset);
+		int retry = 5;
+
+		wp_err = -1;
+		while (wp_err && --retry > 0) {
+			if (znd->ata_passthrough)
+				wp_err = blk_zoned_open_ata(disk, s_addr);
+			else
+				wp_err = blk_zoned_open(disk, s_addr);
+		}
+
+		if (wp_err) {
+			Z_ERR(znd, "Open Zone: LBA: %" PRIx64
+			      " [Z:%" PRIu64 "] -> %d failed.",
+			      s_addr, z_id, wp_err);
+			Z_ERR(znd, "ZAC/ZBC support disabled.");
+			znd->zinqtype = 0;
+			wp_err = -ENOTSUPP;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+
+	return wp_err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int dmz_close_zone(struct zoned *znd, u64 z_id)
+{
+	int wp_err = 0;
+
+	if (is_non_wp_zone(znd, z_id))
+		return wp_err;
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = znd->dev->bdev->bd_disk;
+		u64 z_offset = z_id + znd->zdstart;
+		u64 s_addr = zone_to_sector(z_offset);
+		int retry = 5;
+
+		wp_err = -1;
+		while (wp_err && --retry > 0) {
+			if (znd->ata_passthrough)
+				wp_err = blk_zoned_close_ata(disk, s_addr);
+			else
+				wp_err = blk_zoned_close(disk, s_addr);
+		}
+
+		if (wp_err) {
+			Z_ERR(znd, "Close Zone: LBA: %" PRIx64
+			      " [Z:%" PRIu64 "] -> %d failed.",
+			      s_addr, z_id, wp_err);
+			Z_ERR(znd, "ZAC/ZBC support disabled.");
+			znd->zinqtype = 0;
+			wp_err = -ENOTSUPP;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u32 dmz_report_count(struct zoned *znd, void *rpt_in, size_t bufsz)
+{
+	u32 count;
+	u32 max_count = (bufsz - sizeof(struct bdev_zone_report))
+		      /	 sizeof(struct bdev_zone_descriptor);
+
+	if (REPORT_ZONES_LE_ONLY || znd->ata_passthrough) {
+		struct bdev_zone_report_le *report = rpt_in;
+
+		/* ZAC: ata results are little endian */
+		if (max_count > le32_to_cpu(report->descriptor_count))
+			report->descriptor_count = cpu_to_le32(max_count);
+		count = le32_to_cpu(report->descriptor_count);
+	} else {
+		struct bdev_zone_report *report = rpt_in;
+
+		/* ZBC: scsi results are big endian */
+		if (max_count > be32_to_cpu(report->descriptor_count))
+			report->descriptor_count = cpu_to_be32(max_count);
+		count = be32_to_cpu(report->descriptor_count);
+	}
+	return count;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * dmz_report_zones() - issue report zones from z_id zones after zdstart
+ * @znd: ZDM Target
+ * @z_id: Zone past zdstart
+ * @report: structure filled
+ * @bufsz: kmalloc()'d space reserved for report
+ *
+ * Return: -ENOTSUPP or 0 on success
+ */
+static int dmz_report_zones(struct zoned *znd, u64 z_id,
+			    struct bdev_zone_report *report, size_t bufsz)
+{
+	int wp_err = -ENOTSUPP;
+
+#ifdef CONFIG_BLK_ZONED_CTRL
+	if (znd->zinqtype == Z_TYPE_SMR_HA) {
+		struct gendisk *disk = znd->dev->bdev->bd_disk;
+		u64 s_addr = (z_id + znd->zdstart) << 19;
+		u8  opt = ZOPT_NON_SEQ_AND_RESET;
+		int retry = 5;
+
+		wp_err = -1;
+		while (wp_err && --retry > 0) {
+			if (znd->ata_passthrough)
+				wp_err = blk_zoned_report_ata(disk, s_addr, opt,
+							      report, bufsz);
+			else
+				wp_err = blk_zoned_report(disk, s_addr, opt,
+							  report, bufsz);
+		}
+
+		if (wp_err) {
+			Z_ERR(znd, "Report Zones: LBA: %" PRIx64
+			      " [Z:%" PRIu64 " -> %d failed.",
+			      s_addr, z_id + znd->zdstart, wp_err);
+			Z_ERR(znd, "ZAC/ZBC support disabled.");
+			znd->zinqtype = 0;
+			wp_err = -ENOTSUPP;
+		}
+	}
+#endif /* CONFIG_BLK_ZONED_CTRL */
+	return wp_err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_zone_reset(struct bdev_zone_descriptor *dentry)
+{
+	u8 type = dentry->type & 0x0F;
+	u8 cond = (dentry->flags & 0xF0) >> 4;
+
+	return (ZCOND_ZC1_EMPTY == cond || ZTYP_CONVENTIONAL == type) ? 1 : 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u32 get_wp_from_descriptor(struct zoned *znd, void *dentry_in)
+{
+	u32 wp = 0;
+
+	/*
+	 * If ATA passthrough was used then ZAC results are little endian.
+	 * otherwise ZBC results are big endian.
+	 */
+
+	if (REPORT_ZONES_LE_ONLY || znd->ata_passthrough) {
+		struct bdev_zone_descriptor_le *lil = dentry_in;
+
+		wp = le64_to_cpu(lil->lba_wptr) - le64_to_cpu(lil->lba_start);
+	} else {
+		struct bdev_zone_descriptor *big = dentry_in;
+
+		wp = be64_to_cpu(big->lba_wptr) - be64_to_cpu(big->lba_start);
+	}
+	return wp;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_conventional(struct bdev_zone_descriptor *dentry)
+{
+	return (ZTYP_CONVENTIONAL == (dentry->type & 0x0F)) ? 1 : 0;
+}
+
+static inline void _inc_wp_free(struct meta_pg *wpg, u32 gzoff, u32 lost)
+{
+	wpg->zf_est[gzoff] = cpu_to_le32(le32_to_cpu(wpg->zf_est[gzoff])+lost);
+}
+
+static int zoned_wp_sync(struct zoned *znd, int reset_non_empty)
+{
+	int rcode = 0;
+	u32 rcount = 0;
+	u32 iter;
+	size_t bufsz = REPORT_BUFFER * Z_C4K;
+	struct bdev_zone_report *report = kmalloc(bufsz, GFP_KERNEL);
+
+	if (!report) {
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	for (iter = 0; iter < znd->data_zones; iter++) {
+		u32 entry = iter % 4096;
+		u32 gzno  = iter >> 10;
+		u32 gzoff = iter & ((1 << 10) - 1);
+		struct meta_pg *wpg = &znd->wp[gzno];
+		struct bdev_zone_descriptor *dentry;
+		u32 wp_flgs;
+		u32 wp_at;
+		u32 wp;
+
+		if (entry == 0) {
+			int err = dmz_report_zones(znd, iter, report, bufsz);
+
+			if (err) {
+				Z_ERR(znd, "report zones-> %d", err);
+				if (err != -ENOTSUPP)
+					rcode = err;
+				goto out;
+			}
+			rcount = dmz_report_count(znd, report, bufsz);
+		}
+
+		dentry = &report->descriptors[entry];
+		if (reset_non_empty && !is_conventional(dentry)) {
+			int err = 0;
+
+			if (!is_zone_reset(dentry))
+				err = dmz_reset_wp(znd, gzoff);
+
+			if (err) {
+				Z_ERR(znd, "reset wp-> %d", err);
+				if (err != -ENOTSUPP)
+					rcode = err;
+				goto out;
+			}
+			wp = wp_at = 0;
+			wpg->wp_alloc[gzoff] = cpu_to_le32(0);
+			wpg->zf_est[gzoff] = cpu_to_le32(Z_BLKSZ);
+			continue;
+		}
+
+		wp = get_wp_from_descriptor(znd, dentry);
+		wp >>= Z_SHFT4K; /* 512 sectors to 4k sectors */
+		wp_at = le32_to_cpu(wpg->wp_alloc[gzoff]) & Z_WP_VALUE_MASK;
+		wp_flgs = le32_to_cpu(wpg->wp_alloc[gzoff]) & Z_WP_FLAGS_MASK;
+
+		if (is_conventional(dentry)) {
+			wp = wp_at;
+			wp_flgs |= Z_WP_NON_SEQ;
+		} else {
+			wp_flgs &= ~Z_WP_NON_SEQ;
+		}
+
+		if (wp > wp_at) {
+			u32 lost = wp - wp_at;
+
+			wp_at = wp;
+			_inc_wp_free(wpg, gzoff, lost);
+
+			Z_ERR(znd, "Z#%u z:%x [wp:%x rz:%x] lost %u blocks.",
+			      iter, gzoff, wp_at, wp, lost);
+		}
+		wpg->wp_alloc[gzoff] = cpu_to_le32(wp_at|wp_flgs);
+	}
+
+out:
+	kfree(report);
+
+	return rcode;
+}
+
+#if USE_KTHREAD
+
+static int bio_queue_empty(struct zoned *znd)
+{
+	int empty;
+	unsigned long flags;
+
+	spin_lock_irqsave(&znd->bio_qlck, flags);
+	empty = (znd->bio_in == znd->bio_out) ? 1 : 0;
+	spin_unlock_irqrestore(&znd->bio_qlck, flags);
+
+	return empty;
+}
+
+static inline int bio_queue_next(u32 io)
+{
+	return ((io + 1) & 0x1FF);
+}
+
+static int bio_queue_full(struct zoned *znd)
+{
+	int full;
+
+	spin_lock(&znd->bio_qlck);
+	full = (znd->bio_out == bio_queue_next(znd->bio_in)) ? 1 : 0;
+	spin_unlock(&znd->bio_qlck);
+
+	return full;
+}
+
+static void bio_queue_add(struct zoned *znd, struct bio *bio)
+{
+	int retry = 0;
+
+	do {
+		retry = 0;
+		if (bio_queue_full(znd))
+			wait_event_interruptible(znd->bio_wait,
+				!bio_queue_full(znd));
+
+		spin_lock(&znd->bio_qlck);
+		if (znd->bio_in == bio_queue_next(znd->bio_out)) {
+			spin_unlock(&znd->bio_qlck);
+			retry = 1;
+		}
+	} while (retry);
+
+	znd->bio_queue[znd->bio_in] = bio;
+	znd->bio_in = bio_queue_next(znd->bio_in);
+	spin_unlock(&znd->bio_qlck);
+}
+
+static struct bio *bio_queue_get(struct zoned *znd)
+{
+	struct bio *bio = NULL;
+
+	spin_lock(&znd->bio_qlck);
+	if (znd->bio_in != znd->bio_out) {
+		bio = znd->bio_queue[znd->bio_out];
+		znd->bio_out = bio_queue_next(znd->bio_out);
+	}
+	spin_unlock(&znd->bio_qlck);
+
+	if (bio)
+		wake_up_process(znd->bio_kthread); /* add may be waiting */
+
+	return bio;
+}
+
+static int znd_bio_kthread(void *arg)
+{
+	int err = 0;
+	struct zoned *znd = (struct zoned *)arg;
+
+	Z_ERR(znd, "znd_bio_kthread [started]");
+
+	while (!kthread_should_stop()) {
+		struct bio *bio;
+
+		bio = bio_queue_get(znd);
+		if (!bio) {
+			wait_event_interruptible(znd->bio_wait,
+				!bio_queue_empty(znd));
+			continue;
+		}
+
+		err = zoned_bio(znd, bio);
+		if (err < 0) {
+			znd->meta_result = err;
+			goto out;
+		}
+	}
+	err = 0;
+
+	Z_ERR(znd, "znd_bio_kthread [stopped]");
+
+out:
+	return err;
+}
+
+static int zoned_map(struct dm_target *ti, struct bio *bio)
+{
+	struct zoned *znd = ti->private;
+
+	bio_queue_add(znd, bio);
+	wake_up_process(znd->bio_kthread);
+
+	return DM_MAPIO_SUBMITTED;
+}
+
+#else
+
+static int zoned_map(struct dm_target *ti, struct bio *bio)
+{
+	struct zoned *znd = ti->private;
+	int err = zoned_bio(znd, bio);
+
+	if (err < 0) {
+		znd->meta_result = err;
+		err = 0;
+	}
+
+	return err;
+}
+
+#endif
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_actual_size(struct dm_target *ti, struct zoned *znd)
+{
+	znd->nr_blocks = i_size_read(get_bdev_bd_inode(znd)) / Z_C4K;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * <data dev> <format|check|force>
+ */
+static int zoned_ctr(struct dm_target *ti, unsigned argc, char **argv)
+{
+	const int reset_non_empty = 0;
+	int create = 0;
+	int force = 0;
+	int zbc_probe = 1;
+	int zac_probe = 1;
+	int trim = 1;
+	int r;
+	struct zoned *znd;
+	long long first_data_zone = 0;
+	long long mz_md_provision = MZ_METADATA_ZONES;
+
+	BUILD_BUG_ON(Z_C4K != (sizeof(struct map_sect_to_lba) * Z_UNSORTED));
+	BUILD_BUG_ON(Z_C4K != (sizeof(struct io_4k_block)));
+	BUILD_BUG_ON(Z_C4K != (sizeof(struct mz_superkey)));
+
+	if (argc < 1) {
+		ti->error = "Invalid argument count";
+		return -EINVAL;
+	}
+
+	for (r = 1; r < argc; r++) {
+		if (isdigit(*argv[r])) {
+			int krc = kstrtoll(argv[r], 0, &first_data_zone);
+
+			if (krc != 0) {
+				DMERR("Failed to parse %s: %d", argv[r], krc);
+				first_data_zone = 0;
+			}
+		}
+		if (!strcasecmp("create", argv[r]))
+			create = 1;
+		if (!strcasecmp("load", argv[r]))
+			create = 0;
+		if (!strcasecmp("force", argv[r]))
+			force = 1;
+		if (!strcasecmp("nozbc", argv[r]))
+			zbc_probe = 0;
+		if (!strcasecmp("nozac", argv[r]))
+			zac_probe = 0;
+		if (!strcasecmp("discard", argv[r]))
+			trim = 1;
+		if (!strcasecmp("nodiscard", argv[r]))
+			trim = 0;
+
+		if (!strncasecmp("reserve=", argv[r], 8)) {
+			long long mz_resv;
+			int krc = kstrtoll(argv[r] + 8, 0, &mz_resv);
+
+			if (krc == 0) {
+				if (mz_resv > mz_md_provision)
+					mz_md_provision = mz_resv;
+			} else {
+				DMERR("Reserved 'FAILED TO PARSE.' %s: %d",
+					argv[r]+8, krc);
+				mz_resv = 0;
+			}
+		}
+	}
+
+	znd = ZDM_ALLOC(NULL, sizeof(*znd), KM_00, NORMAL);
+	if (!znd) {
+		ti->error = "Error allocating zoned structure";
+		return -ENOMEM;
+	}
+
+	znd->ti = ti;
+	ti->private = znd;
+	znd->zdstart = first_data_zone;
+	znd->mz_provision = mz_md_provision;
+
+	r = dm_get_device(ti, argv[0], FMODE_READ | FMODE_WRITE, &znd->dev);
+	if (r) {
+		ti->error = "Error opening backing device";
+		zoned_destroy(znd);
+		return -EINVAL;
+	}
+
+	if (znd->dev->bdev) {
+		bdevname(znd->dev->bdev, znd->bdev_name);
+		znd->start_sect = get_start_sect(znd->dev->bdev) >> 3;
+	}
+
+	/*
+	 * Set if this target needs to receive flushes regardless of
+	 * whether or not its underlying devices have support.
+	 */
+	ti->flush_supported = true;
+
+	/*
+	 * Set if this target needs to receive discards regardless of
+	 * whether or not its underlying devices have support.
+	 */
+	ti->discards_supported = true;
+
+	/*
+	 * Set if the target required discard bios to be split
+	 * on max_io_len boundary.
+	 */
+	ti->split_discard_bios = false;
+
+	/*
+	 * Set if this target does not return zeroes on discarded blocks.
+	 */
+	ti->discard_zeroes_data_unsupported = false;
+
+	/*
+	 * Set if this target wants discard bios to be sent.
+	 */
+	ti->num_discard_bios = 1;
+
+	if (!trim) {
+		ti->discards_supported = false;
+		ti->num_discard_bios = 0;
+	}
+
+	zoned_actual_size(ti, znd);
+	znd->callbacks.congested_fn = zoned_is_congested;
+	dm_table_add_target_callbacks(ti->table, &znd->callbacks);
+
+	r = do_init_zoned(ti, znd);
+	if (r) {
+		ti->error = "Error in zoned init";
+		zoned_destroy(znd);
+		return -EINVAL;
+	}
+	if (zbc_probe) {
+		Z_ERR(znd, "Checking for ZONED support %s",
+			trim ? "with trim" : "");
+		is_zoned_inquiry(znd, trim, 0);
+	} else if (zac_probe) {
+		Z_ERR(znd, "Checking for ZONED [ATA PASSTHROUGH] support %s",
+			trim ? "with trim" : "");
+		is_zoned_inquiry(znd, trim, 1);
+	} else {
+		Z_ERR(znd, "No PROBE");
+		set_discard_support(znd, trim);
+	}
+	r = zoned_init_disk(ti, znd, create, force);
+	if (r) {
+		ti->error = "Error in zoned init from disk";
+		zoned_destroy(znd);
+		return -EINVAL;
+	}
+	r = zoned_wp_sync(znd, reset_non_empty);
+	if (r) {
+		ti->error = "Error in zoned re-sync WP";
+		zoned_destroy(znd);
+		return -EINVAL;
+	}
+
+#if USE_KTHREAD
+	znd->bio_kthread = kthread_run(znd_bio_kthread, znd, "zdm-bio-%s",
+		znd->bdev_name);
+	if (!znd->bio_kthread) {
+		ti->error = "Couldn't alloc kthread";
+		zoned_destroy(znd);
+		return -EINVAL;
+	}
+#endif
+	r = zdm_create_proc_entries(znd);
+	if (r) {
+		ti->error = "Failed to create /proc entries";
+		zoned_destroy(znd);
+		return -EINVAL;
+	}
+
+	mod_timer(&znd->timer, jiffies + msecs_to_jiffies(5000));
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_dtr(struct dm_target *ti)
+{
+	struct zoned *znd = ti->private;
+
+	if (znd->z_sballoc) {
+		struct mz_superkey *key_blk = znd->z_sballoc;
+		struct zdm_superblock *sblock = &key_blk->sblock;
+
+		sblock->flags = cpu_to_le32(0);
+		sblock->csum = sb_crc32(sblock);
+	}
+
+#if USE_KTHREAD
+	wake_up_process(znd->bio_kthread);
+	wait_event(znd->bio_wait, bio_queue_empty(znd));
+	kthread_stop(znd->bio_kthread);
+#endif
+
+	zdm_remove_proc_entries(znd);
+
+	zoned_destroy(znd);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Read or write a chunk aligned and sized block of data from a device.
+ */
+static void do_io_work(struct work_struct *work)
+{
+	struct z_io_req_t *req = container_of(work, struct z_io_req_t, work);
+	struct dm_io_request *io_req = req->io_req;
+	unsigned long error_bits = 0;
+
+	req->result = dm_io(io_req, 1, req->where, &error_bits);
+	if (error_bits)
+		DMERR("ERROR: dm_io error: %lx", error_bits);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int znd_async_io(struct zoned *znd,
+			enum dm_io_mem_type dtype,
+			void *data,
+			sector_t block, unsigned int nDMsect, int rw, int queue,
+			io_notify_fn callback, void *context)
+{
+	unsigned long error_bits = 0;
+	int rcode;
+	struct dm_io_region where = {
+		.bdev = znd->dev->bdev,
+		.sector = block,
+		.count = nDMsect,
+	};
+	struct dm_io_request io_req = {
+		.bi_rw = rw,
+		.mem.type = dtype,
+		.mem.offset = 0,
+		.mem.ptr.vma = data,
+		.client = znd->io_client,
+		.notify.fn = callback,
+		.notify.context = context,
+	};
+
+	MutexLock(&znd->io_lock);
+	switch (dtype) {
+	case DM_IO_KMEM:
+		io_req.mem.ptr.addr = data;
+		break;
+	case DM_IO_BIO:
+		io_req.mem.ptr.bio = data;
+		where.count = nDMsect;
+		break;
+	case DM_IO_VMA:
+		io_req.mem.ptr.vma = data;
+		break;
+	default:
+		Z_ERR(znd, "page list not handled here ..  see dm-io.");
+		break;
+	}
+
+	if (queue) {
+		struct z_io_req_t req;
+
+		/*
+		 * Issue the synchronous I/O from a different thread
+		 * to avoid generic_make_request recursion.
+		 */
+		INIT_WORK_ONSTACK(&req.work, do_io_work);
+		req.where = &where;
+		req.io_req = &io_req;
+		queue_work(znd->io_wq, &req.work);
+		flush_workqueue(znd->io_wq);
+		destroy_work_on_stack(&req.work);
+
+		rcode = req.result;
+		if (rcode < 0)
+			Z_ERR(znd, "ERROR: dm_io error: %d", rcode);
+		goto out;
+	}
+
+	rcode = dm_io(&io_req, 1, &where, &error_bits);
+	if (error_bits || rcode < 0)
+		Z_ERR(znd, "ERROR: dm_io error: %d -- %lx", rcode, error_bits);
+
+out:
+	mutex_unlock(&znd->io_lock);
+
+	return rcode;
+}
+
+
+static int block_io(struct zoned *znd,
+		    enum dm_io_mem_type dtype,
+		    void *data, sector_t s, unsigned int n, int rw, int queue)
+{
+	return znd_async_io(znd, dtype, data, s, n, rw, queue, NULL, NULL);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * count -> count in 4k sectors.
+ */
+static int read_block(struct dm_target *ti, enum dm_io_mem_type dtype,
+		      void *data, u64 lba, unsigned int count, int queue)
+{
+	struct zoned *znd = ti->private;
+	sector_t block = lba << Z_SHFT4K;
+	unsigned int nDMsect = count << Z_SHFT4K;
+	int rc;
+
+	if (lba >= znd->nr_blocks) {
+		Z_ERR(znd, "Error reading past end of media: %llx.", lba);
+		rc = -EIO;
+		return rc;
+	}
+
+	rc = block_io(znd, dtype, data, block, nDMsect, READ, queue);
+	if (rc) {
+		Z_ERR(znd, "read error: %d -- R: %llx [%u dm sect] (Q:%d)",
+			rc, lba, nDMsect, queue);
+		dump_stack();
+	}
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * count -> count in 4k sectors.
+ */
+static int write_block(struct dm_target *ti, enum dm_io_mem_type dtype,
+		       void *data, u64 lba, unsigned int count, int queue)
+{
+	struct zoned *znd = ti->private;
+	sector_t block = lba << Z_SHFT4K;
+	unsigned int nDMsect = count << Z_SHFT4K;
+	int rc;
+
+	rc = block_io(znd, dtype, data, block, nDMsect, WRITE, queue);
+	if (rc) {
+		Z_ERR(znd, "write error: %d W: %llx [%u dm sect] (Q:%d)",
+			rc, lba, nDMsect, queue);
+		dump_stack();
+	}
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zm_cow(struct zoned *znd, struct bio *bio, u64 s_zdm, u32 blks,
+		  u64 origin)
+{
+	struct dm_target *ti = znd->ti;
+	int count = 1;
+	int use_wq = 1;
+	unsigned int bytes = bio_cur_bytes(bio);
+	u8 *data = bio_data(bio);
+	u8 *io = NULL;
+	u16 ua_off = bio->bi_iter.bi_sector & 0x0007;
+	u16 ua_size = bio->bi_iter.bi_size & 0x0FFF;	/* in bytes */
+	u32 mapped = 0;
+	u64 disk_lba = 0;
+
+	if (!znd->cow_block)
+		znd->cow_block = ZDM_ALLOC(znd, Z_C4K, PG_02, CRIT);
+
+	io = znd->cow_block;
+	if (!io)
+		return -EIO;
+
+	disk_lba = z_acquire(znd, Z_AQ_STREAM_ID, blks, &mapped);
+	if (!disk_lba || !mapped)
+		return -ENOSPC;
+
+	while (bytes) {
+		int ioer;
+		unsigned int iobytes = Z_C4K;
+
+		/* ---------------------------------------------------------- */
+		if (origin) {
+			if (s_zdm != znd->cow_addr) {
+				Z_ERR(znd, "Copy block from %llx <= %llx",
+				      origin, s_zdm);
+				ioer = read_block(ti, DM_IO_KMEM, io, origin,
+						count, use_wq);
+				if (ioer)
+					return -EIO;
+
+				znd->cow_addr = s_zdm;
+			} else {
+				Z_ERR(znd, "Cached block from %llx <= %llx",
+				      origin, s_zdm);
+			}
+		} else {
+			memset(io, 0, Z_C4K);
+		}
+
+		if (ua_off)
+			iobytes -= ua_off * 512;
+
+		if (bytes < iobytes)
+			iobytes = bytes;
+
+		Z_ERR(znd, "Moving %u bytes from origin [offset:%u]",
+		      iobytes, ua_off * 512);
+
+		memcpy(io + (ua_off * 512), data, iobytes);
+
+		/* ---------------------------------------------------------- */
+
+		ioer = write_block(ti, DM_IO_KMEM, io, disk_lba, count, use_wq);
+		if (ioer)
+			return -EIO;
+
+		ioer = z_mapped_addmany(znd, s_zdm, disk_lba, mapped, CRIT);
+		if (ioer) {
+			Z_ERR(znd, "%s: Journal MANY failed.", __func__);
+			return -EIO;
+		}
+
+		data += iobytes;
+		bytes -= iobytes;
+		ua_size -= (ua_size > iobytes) ? iobytes : ua_size;
+		ua_off = 0;
+		disk_lba++;
+
+		if (bytes && (ua_size || ua_off)) {
+			s_zdm++;
+			origin = current_mapping(znd, s_zdm, CRIT);
+		}
+	}
+	bio_endio(bio);
+
+	return DM_MAPIO_SUBMITTED;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+#define BIO_CACHE_SECTORS (IO_VCACHE_PAGES * Z_BLOCKS_PER_DM_SECTOR)
+
+/**
+ * Write 4k blocks from cache to lba.
+ * Move any remaining 512 byte blocks to the start of cache and update
+ * the @_blen count is updated
+ */
+static int zm_write_cache(struct zoned *znd, struct io_dm_block *dm_vbuf,
+			  u64 lba, u32 *_blen)
+{
+	int use_wq    = 1;
+	int cached    = *_blen;
+	int blks      = cached >> 3;
+	int sectors   = blks << 3;
+	int remainder = cached - sectors;
+	int err;
+
+	err = write_block(znd->ti, DM_IO_VMA, dm_vbuf, lba, blks, use_wq);
+	if (!err) {
+		if (remainder)
+			memcpy(dm_vbuf[0].data,
+			       dm_vbuf[sectors].data, remainder * 512);
+		*_blen = remainder;
+	}
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zm_write_one_page(struct zoned *znd, struct bio *bio, u64 s_zdm)
+{
+	u32 acqflgs = Z_AQ_STREAM_ID | bio_stream(bio);
+	u64 lba     = 0;
+	u32 mapped  = 0;
+	int err     = -EIO;
+
+reacquire:
+	/*
+	 * When lba is zero no blocks were not allocated.
+	 * Retry with the smaller request
+	 */
+	lba = z_acquire(znd, acqflgs, 1, &mapped);
+	if (!lba && mapped)
+		lba = z_acquire(znd, acqflgs, mapped, &mapped);
+
+	if (!lba) {
+		if (znd->gc_throttle.counter == 0) {
+			err = -ENOSPC;
+			goto out;
+		}
+
+		Z_ERR(znd, "Throttle input ... Mandatory GC.");
+		if (delayed_work_pending(&znd->gc_work)) {
+			mod_delayed_work(znd->gc_wq, &znd->gc_work, 0);
+			mutex_unlock(&znd->mz_io_mutex);
+			flush_delayed_work(&znd->gc_work);
+			MutexLock(&znd->mz_io_mutex);
+		}
+		goto reacquire;
+	}
+
+	bio->bi_iter.bi_sector = lba << Z_SHFT4K;
+	err = z_mapped_addmany(znd, s_zdm, lba, mapped, CRIT);
+	if (err) {
+		Z_ERR(znd, "%s: Journal MANY failed.", __func__);
+		err = DM_MAPIO_REQUEUE;
+		goto out;
+	}
+	err = DM_MAPIO_REMAPPED;
+
+out:
+	return err;
+
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zm_write_pages(struct zoned *znd, struct bio *bio, u64 s_zdm)
+{
+	u32 blks     = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u64 lba      = 0;
+	u32 blen     = 0; /* total: IO_VCACHE_PAGES * 8 */
+	u32 written  = 0;
+	int avail    = 0;
+	u32 acqflgs  = Z_AQ_STREAM_ID | bio_stream(bio);
+	int err;
+	struct bvec_iter start;
+	struct bvec_iter iter;
+	struct bio_vec bv;
+	struct io_4k_block *io_vcache;
+	struct io_dm_block *dm_vbuf = NULL;
+
+	MutexLock(&znd->vcio_lock);
+	io_vcache = get_io_vcache(znd, CRIT);
+	if (!io_vcache) {
+		Z_ERR(znd, "%s: FAILED to get SYNC CACHE.", __func__);
+		err = -ENOMEM;
+		goto out;
+	}
+
+	dm_vbuf = (struct io_dm_block *)io_vcache;
+
+	/* USE: dm_vbuf for dumping bio pages to disk ... */
+	start = bio->bi_iter; /* struct implicit copy */
+	do {
+		u64 alloc_ori = 0;
+		u32 mcount = 0;
+		u32 mapped = 0;
+
+reacquire:
+		/*
+		 * When lba is zero no blocks were not allocated.
+		 * Retry with the smaller request
+		 */
+		lba = z_acquire(znd, acqflgs, blks - written, &mapped);
+		if (!lba && mapped)
+			lba = z_acquire(znd, acqflgs, mapped, &mapped);
+
+		if (!lba) {
+			if (znd->gc_throttle.counter == 0) {
+				err = -ENOSPC;
+				goto out;
+			}
+
+			Z_ERR(znd, "Throttle input ... Mandatory GC.");
+			if (delayed_work_pending(&znd->gc_work)) {
+				mod_delayed_work(znd->gc_wq, &znd->gc_work, 0);
+				mutex_unlock(&znd->mz_io_mutex);
+				flush_delayed_work(&znd->gc_work);
+				MutexLock(&znd->mz_io_mutex);
+			}
+			goto reacquire;
+		}
+
+		/* this may be redundant .. if we have lba we have mapped > 0 */
+		if (lba && mapped)
+			avail += mapped * 8; /* claimed pages in dm blocks */
+
+		alloc_ori = lba;
+
+		/* copy [upto mapped] pages to buffer */
+		__bio_for_each_segment(bv, bio, iter, start) {
+			int issue_write = 0;
+			unsigned int boff;
+			void *src;
+
+			if (avail <= 0) {
+				Z_ERR(znd, "%s: TBD: Close Z# %llu",
+					__func__, alloc_ori >> 16);
+				start = iter;
+				break;
+			}
+
+			src = kmap_atomic(bv.bv_page);
+			boff = bv.bv_offset;
+			memcpy(dm_vbuf[blen].data, src + boff, bv.bv_len);
+			kunmap_atomic(src);
+			blen   += bv.bv_len / 512;
+			avail  -= bv.bv_len / 512;
+
+			if ((blen >= (mapped * 8)) ||
+			    (blen >= (BIO_CACHE_SECTORS - 8)))
+				issue_write = 1;
+
+			/*
+			 * If there is less than 1 4k block in out cache,
+			 * send the available blocks to disk
+			 */
+			if (issue_write) {
+				int blks = blen / 8;
+
+				err = zm_write_cache(znd, dm_vbuf, lba, &blen);
+				if (err) {
+					Z_ERR(znd, "%s: bio-> %" PRIx64
+					      " [%d of %d blks] -> %d",
+					      __func__, lba, blen, blks, err);
+					bio->bi_error = err;
+					bio_endio(bio);
+					goto out;
+				}
+				lba     += blks;
+				written += blks;
+				mcount  += blks;
+				mapped  -= blks;
+
+				if (mapped == 0) {
+					bio_advance_iter(bio, &iter, bv.bv_len);
+					start = iter;
+					break;
+				}
+				if (mapped < 0) {
+					Z_ERR(znd, "ERROR: Bad write %"
+					      PRId32 " beyond alloc'd space",
+					      mapped);
+				}
+			}
+		} /* end: __bio_for_each_segment */
+		if ((mapped > 0) && ((blen / 8) > 0)) {
+			int blks = blen / 8;
+
+			err = zm_write_cache(znd, dm_vbuf, lba, &blen);
+			if (err) {
+				Z_ERR(znd, "%s: bio-> %" PRIx64
+				      " [%d of %d blks] -> %d",
+				      __func__, lba, blen, blks, err);
+				bio->bi_error = err;
+				bio_endio(bio);
+				goto out;
+			}
+			lba     += blks;
+			written += blks;
+			mcount  += blks;
+			mapped  -= blks;
+
+			if (mapped < 0) {
+				Z_ERR(znd, "ERROR: [2] Bad write %"
+				      PRId32 " beyond alloc'd space",
+				      mapped);
+			}
+		}
+		err = z_mapped_addmany(znd, s_zdm, alloc_ori, mcount, CRIT);
+		if (err) {
+			Z_ERR(znd, "%s: Journal MANY failed.", __func__);
+			err = DM_MAPIO_REQUEUE;
+			/*
+			 * FIXME:
+			 * Ending the BIO here is causing a GFP:
+			 -       DEBUG_PAGEALLOC
+			 -    in Workqueue:
+			 -        writeback bdi_writeback_workfn (flush-252:0)
+			 -    backtrace:
+			 -      __map_bio+0x7a/0x280
+			 -      __split_and_process_bio+0x2e3/0x4e0
+			 -      ? __split_and_process_bio+0x22/0x4e0
+			 -      ? generic_start_io_acct+0x5/0x210
+			 -      dm_make_request+0x6b/0x100
+			 -      generic_make_request+0xc0/0x110
+			 -      ....
+			 -
+			 - bio->bi_error = err;
+			 - bio_endio(bio);
+			 */
+			goto out;
+		}
+
+		if (written < blks)
+			s_zdm += written;
+
+		if (written == blks && blen > 0)
+			Z_ERR(znd, "%s: blen: %d un-written blocks!!",
+			      __func__, blen);
+	} while (written < blks);
+	bio_endio(bio);
+	err = DM_MAPIO_SUBMITTED;
+
+out:
+	put_io_vcache(znd, io_vcache);
+	mutex_unlock(&znd->vcio_lock);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int is_empty_page(struct zoned *znd, void *pg, size_t len)
+{
+	u64 *chk = pg;
+	size_t count = len / sizeof(*chk);
+	size_t entry;
+
+	for (entry = 0; entry < count; entry++) {
+		if (chk[entry])
+			return 0;
+	}
+	return 1;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int is_zero_bio(struct zoned *znd, struct bio *bio)
+{
+	int is_empty = 0;
+	struct bvec_iter iter;
+	struct bio_vec bv;
+
+	/* Scan bio to determine if it is zero'd */
+	bio_for_each_segment(bv, bio, iter) {
+		unsigned int boff;
+		void *src;
+
+		src = kmap_atomic(bv.bv_page);
+		boff = bv.bv_offset;
+		is_empty = is_empty_page(znd, src + boff, bv.bv_len);
+		kunmap_atomic(src);
+
+		if (!is_empty)
+			break;
+	} /* end: __bio_for_each_segment */
+
+	return is_empty;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_map_write(struct zoned *znd, struct bio *bio, u64 s_zdm)
+{
+	u32 blks     = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u16 ua_off   = bio->bi_iter.bi_sector & 0x0007;
+	u16 ua_size  = bio->bi_iter.bi_size & 0x0FFF;	/* in bytes */
+	int rcode    = -EIO;
+
+	if (ua_size || ua_off) {
+		u64 origin;
+
+		MutexLock(&znd->mz_io_mutex);
+		origin = current_mapping(znd, s_zdm, CRIT);
+		if (origin)
+			rcode = zm_cow(znd, bio, s_zdm, blks, origin);
+		mutex_unlock(&znd->mz_io_mutex);
+		return rcode;
+	}
+
+	/* on RAID 4/5/6 all writes are 4k */
+	if (blks == 1) {
+		if (is_zero_bio(znd, bio)) {
+			rcode = zoned_map_discard(znd, bio);
+		} else {
+			MutexLock(&znd->mz_io_mutex);
+			rcode = zm_write_one_page(znd, bio, s_zdm);
+			mutex_unlock(&znd->mz_io_mutex);
+		}
+		return rcode;
+	}
+
+	MutexLock(&znd->mz_io_mutex);
+	rcode = zm_write_pages(znd, bio, s_zdm);
+	mutex_unlock(&znd->mz_io_mutex);
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_map_read(struct zoned *znd, struct bio *bio)
+{
+	int rcode = DM_MAPIO_SUBMITTED;
+	u64 ua_off = bio->bi_iter.bi_sector & 0x0007;
+	u64 ua_size = bio->bi_iter.bi_size & 0x0FFF;	/* in bytes */
+	u64 s_zdm = (bio->bi_iter.bi_sector >> Z_SHFT4K) + znd->md_end;
+	u64 blks = dm_div_up(bio->bi_iter.bi_size, Z_C4K);
+	u64 start_lba;
+
+	start_lba = current_mapping(znd, s_zdm, CRIT);
+	if (start_lba) {
+		u64 sz;
+
+		bio->bi_iter.bi_sector = start_lba << Z_SHFT4K;
+		if (ua_off)
+			bio->bi_iter.bi_sector += ua_off;
+
+		for (sz = 1; sz < blks; sz++) {
+			u64 next_lba;
+
+			next_lba = current_mapping(znd, s_zdm + sz, CRIT);
+			if (next_lba != (start_lba + sz)) {
+				unsigned nsect = sz * 8;
+
+				if (ua_size) {
+					unsigned ua_blocks = ua_size / 512;
+
+					nsect -= 8;
+					nsect += ua_blocks;
+				}
+				Z_DBG(znd,
+					"NON SEQ @ %llx + %llu [%llx] [%llx]",
+					 s_zdm + sz, sz, start_lba, next_lba);
+
+				dm_accept_partial_bio(bio, nsect);
+				break;
+			}
+		}
+
+		if (ua_off || ua_size)
+			Z_ERR(znd, "(R): bio: sector: %lx bytes: %u",
+			      bio->bi_iter.bi_sector, bio->bi_iter.bi_size);
+
+		generic_make_request(bio);
+	} else {
+		zero_fill_bio(bio);
+		bio_endio(bio);
+	}
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_bio(struct zoned *znd, struct bio *bio)
+{
+	bool is_write = (bio_data_dir(bio) == WRITE);
+	u64 s_zdm = (bio->bi_iter.bi_sector >> Z_SHFT4K) + znd->md_end;
+	int rcode = DM_MAPIO_SUBMITTED;
+	struct request_queue *q;
+	int force_sync_now = 0;
+
+	/* map to backing device ... NOT dm-zoned device */
+	bio->bi_bdev = znd->dev->bdev;
+
+	q = bdev_get_queue(bio->bi_bdev);
+	q->queue_flags |= QUEUE_FLAG_NOMERGES;
+
+	if (is_write && znd->meta_result) {
+		if (!(bio->bi_rw & REQ_DISCARD)) {
+			rcode = znd->meta_result;
+			Z_ERR(znd, "MAP ERR (meta): %d", rcode);
+			goto out;
+		}
+	}
+
+	/* check for SYNC flag */
+	if (bio->bi_rw & REQ_SYNC) {
+		set_bit(DO_SYNC, &znd->flags);
+		force_sync_now = 1;
+	}
+
+	Z_DBG(znd, "%s: s:%"PRIx64" sz:%u -> %s", __func__, s_zdm,
+	      bio->bi_iter.bi_size, is_write ? "W" : "R");
+
+	if (bio->bi_iter.bi_size) {
+		if (bio->bi_rw & REQ_DISCARD) {
+			rcode = zoned_map_discard(znd, bio);
+		} else if (is_write) {
+			znd->is_empty = 0;
+			rcode = zoned_map_write(znd, bio, s_zdm);
+		} else {
+			MutexLock(&znd->mz_io_mutex);
+			rcode = zoned_map_read(znd, bio);
+			mutex_unlock(&znd->mz_io_mutex);
+		}
+		znd->age = jiffies;
+	}
+
+	if (test_bit(DO_SYNC, &znd->flags) ||
+	    test_bit(DO_JOURNAL_MOVE, &znd->flags) ||
+	    test_bit(DO_MEMPOOL, &znd->flags)) {
+		if (!test_bit(DO_METAWORK_QD, &znd->flags) &&
+		    !work_pending(&znd->meta_work)) {
+			set_bit(DO_METAWORK_QD, &znd->flags);
+			queue_work(znd->meta_wq, &znd->meta_work);
+		}
+	}
+
+	if (znd->z_gc_free < 5) {
+		Z_ERR(znd, "... issue gc low on free space");
+		MutexLock(&znd->mz_io_mutex);
+		gc_immediate(znd, CRIT);
+		mutex_unlock(&znd->mz_io_mutex);
+	}
+
+	if (force_sync_now && work_pending(&znd->meta_work))
+		flush_workqueue(znd->meta_wq);
+
+out:
+	if (rcode == DM_MAPIO_REMAPPED || rcode == DM_MAPIO_SUBMITTED)
+		goto done;
+	if (rcode < 0 || rcode == DM_MAPIO_REQUEUE) {
+		Z_ERR(znd, "MAP ERR: %d", rcode);
+		Z_ERR(znd, "%s: s:%"PRIx64" sz:%u -> %s", __func__, s_zdm,
+		      bio->bi_iter.bi_size, is_write ? "W" : "R");
+		dump_stack();
+	} else {
+		Z_ERR(znd, "MAP ERR: %d", rcode);
+		Z_ERR(znd, "%s: s:%"PRIx64" sz:%u -> %s", __func__, s_zdm,
+		      bio->bi_iter.bi_size, is_write ? "W" : "R");
+		dump_stack();
+		rcode = -EIO;
+	}
+done:
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int _do_mem_purge(struct zoned *znd)
+{
+	int do_work = 0;
+
+	if (znd->incore_count > 3) {
+		set_bit(DO_MEMPOOL, &znd->flags);
+		if (!work_pending(&znd->meta_work))
+			do_work = 1;
+	}
+	return do_work;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void on_timeout_activity(struct zoned *znd, int mempurge, int delay)
+{
+	if (test_bit(ZF_FREEZE, &znd->flags))
+		return;
+
+	gc_queue_with_delay(znd, delay, CRIT);
+
+	if (mempurge && _do_mem_purge(znd))
+		queue_work(znd->meta_wq, &znd->meta_work);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void bg_work_task(struct work_struct *work)
+{
+	struct zoned *znd;
+
+	if (!work)
+		return;
+
+	znd = container_of(work, struct zoned, bg_work);
+	on_timeout_activity(znd, 1, 1);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void activity_timeout(unsigned long data)
+{
+	struct zoned *znd = (struct zoned *) data;
+
+	if (!work_pending(&znd->bg_work))
+		queue_work(znd->bg_wq, &znd->bg_work);
+
+	if (!test_bit(ZF_FREEZE, &znd->flags))
+		mod_timer(&znd->timer, jiffies + msecs_to_jiffies(2500));
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static sector_t get_dev_size(struct dm_target *ti)
+{
+	struct zoned *znd = ti->private;
+	u64 sz = i_size_read(get_bdev_bd_inode(znd));	/* size in bytes. */
+	u64 lut_resv;
+
+	lut_resv = (znd->gz_count * znd->mz_provision);
+
+	Z_DBG(znd, "%s size: %llu (/8) -> %llu blks -> zones -> %llu mz: %llu",
+		 __func__, sz, sz / 4096, (sz / 4096) / 65536,
+		 ((sz / 4096) / 65536) / 1024);
+
+	sz -= (lut_resv * Z_SMR_SZ_BYTES);
+
+	Z_DBG(znd, "%s backing device size: %llu (4k blocks)", __func__, sz);
+
+	/*
+	 * NOTE: `sz` should match `ti->len` when the dm_table
+	 *       is setup correctly
+	 */
+
+	return to_sector(sz);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_iterate_devices(struct dm_target *ti,
+				 iterate_devices_callout_fn fn, void *data)
+{
+	struct zoned *znd = ti->private;
+	int rc = fn(ti, znd->dev, 0, get_dev_size(ti), data);
+
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_io_hints(struct dm_target *ti, struct queue_limits *limits)
+{
+	u64 io_opt_sectors = limits->io_opt >> SECTOR_SHIFT;
+
+	/*
+	 * If the system-determined stacked limits are compatible with the
+	 * zoned device's blocksize (io_opt is a factor) do not override them.
+	 */
+	if (io_opt_sectors < 8 || do_div(io_opt_sectors, 8)) {
+		blk_limits_io_min(limits, 0);
+		blk_limits_io_opt(limits, 8 << SECTOR_SHIFT);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_status(struct dm_target *ti, status_type_t type,
+			 unsigned status_flags, char *result, unsigned maxlen)
+{
+	struct zoned *znd = (struct zoned *) ti->private;
+
+	switch (type) {
+	case STATUSTYPE_INFO:
+		result[0] = '\0';
+		break;
+
+	case STATUSTYPE_TABLE:
+		scnprintf(result, maxlen, "%s Z#%u", znd->dev->name,
+			 znd->zdstart);
+		break;
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* --ProcFS Support Routines------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+#if defined(CONFIG_PROC_FS)
+
+/**
+ * struct zone_info_entry - Proc zone entry.
+ * @zone: Zone Index
+ * @info: Info (WP/Used).
+ */
+struct zone_info_entry {
+	u32 zone;
+	u32 info;
+};
+
+/**
+ * Startup writing to our proc entry
+ */
+static void *proc_wp_start(struct seq_file *seqf, loff_t *pos)
+{
+	struct zoned *znd = seqf->private;
+
+	if (*pos == 0)
+		znd->wp_proc_at = *pos;
+	return &znd->wp_proc_at;
+}
+
+/**
+ * Increment to our next 'grand zone' 4k page.
+ */
+static void *proc_wp_next(struct seq_file *seqf, void *v, loff_t *pos)
+{
+	struct zoned *znd = seqf->private;
+	u32 zone = ++znd->wp_proc_at;
+
+	return zone < znd->data_zones ? &znd->wp_proc_at : NULL;
+}
+
+/**
+ * Stop ... a place to free resources that we don't hold .. [noop].
+ */
+static void proc_wp_stop(struct seq_file *seqf, void *v)
+{
+}
+
+/**
+ * Write as many entries as possbile ....
+ */
+static int proc_wp_show(struct seq_file *seqf, void *v)
+{
+	int err = 0;
+	struct zoned *znd = seqf->private;
+	u32 zone = znd->wp_proc_at;
+	u32 out = 0;
+
+	while (zone < znd->data_zones) {
+		u32 gzno  = zone >> GZ_BITS;
+		u32 gzoff = zone & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		struct zone_info_entry entry;
+
+		entry.zone = zone;
+		entry.info = le32_to_cpu(wpg->wp_alloc[gzoff]);
+
+		err = seq_write(seqf, &entry, sizeof(entry));
+		if (err) {
+			/*
+			 * write failure is temporary ..
+			 * just return and try again
+			 */
+			err = 0;
+			goto out;
+		}
+		out++;
+		zone = ++znd->wp_proc_at;
+	}
+
+out:
+	if (err)
+		Z_ERR(znd, "%s: %llu -> %d", __func__, znd->wp_proc_at, err);
+
+	return err;
+}
+
+/**
+ * zdm_wp_ops() - Seq_file operations for retrieving WP via proc fs
+ */
+static const struct seq_operations zdm_wp_ops = {
+	.start	= proc_wp_start,
+	.next	= proc_wp_next,
+	.stop	= proc_wp_stop,
+	.show	= proc_wp_show
+};
+
+/**
+ * zdm_wp_open() - Need to migrate our private data to the seq_file
+ */
+static int zdm_wp_open(struct inode *inode, struct file *file)
+{
+	/* seq_open will populate file->private_data with a seq_file */
+	int err = seq_open(file, &zdm_wp_ops);
+
+	if (!err) {
+		struct zoned *znd = PDE_DATA(inode);
+		struct seq_file *seqf = file->private_data;
+
+		seqf->private = znd;
+	}
+	return err;
+}
+
+/**
+ * zdm_wp_fops() - File operations for retrieving WP via proc fs
+ */
+static const struct file_operations zdm_wp_fops = {
+	.open		= zdm_wp_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= seq_release,
+};
+
+
+/**
+ * Startup writing to our proc entry
+ */
+static void *proc_used_start(struct seq_file *seqf, loff_t *pos)
+{
+	struct zoned *znd = seqf->private;
+
+	if (*pos == 0)
+		znd->wp_proc_at = *pos;
+	return &znd->wp_proc_at;
+}
+
+/**
+ * Increment to our next zone
+ */
+static void *proc_used_next(struct seq_file *seqf, void *v, loff_t *pos)
+{
+	struct zoned *znd = seqf->private;
+	u32 zone = ++znd->wp_proc_at;
+
+	return zone < znd->data_zones ? &znd->wp_proc_at : NULL;
+}
+
+/**
+ * Stop ... a place to free resources that we don't hold .. [noop].
+ */
+static void proc_used_stop(struct seq_file *seqf, void *v)
+{
+}
+
+/**
+ * proc_used_show() - Write as many 'used' entries as possbile.
+ * @seqf: seq_file I/O handler
+ * @v: An unused parameter.
+ */
+static int proc_used_show(struct seq_file *seqf, void *v)
+{
+	int err = 0;
+	struct zoned *znd = seqf->private;
+	u32 zone = znd->wp_proc_at;
+	u32 out = 0;
+
+	while (zone < znd->data_zones) {
+		u32 gzno  = zone >> GZ_BITS;
+		u32 gzoff = zone & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		struct zone_info_entry entry;
+
+		entry.zone = zone;
+		entry.info = le32_to_cpu(wpg->zf_est[gzoff]);
+
+		err = seq_write(seqf, &entry, sizeof(entry));
+		if (err) {
+			/*
+			 * write failure is temporary ..
+			 * just return and try again
+			 */
+			err = 0;
+			goto out;
+		}
+		out++;
+		zone = ++znd->wp_proc_at;
+	}
+
+out:
+	if (err)
+		Z_ERR(znd, "%s: %llu -> %d", __func__, znd->wp_proc_at, err);
+
+	return err;
+}
+
+/**
+ * zdm_used_ops() - Seq_file Ops for retrieving 'used' state via proc fs
+ */
+static const struct seq_operations zdm_used_ops = {
+	.start	= proc_used_start,
+	.next	= proc_used_next,
+	.stop	= proc_used_stop,
+	.show	= proc_used_show
+};
+
+/**
+ * zdm_used_open() - Need to migrate our private data to the seq_file
+ */
+static int zdm_used_open(struct inode *inode, struct file *file)
+{
+	/* seq_open will populate file->private_data with a seq_file */
+	int err = seq_open(file, &zdm_used_ops);
+
+	if (!err) {
+		struct zoned *znd = PDE_DATA(inode);
+		struct seq_file *seqf = file->private_data;
+
+		seqf->private = znd;
+	}
+	return err;
+}
+
+/**
+ * zdm_used_fops() - File operations for retrieving 'used' state via proc fs
+ */
+static const struct file_operations zdm_used_fops = {
+	.open		= zdm_used_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= seq_release,
+};
+
+/**
+ * zdm_status_show() - Dump the status structure via proc fs
+ */
+static int zdm_status_show(struct seq_file *seqf, void *unused)
+{
+	struct zoned *znd = seqf->private;
+	struct zdm_ioc_status status;
+	u32 zone;
+
+	memset(&status, 0, sizeof(status));
+
+	for (zone = 0; zone < znd->data_zones; zone++) {
+		u32 gzno  = zone >> GZ_BITS;
+		u32 gzoff = zone & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		u32 wp_at = le32_to_cpu(wpg->wp_alloc[gzoff]) & Z_WP_VALUE_MASK;
+
+		status.b_used += wp_at;
+		status.b_available += Z_BLKSZ - wp_at;
+	}
+	status.mc_entries = znd->mc_entries;
+	status.b_discard = znd->discard_count;
+
+	/*  fixed array of ->fwd_tm and ->rev_tm */
+	status.m_zones = znd->data_zones;
+
+	status.memstat = znd->memstat;
+	memcpy(status.bins, znd->bins, sizeof(status.bins));
+	status.mlut_blocks = znd->incore_count;
+
+	return seq_write(seqf, &status, sizeof(status));
+}
+
+/**
+ * zdm_status_open() - Open seq_file from file.
+ * @inode: Our data is stuffed here, Retrieve it.
+ * @file: file objected used by seq_file.
+ */
+static int zdm_status_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, zdm_status_show, PDE_DATA(inode));
+}
+
+/**
+ * zdm_used_fops() - File operations to chain to zdm_status_open.
+ */
+static const struct file_operations zdm_status_fops = {
+	.open = zdm_status_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+/**
+ * zdm_info_show() - Report some information as text.
+ * @seqf: Sequence file for writing
+ * @unused: Not used.
+ */
+static int zdm_info_show(struct seq_file *seqf, void *unused)
+{
+	struct zoned *znd = seqf->private;
+
+	seq_printf(seqf, "Data Zones:   %u\n", znd->data_zones);
+	seq_printf(seqf, "Empty Zones:  %u\n", znd->z_gc_free);
+	seq_printf(seqf, "Cached Pages: %u\n", znd->mc_entries);
+	seq_printf(seqf, "ZTL Pages:    %u\n", znd->incore_count);
+	seq_printf(seqf, "RAM in Use:   %lu\n", znd->memstat);
+
+	return 0;
+}
+
+/**
+ * zdm_info_open() - Open seq_file from file.
+ * @inode: Our data is stuffed here, Retrieve it.
+ * @file: file objected used by seq_file.
+ */
+static int zdm_info_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, zdm_info_show, PDE_DATA(inode));
+}
+
+/**
+ * zdm_used_fops() - File operations to chain to zdm_info_open.
+ */
+static const struct file_operations zdm_info_fops = {
+	.open = zdm_info_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+/**
+ * zdm_create_proc_entries() - Create proc entries for ZDM utilities
+ * @znd: ZDM Instance
+ */
+static int zdm_create_proc_entries(struct zoned *znd)
+{
+	snprintf(znd->proc_name, sizeof(znd->proc_name), "zdm_%s", _zdisk(znd));
+
+	znd->proc_fs = proc_mkdir(znd->proc_name, NULL);
+	if (!znd->proc_fs)
+		return -ENOMEM;
+
+	proc_create_data(PROC_WP, 0, znd->proc_fs, &zdm_wp_fops, znd);
+	proc_create_data(PROC_FREE, 0, znd->proc_fs, &zdm_used_fops, znd);
+	proc_create_data(PROC_DATA, 0, znd->proc_fs, &zdm_status_fops, znd);
+	proc_create_data(PROC_STATUS, 0, znd->proc_fs, &zdm_info_fops, znd);
+
+	return 0;
+}
+
+/**
+ * zdm_remove_proc_entries() - Remove proc entries
+ * @znd: ZDM Instance
+ */
+static void zdm_remove_proc_entries(struct zoned *znd)
+{
+	remove_proc_subtree(znd->proc_name, NULL);
+}
+
+#else /* !CONFIG_PROC_FS */
+
+static int zdm_create_proc_entries(struct zoned *znd)
+{
+	(void)znd;
+	return 0;
+}
+static void zdm_remove_proc_entries(struct zoned *znd)
+{
+	(void)znd;
+}
+#endif /* CONFIG_PROC_FS */
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_ioctl_fwd(struct dm_dev *dev, unsigned int cmd,
+			   unsigned long arg)
+{
+	int r = scsi_verify_blk_ioctl(NULL, cmd);
+
+	if (r == 0)
+		r = __blkdev_driver_ioctl(dev->bdev, dev->mode, cmd, arg);
+
+	return r;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_ioc_wpstat(struct zoned *znd, unsigned long arg, int what)
+{
+	void __user *parg = (void __user *)arg;
+	int error = -EFAULT;
+	struct zdm_ioc_request *req;
+
+	req = kzalloc(sizeof(*req), GFP_KERNEL);
+	if (!req) {
+		error = -ENOMEM;
+		goto out;
+	}
+
+	if (copy_from_user(req, parg, sizeof(*req)))
+		goto out;
+
+	if (req->megazone_nr < znd->gz_count) {
+		struct meta_pg *wpg = &znd->wp[req->megazone_nr];
+		size_t rs = req->result_size < Z_C4K ? req->result_size : Z_C4K;
+		void *send_what = what ? wpg->wp_alloc : wpg->zf_est;
+
+		if (copy_to_user(parg, send_what, rs))
+			goto out;
+
+		error = 0;
+	}
+out:
+	kfree(req);
+
+	return error;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static void fill_ioc_status(struct zoned *znd, struct zdm_ioc_status *status,
+			    u32 mz_no)
+{
+	int entry;
+	struct meta_pg *wpg = &znd->wp[mz_no];
+
+	for (entry = (mz_no << GZ_BITS); entry < znd->data_zones; entry++) {
+		u32 gzoff = entry & GZ_MMSK;
+		u32 wp_at = le32_to_cpu(wpg->wp_alloc[gzoff]) & Z_WP_VALUE_MASK;
+
+		status->b_used += wp_at;
+		status->b_available += Z_BLKSZ - wp_at;
+	}
+	if (mz_no)
+		return;
+
+	status->mc_entries = znd->mc_entries;
+	status->b_discard = znd->discard_count;
+
+	/*  fixed array of ->fwd_tm and ->rev_tm */
+	status->m_zones = znd->data_zones;
+
+	status->memstat = znd->memstat;
+	memcpy(status->bins, znd->bins, sizeof(status->bins));
+	status->mlut_blocks = znd->incore_count;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int do_ioc_status(struct zoned *znd, unsigned long arg)
+{
+	void __user *parg = (void __user *)arg;
+	int error = -EFAULT;
+	struct zdm_ioc_request *req;
+	struct zdm_ioc_status *stats;
+
+	req = kzalloc(sizeof(*req), GFP_KERNEL);
+	stats = kzalloc(sizeof(*stats), GFP_KERNEL);
+
+	if (!req || !stats) {
+		error = -ENOMEM;
+		goto out;
+	}
+
+	if (copy_from_user(req, parg, sizeof(*req)))
+		goto out;
+
+	if (req->megazone_nr < znd->gz_count) {
+		if (req->result_size < sizeof(*stats)) {
+			error = -EBADTYPE;
+			goto out;
+		}
+		fill_ioc_status(znd, stats, req->megazone_nr);
+		if (copy_to_user(parg, stats, sizeof(*stats)))
+			goto out;
+
+		error = 0;
+	}
+
+out:
+	kfree(req);
+	kfree(stats);
+	return error;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_ioctl(struct dm_target *ti, unsigned int cmd,
+		       unsigned long arg)
+{
+	int rcode = 0;
+	struct zoned *znd = (struct zoned *) ti->private;
+
+	switch (cmd) {
+	case ZDM_IOC_MZCOUNT:
+		rcode = znd->gz_count;
+		break;
+	case ZDM_IOC_WPS:
+		do_ioc_wpstat(znd, arg, 1);
+		break;
+	case ZDM_IOC_FREE:
+		do_ioc_wpstat(znd, arg, 0);
+		break;
+	case ZDM_IOC_STATUS:
+		do_ioc_status(znd, arg);
+		break;
+#if USE_KTHREAD
+	case BLKFLSBUF:
+		Z_ERR(znd, "Ign BLKFLSBUF (ZDM: flush backing store) %u %lu\n",
+		      cmd, arg);
+		rcode = 0;
+		break;
+#endif
+	default:
+		rcode = zoned_ioctl_fwd(znd->dev, cmd, arg);
+		break;
+	}
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void start_worker(struct zoned *znd)
+{
+	clear_bit(ZF_FREEZE, &znd->flags);
+	atomic_set(&znd->suspended, 0);
+	mod_timer(&znd->timer, jiffies + msecs_to_jiffies(5000));
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void stop_worker(struct zoned *znd)
+{
+	set_bit(ZF_FREEZE, &znd->flags);
+	atomic_set(&znd->suspended, 1);
+	zoned_io_flush(znd);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void zoned_postsuspend(struct dm_target *ti)
+{
+	struct zoned *znd = ti->private;
+
+	stop_worker(znd);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int zoned_preresume(struct dm_target *ti)
+{
+	struct zoned *znd = ti->private;
+
+	start_worker(znd);
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct target_type zoned_target = {
+	.name = "zoned",
+	.module = THIS_MODULE,
+	.version = {1, 0, 0},
+	.ctr = zoned_ctr,
+	.dtr = zoned_dtr,
+	.map = zoned_map,
+
+	.postsuspend = zoned_postsuspend,
+	.preresume = zoned_preresume,
+	.status = zoned_status,
+		/*  .message = zoned_message, */
+	.ioctl = zoned_ioctl,
+
+	.iterate_devices = zoned_iterate_devices,
+	.io_hints = zoned_io_hints
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int __init dm_zoned_init(void)
+{
+	int rcode = dm_register_target(&zoned_target);
+
+	if (rcode)
+		DMERR("zoned target registration failed: %d", rcode);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+static void __exit dm_zoned_exit(void)
+{
+	dm_unregister_target(&zoned_target);
+}
+
+module_init(dm_zoned_init);
+module_exit(dm_zoned_exit);
+
+MODULE_DESCRIPTION(DM_NAME " zoned target for Host Aware/Managed drives.");
+MODULE_AUTHOR("Shaun Tancheff <shaun.tancheff@seagate.com>");
+MODULE_LICENSE("GPL");
diff --git a/drivers/md/dm-zoned.h b/drivers/md/dm-zoned.h
new file mode 100644
index 0000000..0af8762
--- /dev/null
+++ b/drivers/md/dm-zoned.h
@@ -0,0 +1,665 @@
+/*
+ * Kernel Device Mapper for abstracting ZAC/ZBC devices as normal
+ * block devices for linux file systems.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#ifndef _DM_ZONED_H
+#define _DM_ZONED_H
+
+#define USE_KTHREAD		0
+
+#define CRIT			GFP_ATOMIC
+#define NORMAL			0
+
+#define ZDM_IOC_MZCOUNT		0x5a4e0001
+#define ZDM_IOC_WPS		0x5a4e0002
+#define ZDM_IOC_FREE		0x5a4e0003
+#define ZDM_IOC_STATUS		0x5a4e0004
+
+#define DM_MSG_PREFIX		"zoned"
+
+#define PROC_WP			"wp.bin"
+#define PROC_FREE		"free.bin"
+#define PROC_DATA		"data.bin"
+#define PROC_STATUS		"status"
+
+#define ZDM_RESERVED_ZNR	0
+#define ZDM_CRC_STASH_ZNR	1 /* first 64 blocks */
+#define ZDM_RMAP_ZONE		2
+#define ZDM_SECTOR_MAP_ZNR	3
+#define ZDM_DATA_START_ZNR	4
+
+#define Z_WP_GC_FULL		(1u << 31)
+#define Z_WP_GC_ACTIVE		(1u << 30)
+#define Z_WP_GC_TARGET		(1u << 29)
+#define Z_WP_GC_READY		(1u << 28)
+#define Z_WP_GC_BITS		(0xFu << 28)
+
+#define Z_WP_RRECALC		(1u << 31)
+
+#define Z_WP_GC_PENDING		(Z_WP_GC_FULL|Z_WP_GC_ACTIVE)
+#define Z_WP_NON_SEQ		(1u << 27)
+#define Z_WP_RESV_01		(1u << 26)
+#define Z_WP_RESV_02		(1u << 25)
+#define Z_WP_RESV_03		(1u << 24)
+
+#define Z_WP_VALUE_MASK		(~0u >> 8)
+#define Z_WP_FLAGS_MASK		(~0u << 24)
+#define Z_WP_STREAM_MASK        Z_WP_FLAGS_MASK
+
+#define Z_AQ_GC			(1u << 31)
+#define Z_AQ_META		(1u << 30)
+#define Z_AQ_NORMAL		(1u << 29)
+#define Z_AQ_STREAM_ID		(1u << 28)
+#define Z_AQ_STREAM_MASK	(0xFF)
+#define Z_AQ_META_STREAM	(Z_AQ_META | Z_AQ_STREAM_ID | 0xFE)
+
+#define Z_C4K			(4096ul)
+#define Z_UNSORTED		(Z_C4K / sizeof(struct map_sect_to_lba))
+#define Z_BLOCKS_PER_DM_SECTOR	(Z_C4K/512)
+#define MZ_METADATA_ZONES	(8ul)
+#define Z_SHFT4K		(3)
+
+
+#define LBA_SB_START		1
+
+#define SUPERBLOCK_LOCATION	0
+#define SUPERBLOCK_MAGIC	0x5a6f4e65ul	/* ZoNe */
+#define SUPERBLOCK_CSUM_XOR	146538381
+#define MIN_ZONED_VERSION	1
+#define Z_VERSION		1
+#define MAX_ZONED_VERSION	1
+#define INVALID_WRITESET_ROOT	SUPERBLOCK_LOCATION
+
+#define UUID_LEN		16
+
+#define Z_TYPE_SMR		2
+#define Z_TYPE_SMR_HA		1
+#define Z_VPD_INFO_BYTE		8
+
+#define MAX_CACHE_INCR		320ul
+#define CACHE_COPIES		3
+#define MAX_MZ_SUPP		64
+#define FWD_TM_KEY_BASE		4096ul
+
+#define Z_HASH_SZ		1
+
+#define IO_VCACHE_ORDER		8
+#define IO_VCACHE_PAGES		(1 << IO_VCACHE_ORDER)  /* 256 pages => 1MiB */
+
+enum superblock_flags_t {
+	SB_DIRTY = 1,
+};
+
+struct z_io_req_t {
+	struct dm_io_region *where;
+	struct dm_io_request *io_req;
+	struct work_struct work;
+	int result;
+};
+
+#define Z_LOWER48 (~0ul >> 16)
+#define Z_UPPER16 (~Z_LOWER48)
+
+#define STREAM_SIZE	256
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * enum pg_flag_enum - Map Pg flags
+ */
+enum pg_flag_enum {
+	IS_DIRTY,
+	IS_GC,
+	IS_STALE,
+
+	IS_FWD,
+	IS_REV,
+	IS_CRC,
+	IS_LUT,
+
+	IS_MEMPOOL,
+};
+
+/**
+ * enum gc_flags_enum - GC/Compaction states
+ */
+enum gc_flags_enum {
+	DO_GC_NEW,
+	DO_GC_PREPARE,		/* -> READ or COMPLETE state */
+	DO_GC_WRITE,
+	DO_GC_META,		/* -> PREPARE state */
+	DO_GC_COMPLETE,
+};
+
+/**
+ * enum znd_flags_enum - zoned state/feature/action flags
+ */
+enum znd_flags_enum {
+	ZF_FREEZE,
+	ZF_POOL_FWD,
+	ZF_POOL_REV,
+	ZF_POOL_CRCS,
+
+	ZF_RESV_1,
+	ZF_RESV_2,
+	ZF_RESV_3,
+	ZF_RESV_4,
+
+	DO_JOURNAL_MOVE,
+	DO_MEMPOOL,
+	DO_SYNC,
+	DO_JOURNAL_LOAD,
+
+	DO_GC_NO_PURGE,
+	DO_METAWORK_QD,
+};
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct zoned;
+
+/**
+ * struct gc_state - A page of map table
+ * @znd: ZDM Instance
+ * @gc_flags: See gc_flags_enum
+ * @r_ptr: Next read in zone.
+ * @w_ptr: Next write in target zone.
+ * @nblks: Number of blocks in I/O
+ * @result: GC operation result.
+ * @z_gc: Zone undergoing compacation
+ * @tag: System wide serial number (debugging).
+ *
+ * Longer description of this structure.
+ */
+struct gc_state {
+	struct zoned *znd;
+	unsigned long gc_flags;
+
+	u32 r_ptr;
+	u32 w_ptr;
+
+	u32 nblks;		/* 1-65536 */
+	u32 z_gc;
+
+	int result;
+	u16 tag;
+};
+
+/**
+ * struct map_addr - A page of map table
+ * @dm_s:    full map on dm layer
+ * @zone_id: z_id match zone_list_t.z_id
+ * @pg_idx:  entry in lut (0-1023)
+ * @lut_s:   sector table lba
+ * @lut_r:   reverse table lba
+ *
+ * Longer description of this structure.
+ */
+struct map_addr {
+	u64 dm_s;
+	u64 lut_s;
+	u64 lut_r;
+
+	u32 zone_id;
+	u32 pg_idx;
+};
+
+/**
+ * struct map_sect_to_lba - Sector to LBA mapping.
+ * @tlba: tlba
+ * @physical: blba or number of blocks
+ *
+ * Longer description of this structure.
+ */
+struct map_sect_to_lba {
+	__le64 logical;		/* record type [16 bits] + logical sector # */
+	__le64 physical;	/* csum 16 [16 bits] + 'physical' block lba */
+} __packed;
+
+/**
+ * enum gc_flags_enum - Garbage Collection [GC] states.
+ */
+enum map_type_enum {
+	IS_MAP,
+	IS_DISCARD,
+	MAP_COUNT,
+};
+
+/**
+ * struct map_cache - A page of map table
+ * @mclist:
+ * @jdata: 4k page of data
+ * @refcount:
+ * @cached_lock:
+ * @busy_locked:
+ * @jcount:
+ * @jsorted:
+ * @jsize:
+ *
+ * Longer description of this structure.
+ */
+struct map_cache {
+	struct list_head mclist;
+	struct map_sect_to_lba *jdata;
+	atomic_t refcount;
+	struct mutex cached_lock;
+	atomic_t busy_locked;
+	u32 jcount;
+	u32 jsorted;
+	u32 jsize;
+};
+
+
+union map_pg_data {
+	__le32 *addr;
+	__le16 *crc;
+};
+
+
+/**
+ * struct map_pg - A page of map table
+ *
+ * @mdata:        4k page of table entries
+ * @refcount:
+ * @age:          most recent access in jiffies
+ * @lba:          logical position (use lookups to find actual)
+ * @md_lock:      lock mdata i/o
+ * @last_write:   last known position on disk
+ * @flags:        is dirty flag, is fwd, is rev, is crc, is lut
+ * @inpool:
+ *
+ * Longer description of this structure.
+ */
+struct map_pg {
+	union map_pg_data data;
+	atomic_t refcount;
+	u64 age;
+	u64 lba;
+	u64 last_write;
+	struct mutex md_lock;
+	unsigned long flags;
+	struct list_head inpool;
+
+	/* in flight tracking */
+	struct zoned *znd;
+	struct map_pg *crc_pg;
+	__le16 md_crc;
+};
+
+/**
+ * struct map_crc - Map to backing crc16.
+ *
+ * @table:        Maybe via table
+ * @pg_no:        Page [index] of table entry if applicable.
+ * @pg_idx:       Offset within page (From: zoned::md_crcs when table is null)
+ *
+ * Longer description of this structure.
+ */
+struct map_crc {
+	struct map_pg **table;
+	int pg_no;
+	int pg_idx;
+};
+
+/**
+ * struct map_info - Map to backing lookup table.
+ *
+ * @table:        backing table
+ * @crc:          backing crc16 detail.
+ * @index:        index [page] of table entry. Use map_addr::pg_idx for offset.
+ * @bit_type:     IS_LUT or IS_CRC
+ * @bit_dir:      IS_FWD or IS_REV
+ *
+ * Longer description of this structure.
+ */
+struct mpinfo {
+	struct map_pg **table;
+	struct map_crc crc;
+	int index;
+	int bit_type;
+	int bit_dir;
+};
+
+
+/**
+ * struct meta_pg - A page of zone WP mapping.
+ *
+ * @wp_alloc: Bits 23-0: wp alloc location.   Bits 31-24: GC Flags, Type Flags
+ * @zf_est:   Bits 23-0: free block count.    Bits 31-24: Stream Id
+ * @wp_used:  Bits 23-0: wp written location. Bits 31-24: Ratio ReCalc flag.
+ * @lba: pinned LBA in conventional/preferred zone.
+ * @wplck: spinlock held during data updates.
+ * @flags: IS_DIRTY flag
+ *
+ * One page is used per 1024 zones on media.
+ * For an 8TB drive this uses 30 entries or about 360k RAM.
+ */
+struct meta_pg {
+	__le32 *wp_alloc;
+	__le32 *zf_est;
+	__le32 *wp_used;
+	u64 lba;
+	struct mutex wplck;
+	unsigned long flags;
+};
+
+/**
+ * struct zdm_superblock - A page of map table
+ * @uuid:
+ * @nr_zones:
+ * @magic:
+ * @zdstart:
+ * @version:
+ * @packed_meta:
+ * @flags:
+ * @csum:
+ *
+ * Longer description of this structure.
+ */
+struct zdm_superblock {
+	u8 uuid[UUID_LEN];	/* 16 */
+	__le64 nr_zones;	/*  8 */
+	__le64 magic;		/*  8 */
+	__le32 resvd;		/*  4 */
+	__le32 zdstart;		/*  4 */
+	__le32 version;		/*  4 */
+	__le32 packed_meta;	/*  4 */
+	__le32 flags;		/*  4 */
+	__le32 csum;		/*  4 */
+} __packed;			/* 56 */
+
+#define MAX_CACHE_SYNC 400
+
+/**
+ * struct mz_superkey - A page of map table
+ * @sig0:           8 - Native endian
+ * @sig1:           8 - Little endian
+ * @sblock:        56 -
+ * @stream:      1024 -
+ * @reserved:    2982 -
+ * @gc_resv:        4 -
+ * @meta_resv:      4 -
+ * @generation:     8 -
+ * @key_crc:        2 -
+ * @magic:          8 -
+ *
+ * Longer description of this structure.
+ */
+struct mz_superkey {
+	u64 sig0;
+	__le64 sig1;
+	struct zdm_superblock sblock;
+	__le32 stream[STREAM_SIZE];
+	__le32 gc_resv;
+	__le32 meta_resv;
+	__le16 n_crcs;
+	__le16 crcs[MAX_CACHE_SYNC];
+	__le16 md_crc;
+	__le16 wp_crc[64];
+	__le16 zf_crc[64];
+	u8 reserved[1912];
+	__le32 crc32;
+	__le64 generation;
+	__le64 magic;
+} __packed;
+
+/**
+ * struct io_4k_block - Sector to LBA mapping.
+ * @data:	A 4096 byte block
+ *
+ * Longer description of this structure.
+ */
+struct io_4k_block {
+	u8 data[Z_C4K];
+};
+
+/**
+ * struct io_dm_block - Sector to LBA mapping.
+ * @data:	A 512 byte block
+ *
+ * Longer description of this structure.
+ */
+struct io_dm_block {
+	u8 data[512];
+};
+
+struct stale_tracking {
+	u32 binsz;
+	u32 count;
+	int bins[STREAM_SIZE];
+};
+
+
+/*
+ *
+ * Partition -----------------------------------------------------------------+
+ *     Table ---+                                                             |
+ *              |                                                             |
+ *   SMR Drive |^-------------------------------------------------------------^|
+ * CMR Zones     ^^^^^^^^^
+ *  meta data    |||||||||
+ *
+ *   Remaining partition is filesystem data
+ *
+ */
+
+/**
+ * struct zoned - A page of map table
+ * @ti:
+ * @callbacks:
+ * @dev:
+ * @mclist:	list of pages of in-memory LBA mappings.
+ * @mclck:	in memory map-cache lock (spinlock)
+
+ * @smtpool:		pages of lookup table entries
+ * @map_pool_lock:	smtpool: memory pool lock
+ * @fwd_tm:
+ * @rev_tm:
+
+ * @bg_work:	background worker (periodic GC/Mem purge)
+ * @bg_wq:	background work queue
+ * @stats_lock:
+ * @gc_active:	Current GC state
+ * @gc_lock:	GC Lock
+ * @gc_work:	GC Worker
+ * @gc_wq:	GC Work Queue
+ * @data_zones:	# of data zones on device
+ * @gz_count:	# of 256G mega-zones
+ * @nr_blocks:	4k blocks on backing device
+ * @md_start:	LBA at start of metadata pool
+ * @data_lba:	LBA at start of data pool
+ * @zdstart:	ZONE # at start of data pool (first Possible WP ZONE)
+ * @start_sect:	where ZDM partition starts (RAW LBA)
+ * @flags:	See: enum znd_flags_enum
+ * @gc_backlog:
+ * @gc_io_buf:
+ * @io_vcache[32]:
+ * @io_vcache_flags:
+ * @z_sballoc:
+ * @super_block:
+ * @z_mega:
+ * @meta_wq:
+ * @gc_postmap:
+ * @io_client:
+ * @io_wq:
+ * @timer:
+ * @bins:	Memory usage accounting/reporting.
+ * @bdev_name:
+ * @memstat:
+ * @suspended:
+ * @gc_mz_pref:
+ * @mz_provision:	Number of zones per 1024 of over-provisioning.
+ * @zinqtype:
+ * @ata_passthrough:
+ * @is_empty:		For fast discards on initial format
+ *
+ * Longer description of this structure.
+ */
+struct zoned {
+	struct dm_target *ti;
+	struct dm_target_callbacks callbacks;
+	struct dm_dev *dev;
+
+	struct list_head mclist[Z_HASH_SZ];
+	spinlock_t       mclck[Z_HASH_SZ];
+	struct mutex     mcmutex[Z_HASH_SZ];
+
+	struct list_head smtpool;
+	struct mutex map_pool_lock;
+
+	struct work_struct bg_work;
+	struct workqueue_struct *bg_wq;
+	spinlock_t stats_lock;
+
+	struct mutex mapkey_lock; /* access LUT and CRC array of pointers */
+	struct mutex ct_lock; /* access LUT and CRC array of pointers */
+	struct mutex mz_io_mutex;
+	struct mutex vcio_lock;
+	struct mutex io_lock;
+
+#if USE_KTHREAD
+	struct bio *bio_queue[512];
+	u32 bio_in;
+	u32 bio_out;
+	struct task_struct *bio_kthread;
+	wait_queue_head_t bio_wait;
+	spinlock_t bio_qlck;
+#endif
+
+	struct gc_state *gc_active;
+	spinlock_t gc_lock;
+	struct delayed_work gc_work;
+	struct workqueue_struct *gc_wq;
+
+	u64 nr_blocks;
+	u64 start_sect;
+
+	u64 md_start;
+	u64 md_end;
+	u64 data_lba;
+
+	unsigned long flags;
+
+	u64 r_base;
+	u64 s_base;
+	u64 c_base;
+	u64 c_mid;
+	u64 c_end;
+
+	u64 sk_low;   /* unused ? */
+	u64 sk_high;  /* unused ? */
+
+	struct meta_pg *wp;
+
+	struct map_pg **fwd_tm;  /* nr_blocks / 1024 */
+	struct map_pg **rev_tm;  /* nr_blocks / 1024 */
+	struct map_pg **fwd_crc; /* (nr_blocks / 1024) / 2048 */
+	struct map_pg **rev_crc; /* (nr_blocks / 1024) / 2048 */
+	__le16 *md_crcs;   /* one of crc16's for fwd, 1 for rev */
+	u32 crc_count;
+	u32 map_count;
+
+	void *z_sballoc;
+	struct mz_superkey *bmkeys;
+	struct zdm_superblock *super_block;
+
+	struct work_struct meta_work;
+	sector_t last_w;
+	u8 *cow_block;
+	u64 cow_addr;
+	u32 data_zones;
+	u32 gz_count;
+	u32 zdstart;
+	u32 z_gc_free;
+	s32 incore_count;
+	u32 discard_count;
+	u32 z_current;
+	u32 z_meta_resv;
+	u32 z_gc_resv;
+	int mc_entries;
+	int meta_result;
+	struct stale_tracking stale;
+
+	int gc_backlog;
+	void *gc_io_buf;
+	struct mutex gc_vcio_lock;
+	struct io_4k_block *io_vcache[32];
+	unsigned long io_vcache_flags;
+	u64 age;
+	struct workqueue_struct *meta_wq;
+	struct map_cache gc_postmap;
+	struct dm_io_client *io_client;
+	struct workqueue_struct *io_wq;
+	struct timer_list timer;
+
+	u32 bins[40];
+	char bdev_name[BDEVNAME_SIZE];
+	char proc_name[BDEVNAME_SIZE+4];
+	struct proc_dir_entry *proc_fs;
+	loff_t wp_proc_at;
+	loff_t used_proc_at;
+
+	size_t memstat;
+	atomic_t suspended;
+	atomic_t gc_throttle;
+
+	u16 mz_provision;
+	u8 zinqtype;
+	u8 ata_passthrough;
+	u8 is_empty;
+};
+
+/**
+ * struct zdm_ioc_request - Sector to LBA mapping.
+ * @result_size:
+ * @megazone_nr:
+ *
+ * Longer description of this structure.
+ */
+struct zdm_ioc_request {
+	u32 result_size;
+	u32 megazone_nr;
+};
+
+/**
+ * struct zdm_ioc_status - Sector to LBA mapping.
+ * @b_used: Number of blocks used
+ * @b_available: Number of blocks free
+ * @b_discard: Number of blocks stale
+ * @m_zones: Number of zones.
+ * @mc_entries: Mem cache blocks in use
+ * @dc_entries: Discard cache blocks in use.
+ * @mlut_blocks:
+ * @crc_blocks:
+ * @memstat: Total memory in use by ZDM via *alloc()
+ * @bins: Allocation by subsystem.
+ *
+ * This status structure is used to pass run-time information to
+ * user spaces tools (zdm-tools) for diagnostics and tuning.
+ */
+struct zdm_ioc_status {
+	u64 b_used;
+	u64 b_available;
+	u64 b_discard;
+	u64 m_zones;
+	u32 mc_entries;
+	u32 dc_entries;
+	u64 mlut_blocks;
+	u64 crc_blocks;
+	u64 memstat;
+	u32 bins[40];
+};
+
+#endif /* _DM_ZONED_H */
diff --git a/drivers/md/libzoned.c b/drivers/md/libzoned.c
new file mode 100644
index 0000000..a69f130
--- /dev/null
+++ b/drivers/md/libzoned.c
@@ -0,0 +1,5372 @@
+/*
+ * Kernel Device Mapper for abstracting ZAC/ZBC devices as normal
+ * block devices for linux file systems.
+ *
+ * Copyright (C) 2015 Seagate Technology PLC
+ *
+ * Written by:
+ * Shaun Tancheff <shaun.tancheff@seagate.com>
+ *
+ * This file is licensed under  the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#define BUILD_NO		101
+
+#define EXTRA_DEBUG		0
+
+#define MZ_MEMPOOL_SZ		256
+#define JOURNAL_MEMCACHE_BLOCKS	4
+#define MEM_PURGE_MSECS		10000
+
+/* acceptable 'free' count for MZ free levels */
+#define GC_PRIO_DEFAULT		0xFF00
+#define GC_PRIO_LOW		0x7FFF
+#define GC_PRIO_HIGH		0x0400
+#define GC_PRIO_CRIT		0x0040
+
+/* When less than 20 zones are free use aggressive gc in the megazone */
+#define GC_COMPACT_AGGRESSIVE	32
+
+/*
+ *  For performance tuning:
+ *   Q? smaller strips give smoother performance
+ *      a single drive I/O is 8 (or 32?) blocks?
+ *   A? Does not seem to ...
+ */
+#define GC_MAX_STRIPE		256
+#define REPORT_BUFFER		65 /* 65 -> min # pages for 4096 descriptors */
+#define SYNC_IO_ORDER		2
+#define SYNC_IO_SZ		((1 << SYNC_IO_ORDER) * PAGE_SIZE)
+
+#define MZTEV_UNUSED		(cpu_to_le32(0xFFFFFFFFu))
+#define MZTEV_NF		(cpu_to_le32(0xFFFFFFFEu))
+
+#define Z_TABLE_MAGIC		0x123456787654321Eul
+#define Z_KEY_SIG		0xFEDCBA987654321Ful
+
+#define Z_CRC_4K		4096
+#define ZONE_SECT_BITS		19
+#define Z_BLKBITS		16
+#define Z_BLKSZ			(1ul << Z_BLKBITS)
+#define MAX_ZONES_PER_MZ	1024
+#define Z_SMR_SZ_BYTES		(Z_C4K << Z_BLKBITS)
+
+#define GC_READ			(1ul << 15)
+#define BAD_ADDR		(~0ul)
+#define MC_INVALID		(cpu_to_le64(BAD_ADDR))
+
+#define GZ_BITS 10
+#define GZ_MMSK ((1u << GZ_BITS) - 1)
+
+#define CRC_BITS 11
+#define CRC_MMSK ((1u << CRC_BITS) - 1)
+
+#define MD_CRC_INIT		(cpu_to_le16(0x5249u))
+
+static int map_addr_calc(struct zoned *, u64 dm_s, struct map_addr *out);
+static int zoned_io_flush(struct zoned *znd);
+static int zoned_wp_sync(struct zoned *znd, int reset_non_empty);
+
+static void cache_if_dirty(struct zoned *znd, struct map_pg *pg, int wq);
+static int write_if_dirty(struct zoned *, struct map_pg *, int use_wq, int snc);
+
+static void gc_work_task(struct work_struct *work);
+static void meta_work_task(struct work_struct *work);
+static u64 mcache_greatest_gen(struct zoned *, int, u64 *, u64 *);
+static u64 mcache_find_gen(struct zoned *, u64 base, int, u64 *out);
+static int find_superblock(struct zoned *znd, int use_wq, int do_init);
+static int do_sync_metadata(struct zoned *znd);
+static int sync_mapped_pages(struct zoned *znd);
+static int sync_crc_pages(struct zoned *znd);
+static int unused_phy(struct zoned *znd, u64 lba, u64 orig_s, int gfp);
+static struct io_4k_block *get_io_vcache(struct zoned *znd, int gfp);
+static int put_io_vcache(struct zoned *znd, struct io_4k_block *cache);
+static struct map_pg *get_map_entry(struct zoned *, u64 lba, int gfp);
+static void put_map_entry(struct map_pg *);
+static int cache_pg(struct zoned *znd, struct map_pg *pg, int, struct mpinfo *);
+static int move_to_map_tables(struct zoned *znd, struct map_cache *jrnl);
+static int keep_active_pages(struct zoned *znd, int allowed_pages);
+static int zoned_create_disk(struct dm_target *ti, struct zoned *znd);
+static int do_init_zoned(struct dm_target *ti, struct zoned *znd);
+static sector_t jentry_value(struct map_sect_to_lba *e, bool is_block);
+static u64 z_lookup_cache(struct zoned *znd, u64 addr);
+static u64 z_lookup_table(struct zoned *znd, u64 addr, int gfp);
+static u64 current_mapping(struct zoned *znd, u64 addr, int gfp);
+static int z_mapped_add_one(struct zoned *znd, u64 dm_s, u64 lba, int gfp);
+static int z_mapped_discard(struct zoned *znd, u64 dm_s, u64 lba);
+static int z_mapped_addmany(struct zoned *znd, u64 dm_s, u64 lba, u64, int gfp);
+static int z_mapped_to_list(struct zoned *znd, u64 dm_s, u64 lba, int gfp);
+static int z_mapped_sync(struct zoned *znd);
+static int z_mapped_init(struct zoned *znd);
+static u64 z_acquire(struct zoned *znd, u32 flags, u32 nblks, u32 *nfound);
+static __le32 sb_crc32(struct zdm_superblock *sblock);
+static int update_map_entry(struct zoned *, struct map_pg *,
+			    struct map_addr *, u64, int);
+static int read_block(struct dm_target *, enum dm_io_mem_type,
+		      void *, u64, unsigned int, int);
+static int write_block(struct dm_target *, enum dm_io_mem_type,
+		       void *, u64, unsigned int, int);
+static int zoned_init_disk(struct dm_target *ti, struct zoned *znd,
+			   int create, int force);
+
+#define MutexLock(m)  test_and_lock((m), __LINE__)
+#define SpinLock(s)   test_and_spin((s), __LINE__)
+
+static inline void test_and_lock(struct mutex *m, int lineno)
+{
+	if (!mutex_trylock(m)) {
+		pr_debug("mutex stall at %d\n", lineno);
+		mutex_lock(m);
+	}
+}
+
+static __always_inline void test_and_spin(spinlock_t *lock, int lineno)
+{
+	if (!spin_trylock(lock)) {
+		pr_debug("spin stall at %d\n", lineno);
+		spin_lock(lock);
+	}
+}
+
+
+/**
+ * crc16_md() - 16 bit CRC on metadata blocks
+ * @data: Block of metadata.
+ * @len:  Number of bytes in block.
+ *
+ * Return: 16 bit CRC.
+ */
+static inline u16 crc16_md(void const *data, size_t len)
+{
+	const u16 init = 0xFFFF;
+	const u8 *p = data;
+
+	return crc16(init, p, len);
+}
+
+/**
+ * crc_md_le16() - 16 bit CRC on metadata blocks in little endian
+ * @data: Block of metadata.
+ * @len:  Number of bytes in block.
+ *
+ * Return: 16 bit CRC.
+ */
+static inline __le16 crc_md_le16(void const *data, size_t len)
+{
+	u16 crc = crc16_md(data, len);
+
+	return cpu_to_le16(crc);
+}
+
+/**
+ * crcpg() - 32 bit CRC [NOTE: 32c is HW assisted on Intel]
+ * @data: Block of metadata [4K bytes].
+ *
+ * Return: 32 bit CRC.
+ */
+static inline u32 crcpg(void *data)
+{
+	return crc32c(~0u, data, Z_CRC_4K) ^ SUPERBLOCK_CSUM_XOR;
+}
+
+/**
+ * le64_to_lba48() - Return the lower 48 bits of LBA
+ * @enc: 64 bit LBA + flags
+ * @flg: optional 16 bits of classification.
+ *
+ * Return: 48 bits of LBA [and flg].
+ */
+static inline u64 le64_to_lba48(__le64 enc, u16 *flg)
+{
+	const u64 lba64 = le64_to_cpu(enc);
+
+	if (flg)
+		*flg = (lba64 >> 48) & 0xFFFF;
+
+	return lba64 & Z_LOWER48;
+}
+
+/**
+ * lba48_to_le64() - Encode 48 bits of lba + 16 bits of flags.
+ * @flags: flags to encode.
+ * @lba48: LBA to encode
+ *
+ * Return: Little endian u64.
+ */
+static inline __le64 lba48_to_le64(u16 flags, u64 lba48)
+{
+	u64 high_bits = flags;
+
+	return cpu_to_le64((high_bits << 48) | (lba48 & Z_LOWER48));
+}
+
+/**
+ * sb_test_flag() - Test if flag is set in Superblock.
+ * @sb: zdm_superblock.
+ * @bit_no: superblock flag
+ *
+ * Return: non-zero if flag is set.
+ */
+static inline int sb_test_flag(struct zdm_superblock *sb, int bit_no)
+{
+	u32 flags = le32_to_cpu(sb->flags);
+
+	return (flags & (1 << bit_no)) ? 1 : 0;
+}
+
+/**
+ * sb_set_flag() - Set a flag in superblock.
+ * @sb: zdm_superblock.
+ * @bit_no: superblock flag
+ */
+static inline void sb_set_flag(struct zdm_superblock *sb, int bit_no)
+{
+	u32 flags = le32_to_cpu(sb->flags);
+
+	flags |= (1 << bit_no);
+	sb->flags = cpu_to_le32(flags);
+}
+
+/**
+ * zone_to_sector() - Calculate starting LBA of zone
+ * @zone: zone number (0 based)
+ *
+ * Return: LBA at start of zone.
+ */
+static inline u64 zone_to_sector(u64 zone)
+{
+	return zone << ZONE_SECT_BITS;
+}
+
+/**
+ * is_expired_msecs() - Determine if age + msecs is older than now.
+ * @age: jiffies at last access
+ * @msecs: msecs of extra time.
+ *
+ * Return: non-zero if block is expired.
+ */
+static inline int is_expired_msecs(u64 age, u32 msecs)
+{
+	u64 expire_at = age + msecs_to_jiffies(msecs);
+	int expired = time_after64(jiffies_64, expire_at);
+
+	return expired;
+}
+
+/**
+ * is_expired() - Determine if age is older than MEM_PURGE_MSECS.
+ * @age: jiffies at last access
+ *
+ * Return: non-zero if block is expired.
+ */
+static inline int is_expired(u64 age)
+{
+	return is_expired_msecs(age, MEM_PURGE_MSECS);
+}
+
+/**
+ * _calc_zone() - Determine zone number from addr
+ * @addr: 4k sector number
+ *
+ * Return: znum or 0xFFFFFFFF if addr is in metadata space.
+ */
+static inline u32 _calc_zone(struct zoned *znd, u64 addr)
+{
+	u32 znum = ~0u;
+
+	if (addr < znd->data_lba)
+		return znum;
+
+	addr -= znd->data_lba;
+	znum = addr >> Z_BLKBITS;
+
+	return znum;
+}
+
+/**
+ * _pool_last() - Retrieve oldest metadata block from inpool
+ * @znd: ZDM instance
+ *
+ * Return: oldest page of metadata or NULL.
+ */
+static inline struct map_pg *_pool_last(struct zoned *znd)
+{
+	struct map_pg *expg = NULL;
+
+	if (!list_empty(&znd->smtpool)) {
+		expg = list_last_entry(&znd->smtpool, typeof(*expg), inpool);
+		if (&expg->inpool == &znd->smtpool)
+			expg = NULL;
+	}
+	if (expg)
+		atomic_inc(&expg->refcount);
+
+	return expg;
+}
+
+/**
+ * _pool_prev() - Retrieve previous metadata block from inpool
+ * @znd: ZDM instance
+ * @expg: current metadata block in inpool list.
+ *
+ * Return: next oldest [previous] page of metadata or NULL.
+ */
+static struct map_pg *_pool_prev(struct zoned *znd, struct map_pg *expg)
+{
+	struct map_pg *aux = NULL;
+
+	if (expg)
+		aux = list_prev_entry(expg, inpool);
+	if (aux)
+		if (&aux->inpool == &znd->smtpool)
+			aux = NULL;
+	if (aux)
+		atomic_inc(&aux->refcount);
+
+	return aux;
+}
+
+/**
+ * pool_add() - Add metadata block to inpool
+ * @znd: ZDM instance
+ * @expg: current metadata block to add to inpool list.
+ */
+static inline void pool_add(struct zoned *znd, struct map_pg *expg)
+{
+	if (expg) {
+		MutexLock(&znd->map_pool_lock);
+		list_add(&expg->inpool, &znd->smtpool);
+		mutex_unlock(&znd->map_pool_lock);
+	}
+}
+
+/**
+ * incore_hint() - Make expg first entry in pool list
+ * @znd: ZDM instance
+ * @expg: active map_pg
+ */
+static inline void incore_hint(struct zoned *znd, struct map_pg *expg)
+{
+	MutexLock(&znd->map_pool_lock);
+	if (znd->smtpool.next != &expg->inpool)
+		list_move(&expg->inpool, &znd->smtpool);
+	mutex_unlock(&znd->map_pool_lock);
+}
+
+/**
+ * to_table_entry() - Deconstrct metadata page into mpinfo
+ * @znd: ZDM instance
+ * @lba: Address (4k resolution)
+ * @expg: current metadata block in inpool list.
+ *
+ * Return: Index into mpinfo.table.
+ */
+static int to_table_entry(struct zoned *znd, u64 lba, struct mpinfo *mpi)
+{
+	int index = -1;
+
+	if (lba >= znd->s_base && lba < znd->r_base) {
+		mpi->table	= znd->fwd_tm;
+		index		= lba - znd->s_base;
+		mpi->bit_type	= IS_LUT;
+		mpi->bit_dir	= IS_FWD;
+		mpi->crc.table	= znd->fwd_crc;
+		mpi->crc.pg_no	= index >> CRC_BITS;
+		mpi->crc.pg_idx	= index & CRC_MMSK;
+		if (index < 0 ||  index >= znd->map_count) {
+			Z_ERR(znd, "%s: FWD BAD IDX %"PRIx64" %d of %d",
+				__func__, lba, index, znd->map_count);
+		}
+	} else if (lba >= znd->r_base && lba < znd->c_base) {
+		mpi->table	= znd->rev_tm;
+		index		= lba - znd->r_base;
+		mpi->bit_type	= IS_LUT;
+		mpi->bit_dir	= IS_REV;
+		mpi->crc.table	= znd->rev_crc;
+		mpi->crc.pg_no	= index >> CRC_BITS;
+		mpi->crc.pg_idx	= index & CRC_MMSK;
+		if (index < 0 ||  index >= znd->map_count) {
+			Z_ERR(znd, "%s: REV BAD IDX %"PRIx64" %d of %d",
+				__func__, lba, index, znd->map_count);
+		}
+	} else if (lba >= znd->c_base && lba < znd->c_mid) {
+		mpi->table	= znd->fwd_crc;
+		index		= lba - znd->c_base;
+		mpi->bit_type	= IS_CRC;
+		mpi->bit_dir	= IS_FWD;
+		mpi->crc.table	= NULL;
+		mpi->crc.pg_no	= 0;
+		mpi->crc.pg_idx	= index & CRC_MMSK;
+		if (index < 0 ||  index >= znd->crc_count) {
+			Z_ERR(znd, "%s: CRC BAD IDX %"PRIx64" %d of %d",
+				__func__, lba, index, znd->crc_count);
+		}
+	} else if (lba >= znd->c_mid && lba < znd->c_end) {
+		mpi->table	= znd->rev_crc;
+		index		= lba - znd->c_mid;
+		mpi->bit_type	= IS_CRC;
+		mpi->bit_dir	= IS_REV;
+		mpi->crc.table	= NULL;
+		mpi->crc.pg_no	= 1;
+		mpi->crc.pg_idx	= (1 << CRC_BITS) + (index & CRC_MMSK);
+		if (index < 0 ||  index >= znd->crc_count) {
+			Z_ERR(znd, "%s: CRC BAD IDX %"PRIx64" %d of %d",
+				__func__, lba, index, znd->crc_count);
+		}
+	} else {
+		Z_ERR(znd, "** Corrupt lba %" PRIx64 " not in range.", lba);
+		znd->meta_result = -EIO;
+		dump_stack();
+	}
+	mpi->index = index;
+	return index;
+}
+
+/**
+ * is_ready_for_gc() - Test zone flags for GC sanity and ready flag.
+ * @znd: ZDM instance
+ * @z_id: Address (4k resolution)
+ *
+ * Return: non-zero if zone is suitable for GC.
+ */
+static inline int is_ready_for_gc(struct zoned *znd, u32 z_id)
+{
+	u32 gzno  = z_id >> GZ_BITS;
+	u32 gzoff = z_id & GZ_MMSK;
+	struct meta_pg *wpg = &znd->wp[gzno];
+	u32 wp = le32_to_cpu(wpg->wp_alloc[gzoff]);
+
+	if (((wp & Z_WP_GC_BITS) == Z_WP_GC_READY) &&
+	    ((wp & Z_WP_VALUE_MASK) == Z_BLKSZ))
+		return 1;
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ *  generic-ish n-way alloc/free
+ *  Use kmalloc for small (< 4k) allocations.
+ *  Use vmalloc for multi-page alloctions
+ *  Except:
+ *  Use multipage allocations for dm_io'd pages that a frequently hit.
+ *
+ *  NOTE: ALL allocations are zero'd before returning.
+ *        alloc/free count is tracked for dynamic analysis.
+ */
+
+#define GET_ZPG       0x040000
+#define GET_KM        0x080000
+#define GET_VM        0x100000
+
+#define PG_01    (GET_ZPG |  1)
+#define PG_02    (GET_ZPG |  2)
+#define PG_05    (GET_ZPG |  5)
+#define PG_06    (GET_ZPG |  6)
+#define PG_08    (GET_ZPG |  8)
+#define PG_09    (GET_ZPG |  9)
+#define PG_10    (GET_ZPG | 10)
+#define PG_11    (GET_ZPG | 11)
+#define PG_13    (GET_ZPG | 13)
+#define PG_17    (GET_ZPG | 17)
+#define PG_27    (GET_ZPG | 27)
+
+#define KM_00    (GET_KM  |  0)
+#define KM_07    (GET_KM  |  7)
+#define KM_14    (GET_KM  | 14)
+#define KM_15    (GET_KM  | 15)
+#define KM_16    (GET_KM  | 16)
+#define KM_18    (GET_KM  | 18)
+#define KM_19    (GET_KM  | 19)
+#define KM_20    (GET_KM  | 20)
+#define KM_25    (GET_KM  | 25)
+#define KM_26    (GET_KM  | 26)
+#define KM_28    (GET_KM  | 28)
+#define KM_29    (GET_KM  | 29)
+#define KM_30    (GET_KM  | 30)
+
+#define VM_03    (GET_VM  |  3)
+#define VM_04    (GET_VM  |  4)
+#define VM_12    (GET_VM  | 12)
+#define VM_21    (GET_VM  | 21)
+#define VM_22    (GET_VM  | 22)
+
+#define ZDM_FREE(z, _p, sz, id) \
+	do { zdm_free((z), (_p), (sz), (id)); (_p) = NULL; } while (0)
+
+#define ZDM_ALLOC(z, sz, id, gfp)       zdm_alloc((z), (sz), (id), (gfp))
+#define ZDM_CALLOC(z, n, sz, id, gfp)   zdm_calloc((z), (n), (sz), (id), (gfp))
+
+/**
+ * zdm_free() - Unified free by allocation 'code'
+ * @znd: ZDM instance
+ * @p: memory to be released.
+ * @sz: allocated size.
+ * @code: allocation size
+ *
+ * This (ugly) unified scheme helps to find leaks and monitor usage
+ *   via ioctl tools.
+ */
+static void zdm_free(struct zoned *znd, void *p, size_t sz, u32 code)
+{
+	int id    = code & 0x00FFFF;
+	int flag  = code & 0xFF0000;
+
+	if (p) {
+		if (znd) {
+			SpinLock(&znd->stats_lock);
+			if (sz > znd->memstat)
+				Z_ERR(znd,
+				      "Free'd more mem than allocated? %d", id);
+
+			if (sz > znd->bins[id]) {
+				Z_ERR(znd,
+				      "Free'd more mem than allocated? %d", id);
+				dump_stack();
+			}
+			znd->memstat -= sz;
+			znd->bins[id] -= sz;
+			spin_unlock(&znd->stats_lock);
+		}
+
+		switch (flag) {
+		case GET_ZPG:
+			free_page((unsigned long)p);
+			break;
+		case GET_KM:
+			kfree(p);
+			break;
+		case GET_VM:
+			vfree(p);
+			break;
+		default:
+			Z_ERR(znd,
+			      "zdm_free %p scheme %x not mapped.", p, code);
+			break;
+		}
+
+	} else {
+		Z_ERR(znd, "double zdm_free %p [%d]", p, id);
+		dump_stack();
+	}
+}
+
+/**
+ * zdm_alloc() - Unified alloc by 'code':
+ * @znd: ZDM instance
+ * @sz: allocated size.
+ * @code: allocation size
+ * @gfp: kernel allocation flags.
+ *
+ * There a few things (like dm_io) that seem to need pages and not just
+ *   kmalloc'd memory.
+ *
+ * This (ugly) unified scheme helps to find leaks and monitor usage
+ *   via ioctl tools.
+ */
+static void *zdm_alloc(struct zoned *znd, size_t sz, int code, int gfp)
+{
+	void *pmem = NULL;
+	int id    = code & 0x00FFFF;
+	int flag  = code & 0xFF0000;
+	int gfp_flags;
+
+#if USE_KTHREAD
+	gfp_flags = GFP_KERNEL;
+#else
+	gfp_flags = gfp ? GFP_ATOMIC : GFP_KERNEL;
+#endif
+
+	might_sleep();
+
+	switch (flag) {
+	case GET_ZPG:
+		pmem = (void *)get_zeroed_page(gfp_flags);
+		if (!pmem && gfp_flags == GFP_ATOMIC) {
+			Z_ERR(znd, "No atomic for %d, try noio.", id);
+			pmem = (void *)get_zeroed_page(GFP_NOIO);
+		}
+		break;
+	case GET_KM:
+		pmem = kzalloc(sz, gfp_flags);
+		if (!pmem && gfp_flags == GFP_ATOMIC) {
+			Z_ERR(znd, "No atomic for %d, try noio.", id);
+			pmem = kzalloc(sz, GFP_NOIO);
+		}
+		break;
+	case GET_VM:
+		WARN_ON(gfp);
+		pmem = vzalloc(sz);
+		break;
+	default:
+		Z_ERR(znd, "zdm alloc scheme for %u unknown.", code);
+		break;
+	}
+	if (!pmem) {
+		Z_ERR(znd, "Out of memory. %d", id);
+		dump_stack();
+	}
+	if (znd && pmem) {
+		SpinLock(&znd->stats_lock);
+		znd->memstat += sz;
+		znd->bins[id] += sz;
+		spin_unlock(&znd->stats_lock);
+	}
+	return pmem;
+}
+
+/**
+ * zdm_calloc() - Unified alloc by 'code':
+ * @znd: ZDM instance
+ * @n: number of elements in array.
+ * @sz: allocation size of each element.
+ * @code: allocation strategy (VM, KM, PAGE, N-PAGES).
+ * @gfp: kernel allocation flags.
+ *
+ * calloc is just an zeroed memory array alloc.
+ * all zdm_alloc schemes are for zeroed memory so no extra memset needed.
+ */
+static void *zdm_calloc(struct zoned *znd, size_t n, size_t sz, int code, int q)
+{
+	return zdm_alloc(znd, sz * n, code, q);
+}
+
+/**
+ * get_io_vcache() - Get a pre-allocated pool of memory for IO.
+ * @znd: ZDM instance
+ * @gfp: Allocation flags if no pre-allocated pool can be found.
+ *
+ * Return: Pointer to pool memory or NULL.
+ */
+static struct io_4k_block *get_io_vcache(struct zoned *znd, int gfp)
+{
+	struct io_4k_block *cache = NULL;
+	int avail;
+
+	might_sleep();
+
+	for (avail = 0; avail < ARRAY_SIZE(znd->io_vcache); avail++) {
+		if (!test_and_set_bit(avail, &znd->io_vcache_flags)) {
+			cache = znd->io_vcache[avail];
+			if (!cache)
+				znd->io_vcache[avail] = cache =
+					ZDM_CALLOC(znd, IO_VCACHE_PAGES,
+						sizeof(*cache), VM_12, gfp);
+			if (cache)
+				break;
+		}
+	}
+	return cache;
+}
+
+/**
+ * put_io_vcache() - Get a pre-allocated pool of memory for IO.
+ * @znd: ZDM instance
+ * @gfp: Allocation flags if no pre-allocated pool can be found.
+ *
+ * Return: Pointer to pool memory or NULL.
+ */
+static int put_io_vcache(struct zoned *znd, struct io_4k_block *cache)
+{
+	int err = -ENOENT;
+	int avail;
+
+	if (cache) {
+		for (avail = 0; avail < ARRAY_SIZE(znd->io_vcache); avail++) {
+			if (cache == znd->io_vcache[avail]) {
+				WARN_ON(!test_and_clear_bit(avail,
+					&znd->io_vcache_flags));
+				err = 0;
+				break;
+			}
+		}
+	}
+	return err;
+}
+
+/**
+ * map_value() - translate a lookup table entry to a Sector #, or LBA.
+ * @znd: ZDM instance
+ * @delta: little endian map entry.
+ *
+ * Return: LBA or 0 if invalid.
+ */
+static inline u64 map_value(struct zoned *znd, __le32 delta)
+{
+	u64 mval = 0ul;
+
+	if ((delta != MZTEV_UNUSED) && (delta != MZTEV_NF))
+		mval = le32_to_cpu(delta);
+
+	return mval;
+}
+
+/**
+ * map_encode() - Encode a Sector # or LBA to a lookup table entry value.
+ * @znd: ZDM instance
+ * @to_addr: address to encode.
+ * @value: encoded value
+ *
+ * Return: 0.
+ */
+static int map_encode(struct zoned *znd, u64 to_addr, __le32 *value)
+{
+	int err = 0;
+
+	*value = MZTEV_UNUSED;
+	if (to_addr != BAD_ADDR)
+		*value = cpu_to_le32((u32)to_addr);
+
+	return err;
+}
+
+/**
+ * release_memcache() - Free all the memcache blocks.
+ * @znd: ZDM instance
+ *
+ * Return: 0.
+ */
+static int release_memcache(struct zoned *znd)
+{
+	int no;
+
+	for (no = 0; no < Z_HASH_SZ; no++) {
+		struct list_head *_jhead = &(znd->mclist[no]);
+		struct map_cache *jrnl;
+		struct map_cache *jtmp;
+
+		if (list_empty(_jhead))
+			return 0;
+
+		list_for_each_entry_safe(jrnl, jtmp, _jhead, mclist) {
+			/** move all the journal entries into the SLT */
+			SpinLock(&znd->mclck[no]);
+			list_del(&jrnl->mclist);
+			ZDM_FREE(znd, jrnl->jdata, Z_C4K, PG_08);
+			ZDM_FREE(znd, jrnl, sizeof(*jrnl), KM_07);
+			spin_unlock(&znd->mclck[no]);
+		}
+
+	}
+	return 0;
+}
+
+/**
+ * warn_bad_lba() - Warn if a give LBA is not valid (Esp if beyond a WP)
+ * @znd: ZDM instance
+ * @lba48: 48 bit lba.
+ *
+ * Return: non-zero if lba is not valid.
+ */
+static inline int warn_bad_lba(struct zoned *znd, u64 lba48)
+{
+#define FMT_ERR "LBA %" PRIx64 " is not valid: Z# %u, off:%x wp:%x"
+	int rcode = 0;
+	u32 zone;
+
+	if (lba48 < znd->data_lba)
+		return rcode;
+
+	zone = _calc_zone(znd, lba48);
+	if (zone < znd->data_zones) { /* FIXME: MAYBE? md_end */
+		u32 gzoff = zone & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[zone >> GZ_BITS];
+		u32 wp_at = le32_to_cpu(wpg->wp_alloc[gzoff]) & Z_WP_VALUE_MASK;
+		u16 off = (lba48 - znd->data_lba) % Z_BLKSZ;
+
+		if (off >= wp_at) {
+			rcode = 1;
+			Z_ERR(znd, FMT_ERR, lba48, zone, off, wp_at);
+			dump_stack();
+		}
+	} else {
+		rcode = 1;
+		Z_ERR(znd, "LBA is not valid - Z# %u, count %u",
+		      zone, znd->data_zones);
+	}
+
+	return rcode;
+}
+
+/**
+ * mapped_free() - Release a page of lookup table entries.
+ * @znd: ZDM instance
+ * @mapped: mapped page struct to free.
+ */
+static void mapped_free(struct zoned *znd, struct map_pg *mapped)
+{
+	if (mapped) {
+		MutexLock(&mapped->md_lock);
+		WARN_ON(test_bit(IS_DIRTY, &mapped->flags));
+		if (mapped->data.addr) {
+			ZDM_FREE(znd, mapped->data.addr, Z_C4K, PG_27);
+			znd->incore_count--;
+		}
+		mutex_unlock(&mapped->md_lock);
+		ZDM_FREE(znd, mapped, sizeof(*mapped), KM_20);
+	}
+}
+
+/**
+ * flush_map() - write dirty map entres to disk.
+ * @znd: ZDM instance
+ * @map: Array of mapped pages.
+ * @count: number of elements in range.
+ * Return: non-zero on error.
+ */
+static int flush_map(struct zoned *znd, struct map_pg **map, u32 count)
+{
+	const int use_wq = 1;
+	const int sync = 0;
+	u32 ii;
+	int err = 0;
+
+	if (!map)
+		return err;
+
+	for (ii = 0; ii < count; ii++) {
+		if (map[ii] && map[ii]->data.addr) {
+			cache_if_dirty(znd, map[ii], use_wq);
+			err |= write_if_dirty(znd, map[ii], use_wq, sync);
+		}
+	}
+
+	return err;
+}
+
+/**
+ * zoned_io_flush() - flush all pending IO.
+ * @znd: ZDM instance
+ */
+static int zoned_io_flush(struct zoned *znd)
+{
+	int err = 0;
+
+	set_bit(ZF_FREEZE, &znd->flags);
+	atomic_inc(&znd->gc_throttle);
+
+	mod_delayed_work(znd->gc_wq, &znd->gc_work, 0);
+	flush_delayed_work(&znd->gc_work);
+
+	clear_bit(DO_GC_NO_PURGE, &znd->flags);
+	set_bit(DO_JOURNAL_MOVE, &znd->flags);
+	set_bit(DO_MEMPOOL, &znd->flags);
+	set_bit(DO_SYNC, &znd->flags);
+	queue_work(znd->meta_wq, &znd->meta_work);
+	flush_workqueue(znd->meta_wq);
+
+	mod_delayed_work(znd->gc_wq, &znd->gc_work, 0);
+	flush_delayed_work(&znd->gc_work);
+	atomic_dec(&znd->gc_throttle);
+
+	INIT_LIST_HEAD(&znd->smtpool);
+
+	err = flush_map(znd, znd->fwd_tm, znd->map_count);
+	if (err)
+		goto out;
+
+	err = flush_map(znd, znd->rev_tm, znd->map_count);
+	if (err)
+		goto out;
+
+	err = flush_map(znd, znd->fwd_crc, znd->crc_count);
+	if (err)
+		goto out;
+
+	err = flush_map(znd, znd->rev_crc, znd->crc_count);
+	if (err)
+		goto out;
+
+	set_bit(DO_SYNC, &znd->flags);
+	queue_work(znd->meta_wq, &znd->meta_work);
+	flush_workqueue(znd->meta_wq);
+
+out:
+	return err;
+}
+
+/**
+ * release_table_pages() - flush and free all table map entries.
+ * @znd: ZDM instance
+ */
+static void release_table_pages(struct zoned *znd)
+{
+	int entry;
+
+	if (znd->fwd_tm) {
+		for (entry = 0; entry < znd->map_count; entry++) {
+			mapped_free(znd, znd->fwd_tm[entry]);
+			znd->fwd_tm[entry] = NULL;
+		}
+	}
+	if (znd->rev_tm) {
+		for (entry = 0; entry < znd->map_count; entry++) {
+			mapped_free(znd, znd->rev_tm[entry]);
+			znd->rev_tm[entry] = NULL;
+		}
+	}
+	if (znd->fwd_crc) {
+		for (entry = 0; entry < znd->crc_count; entry++) {
+			mapped_free(znd, znd->fwd_crc[entry]);
+			znd->fwd_crc[entry] = NULL;
+		}
+	}
+	if (znd->rev_crc) {
+		for (entry = 0; entry < znd->crc_count; entry++) {
+			mapped_free(znd, znd->rev_crc[entry]);
+			znd->rev_crc[entry] = NULL;
+		}
+	}
+}
+
+/**
+ * _release_wp() - free all WP alloc/usage/used data.
+ * @znd: ZDM instance
+ *
+ */
+static void _release_wp(struct zoned *znd, struct meta_pg *wp)
+{
+	u32 gzno;
+
+	for (gzno = 0; gzno < znd->gz_count; gzno++) {
+		struct meta_pg *wpg = &wp[gzno];
+
+		if (wpg->wp_alloc)
+			ZDM_FREE(znd, wpg->wp_alloc, Z_C4K, PG_06);
+		if (wpg->zf_est)
+			ZDM_FREE(znd, wpg->zf_est, Z_C4K, PG_06);
+		if (wpg->wp_used)
+			ZDM_FREE(znd, wpg->wp_used, Z_C4K, PG_06);
+	}
+	ZDM_FREE(znd, wp, znd->gz_count * sizeof(*wp), VM_21);
+	znd->wp = NULL;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ *  Teardown a zoned device mapper instance.
+ */
+static void zoned_destroy(struct zoned *znd)
+{
+	int purge;
+	size_t ppgsz = sizeof(struct map_pg *);
+	size_t mapsz = ppgsz * znd->map_count;
+	size_t crcsz = ppgsz * znd->crc_count;
+
+	del_timer_sync(&znd->timer);
+
+	if (zoned_io_flush(znd))
+		Z_ERR(znd, "sync/flush failure");
+
+	release_table_pages(znd);
+	release_memcache(znd);
+	if (znd->dev) {
+		dm_put_device(znd->ti, znd->dev);
+		znd->dev = NULL;
+	}
+	if (znd->io_wq) {
+		destroy_workqueue(znd->io_wq);
+		znd->io_wq = NULL;
+	}
+	if (znd->bg_wq) {
+		destroy_workqueue(znd->bg_wq);
+		znd->bg_wq = NULL;
+	}
+	if (znd->gc_wq) {
+		destroy_workqueue(znd->gc_wq);
+		znd->gc_wq = NULL;
+	}
+	if (znd->meta_wq) {
+		destroy_workqueue(znd->meta_wq);
+		znd->meta_wq = NULL;
+	}
+	if (znd->io_client)
+		dm_io_client_destroy(znd->io_client);
+	if (znd->wp)
+		_release_wp(znd, znd->wp);
+	if (znd->md_crcs)
+		ZDM_FREE(znd, znd->md_crcs, Z_C4K * 2, VM_22);
+	if (znd->gc_io_buf)
+		ZDM_FREE(znd, znd->gc_io_buf, Z_C4K * GC_MAX_STRIPE, VM_04);
+	if (znd->gc_postmap.jdata) {
+		size_t sz = Z_BLKSZ * sizeof(*znd->gc_postmap.jdata);
+
+		ZDM_FREE(znd, znd->gc_postmap.jdata, sz, VM_03);
+	}
+	if (znd->fwd_tm)
+		ZDM_FREE(znd, znd->fwd_tm, mapsz, VM_21);
+	if (znd->rev_tm)
+		ZDM_FREE(znd, znd->rev_tm, mapsz, VM_22);
+	if (znd->fwd_crc)
+		ZDM_FREE(znd, znd->fwd_crc, crcsz, VM_21);
+	if (znd->rev_crc)
+		ZDM_FREE(znd, znd->rev_crc, crcsz, VM_22);
+
+	for (purge = 0; purge < ARRAY_SIZE(znd->io_vcache); purge++) {
+		size_t vcsz = IO_VCACHE_PAGES * sizeof(struct io_4k_block *);
+
+		if (znd->io_vcache[purge]) {
+			if (test_and_clear_bit(purge, &znd->io_vcache_flags))
+				Z_ERR(znd, "sync cache entry %d still in use!",
+				      purge);
+			ZDM_FREE(znd, znd->io_vcache[purge], vcsz, VM_12);
+		}
+	}
+	if (znd->z_sballoc)
+		ZDM_FREE(znd, znd->z_sballoc, Z_C4K, PG_05);
+
+	ZDM_FREE(NULL, znd, sizeof(*znd), KM_00);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void _init_streams(struct zoned *znd)
+{
+	u64 stream;
+
+	for (stream = 0; stream < ARRAY_SIZE(znd->bmkeys->stream); stream++)
+		znd->bmkeys->stream[stream] = cpu_to_le32(~0u);
+
+	znd->z_meta_resv = cpu_to_le32(znd->data_zones - 2);
+	znd->z_gc_resv   = cpu_to_le32(znd->data_zones - 1);
+	znd->z_gc_free   = znd->data_zones - 2;
+}
+
+static void _init_mdcrcs(struct zoned *znd)
+{
+	int idx;
+
+	for (idx = 0; idx < Z_C4K; idx++)
+		znd->md_crcs[idx] = MD_CRC_INIT;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void _init_wp(struct zoned *znd, u32 gzno, struct meta_pg *wpg)
+{
+	u32 gzcount = 1 << GZ_BITS;
+	u32 iter;
+
+	if (znd->data_zones < ((gzno+1) << GZ_BITS))
+		gzcount = znd->data_zones & GZ_MMSK;
+
+	/* mark as empty */
+	for (iter = 0; iter < gzcount; iter++)
+		wpg->zf_est[iter] = cpu_to_le32(Z_BLKSZ);
+
+	/* mark as n/a -- full */
+	gzcount = 1 << GZ_BITS;
+	for (; iter < gzcount; iter++) {
+		wpg->wp_alloc[iter] = cpu_to_le32(~0u);
+		wpg->wp_used[iter] = wpg->wp_alloc[iter];
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+struct meta_pg *_alloc_wp(struct zoned *znd)
+{
+	struct meta_pg *wp;
+	u32 gzno;
+
+	wp = ZDM_CALLOC(znd, znd->gz_count, sizeof(*znd->wp), VM_21, NORMAL);
+	if (!wp)
+		goto out;
+	for (gzno = 0; gzno < znd->gz_count; gzno++) {
+		struct meta_pg *wpg = &wp[gzno];
+
+		mutex_init(&wpg->wplck);
+		wpg->lba      = 2048ul + (gzno * 2);
+		wpg->wp_alloc = ZDM_ALLOC(znd, Z_C4K, PG_06, NORMAL);
+		wpg->zf_est   = ZDM_ALLOC(znd, Z_C4K, PG_06, NORMAL);
+		wpg->wp_used  = ZDM_ALLOC(znd, Z_C4K, PG_06, NORMAL);
+		if (!wpg->wp_alloc || !wpg->zf_est || !wpg->wp_used) {
+			_release_wp(znd, wp);
+			wp = NULL;
+			goto out;
+		}
+		_init_wp(znd, gzno, wpg);
+		set_bit(IS_DIRTY, &wpg->flags);
+	}
+
+out:
+	return wp;
+}
+
+/**
+ * alloc_map_tables() - Allocate map table entries.
+ * @znd: ZDM Target
+ * @mapct: Number of map entries needed.
+ * @crcct: Number of CRC entries needed.
+ */
+static int alloc_map_tables(struct zoned *znd, u64 mapct, u64 crcct)
+{
+	const size_t ptrsz = sizeof(struct map_pg *);
+	int rcode = 0;
+
+	znd->fwd_tm  = ZDM_CALLOC(znd, mapct, ptrsz, VM_21, NORMAL);
+	znd->rev_tm  = ZDM_CALLOC(znd, mapct, ptrsz, VM_22, NORMAL);
+	znd->fwd_crc = ZDM_CALLOC(znd, crcct, ptrsz, VM_21, NORMAL);
+	znd->rev_crc = ZDM_CALLOC(znd, crcct, ptrsz, VM_22, NORMAL);
+	if (!znd->fwd_tm || !znd->rev_tm || !znd->fwd_crc || !znd->rev_crc)
+		rcode = -ENOMEM;
+
+	return rcode;
+}
+
+/**
+ * do_init_zoned() - Initialize a zoned device mapper instance
+ * @ti: DM Target Info
+ * @znd: ZDM Target
+ *
+ * Return: 0 on success.
+ *
+ * Setup the zone pointer table and do a one time calculation of some
+ * basic limits.
+ *
+ *  While start of partition may not be zone aligned
+ *  md_start, data_lba and md_end are all zone aligned.
+ *  From start of partition [or device] to md_start some conv/pref
+ *  space is required for superblocks, memcache, zone pointers, crcs
+ *  and optionally pinned forward lookup blocks.
+ *
+ *    0 < znd->md_start <= znd->data_lba <= znd->md_end
+ *
+ *  incoming [FS sectors] are linearly mapped after md_end.
+ *  And blocks following data_lba are serialzied into zones either with
+ *  explicit stream id support from BIOs [FUTURE], or implictly by LBA
+ *  or type of data.
+ */
+static int do_init_zoned(struct dm_target *ti, struct zoned *znd)
+{
+	u64 size = i_size_read(get_bdev_bd_inode(znd));
+	u64 bdev_nr_sect4k = size / Z_C4K;
+	u64 data_zones = (bdev_nr_sect4k >> Z_BLKBITS) - znd->zdstart;
+	u64 mzcount = dm_div_up(data_zones, MAX_ZONES_PER_MZ);
+	u64 cache = MAX_CACHE_INCR * CACHE_COPIES * MAX_MZ_SUPP;
+	u64 z0_lba = dm_round_up(znd->start_sect, Z_BLKSZ);
+	u64 mapct = dm_div_up(data_zones << Z_BLKBITS, 1024);
+	u64 crcct = dm_div_up(mapct, 2048);
+	int no;
+	u32 mz_min = 0; /* cache */
+	int rcode = 0;
+
+	INIT_LIST_HEAD(&znd->smtpool);
+
+	for (no = 0; no < Z_HASH_SZ; no++) {
+		INIT_LIST_HEAD(&znd->mclist[no]);
+		spin_lock_init(&znd->mclck[no]);
+		mutex_init(&znd->mcmutex[no]);
+	}
+	mutex_init(&znd->map_pool_lock);
+
+	spin_lock_init(&znd->gc_lock);
+	spin_lock_init(&znd->stats_lock);
+
+	mutex_init(&znd->gc_postmap.cached_lock);
+	mutex_init(&znd->gc_vcio_lock);
+	mutex_init(&znd->vcio_lock);
+	mutex_init(&znd->io_lock);
+	mutex_init(&znd->mz_io_mutex);
+	mutex_init(&znd->mapkey_lock);
+	mutex_init(&znd->ct_lock);
+
+	znd->data_zones = data_zones;
+	znd->gz_count = mzcount;
+	znd->crc_count = crcct;
+	znd->map_count = mapct;
+
+	/* z0_lba - lba of first full zone in disk addr space      */
+
+	znd->md_start = z0_lba - znd->start_sect;
+	if (znd->md_start < cache) {
+		znd->md_start += Z_BLKSZ;
+		mz_min++;
+	}
+
+	/* md_start - lba of first full zone in partition addr space */
+	znd->s_base = znd->md_start;
+	mz_min += mzcount;
+	if (mz_min < znd->zdstart)
+		set_bit(ZF_POOL_FWD, &znd->flags);
+
+	znd->r_base = znd->s_base + (mzcount << Z_BLKBITS);
+	mz_min += mzcount;
+	if (mz_min < znd->zdstart)
+		set_bit(ZF_POOL_REV, &znd->flags);
+
+	znd->c_base = znd->r_base + (mzcount << Z_BLKBITS);
+	znd->c_mid  = znd->c_base + (mzcount * 0x20);
+	znd->c_end  = znd->c_mid + (mzcount * 0x20);
+	mz_min++;
+
+	if (mz_min < znd->zdstart) {
+		Z_ERR(znd, "Conv space for CRCs: Seting ZF_POOL_CRCS");
+		set_bit(ZF_POOL_CRCS, &znd->flags);
+	}
+
+	if (test_bit(ZF_POOL_FWD, &znd->flags)) {
+		znd->sk_low = znd->sk_high = 0;
+	} else {
+		znd->sk_low = znd->s_base;
+		znd->sk_high = znd->sk_low + (mzcount * 0x40);
+	}
+
+	/* logical *ending* lba for meta [bumped up to next zone alignment] */
+	znd->md_end = znd->c_base + (1 << Z_BLKBITS);
+
+	/* actual starting lba for data pool */
+	znd->data_lba = znd->md_end;
+	if (!test_bit(ZF_POOL_CRCS, &znd->flags))
+		znd->data_lba = znd->c_base;
+	if (!test_bit(ZF_POOL_REV, &znd->flags))
+		znd->data_lba = znd->r_base;
+	if (!test_bit(ZF_POOL_FWD, &znd->flags))
+		znd->data_lba = znd->s_base;
+
+	/* NOTE: md_end == data_lba => all meta is in conventional zones. */
+	Z_INFO(znd, "ZDM #%u", BUILD_NO);
+	Z_INFO(znd, "Start Sect: %" PRIx64 " ZDStart# %u md- start %" PRIx64
+		   " end %" PRIx64 " Data LBA %" PRIx64,
+		znd->start_sect, znd->zdstart, znd->md_start,
+		znd->md_end, znd->data_lba);
+
+	Z_INFO(znd, "%s: size:%" PRIu64 " zones: %u, gzs %u, resvd %d",
+	       __func__, size, znd->data_zones, znd->gz_count,
+	       znd->gz_count * znd->mz_provision);
+
+	znd->z_sballoc = ZDM_ALLOC(znd, Z_C4K, PG_05, NORMAL);
+	if (!znd->z_sballoc) {
+		ti->error = "couldn't allocate in-memory superblock";
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	rcode = alloc_map_tables(znd, mapct, crcct);
+	if (rcode)
+		goto out;
+
+	znd->gc_postmap.jdata = ZDM_CALLOC(znd, Z_BLKSZ,
+		sizeof(*znd->gc_postmap.jdata), VM_03, NORMAL);
+	znd->md_crcs = ZDM_ALLOC(znd, Z_C4K * 2, VM_22, NORMAL);
+	znd->gc_io_buf = ZDM_CALLOC(znd, GC_MAX_STRIPE, Z_C4K, VM_04, NORMAL);
+	znd->wp = _alloc_wp(znd);
+	znd->io_vcache[0] = ZDM_CALLOC(znd, IO_VCACHE_PAGES,
+				sizeof(struct io_4k_block), VM_12, NORMAL);
+	znd->io_vcache[1] = ZDM_CALLOC(znd, IO_VCACHE_PAGES,
+				sizeof(struct io_4k_block), VM_12, NORMAL);
+
+	if (!znd->gc_postmap.jdata || !znd->md_crcs || !znd->gc_io_buf ||
+	    !znd->wp || !znd->io_vcache[0] || !znd->io_vcache[1]) {
+		rcode = -ENOMEM;
+		goto out;
+	}
+	znd->gc_postmap.jsize = Z_BLKSZ;
+	_init_mdcrcs(znd);
+
+	znd->io_client = dm_io_client_create();
+	if (!znd->io_client) {
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	znd->meta_wq = create_singlethread_workqueue("znd_meta_wq");
+	if (!znd->meta_wq) {
+		ti->error = "couldn't start header metadata update thread";
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	znd->gc_wq = create_singlethread_workqueue("znd_gc_wq");
+	if (!znd->gc_wq) {
+		ti->error = "couldn't start GC workqueue.";
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	znd->bg_wq = create_singlethread_workqueue("znd_bg_wq");
+	if (!znd->bg_wq) {
+		ti->error = "couldn't start background workqueue.";
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+	znd->io_wq = create_singlethread_workqueue("kzoned_dm_io_wq");
+	if (!znd->io_wq) {
+		ti->error = "couldn't start header metadata update thread";
+		rcode = -ENOMEM;
+		goto out;
+	}
+
+#if USE_KTHREAD
+	init_waitqueue_head(&znd->bio_wait);
+	spin_lock_init(&znd->bio_qlck);
+#endif
+
+	INIT_WORK(&znd->meta_work, meta_work_task);
+	INIT_WORK(&znd->bg_work, bg_work_task);
+	INIT_DELAYED_WORK(&znd->gc_work, gc_work_task);
+	setup_timer(&znd->timer, activity_timeout, (unsigned long)znd);
+
+	znd->incore_count = 0;
+	znd->last_w = BAD_ADDR;
+
+	set_bit(DO_SYNC, &znd->flags);
+
+out:
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Metadata of zoned device mapper (for future backward compatibility)
+ */
+static int check_metadata_version(struct zdm_superblock *sblock)
+{
+	u32 metadata_version = le32_to_cpu(sblock->version);
+
+	if (metadata_version < MIN_ZONED_VERSION
+	    || metadata_version > MAX_ZONED_VERSION) {
+		DMERR("Unsupported metadata version %u found.",
+		      metadata_version);
+		DMERR("Only versions between %u and %u supported.",
+		      MIN_ZONED_VERSION, MAX_ZONED_VERSION);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * CRC check for superblock.
+ */
+static __le32 sb_crc32(struct zdm_superblock *sblock)
+{
+	const __le32 was = sblock->csum;
+	u32 crc;
+
+	sblock->csum = 0;
+	crc = crc32c(~(u32) 0u, sblock, sizeof(*sblock)) ^ SUPERBLOCK_CSUM_XOR;
+
+	sblock->csum = was;
+	return cpu_to_le32(crc);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Check the superblock to see if it is valid and not corrupt.
+ */
+static int sb_check(struct zdm_superblock *sblock)
+{
+	__le32 csum_le;
+
+	if (le64_to_cpu(sblock->magic) != SUPERBLOCK_MAGIC) {
+		DMERR("sb_check failed: magic %" PRIx64 ": wanted %lx",
+		      le64_to_cpu(sblock->magic), SUPERBLOCK_MAGIC);
+		return -EILSEQ;
+	}
+
+	csum_le = sb_crc32(sblock);
+	if (csum_le != sblock->csum) {
+		DMERR("sb_check failed: csum %u: wanted %u",
+		      csum_le, sblock->csum);
+		return -EILSEQ;
+	}
+
+	return check_metadata_version(sblock);
+}
+
+/*
+ * Initialize the on-disk format of a zoned device mapper.
+ */
+static int zoned_create_disk(struct dm_target *ti, struct zoned *znd)
+{
+	const int reset_non_empty = 1;
+	struct zdm_superblock *sblock = znd->super_block;
+	int err;
+
+	memset(sblock, 0, sizeof(*sblock));
+	generate_random_uuid(sblock->uuid);
+	sblock->magic = cpu_to_le64(SUPERBLOCK_MAGIC);
+	sblock->version = cpu_to_le32(Z_VERSION);
+	sblock->zdstart = cpu_to_le32(znd->zdstart);
+
+	err = zoned_wp_sync(znd, reset_non_empty);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Repair an otherwise good device mapper instance that was not cleanly removed.
+ */
+static int zoned_repair(struct zoned *znd)
+{
+	Z_INFO(znd, "Is Dirty .. zoned_repair consistency fixer TODO!!!.");
+	return -ENOMEM;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Locate the existing SB on disk and re-load or create the device-mapper
+ * instance based on the existing disk state.
+ */
+static int zoned_init_disk(struct dm_target *ti, struct zoned *znd,
+			   int create, int force)
+{
+	struct mz_superkey *key_blk = znd->z_sballoc;
+
+	int jinit = 1;
+	int n4kblks = 1;
+	int use_wq = 1;
+	int rc = 0;
+	u32 zdstart = znd->zdstart;
+
+	memset(key_blk, 0, sizeof(*key_blk));
+
+	znd->super_block = &key_blk->sblock;
+
+	znd->bmkeys = key_blk;
+	znd->bmkeys->sig0 = Z_KEY_SIG;
+	znd->bmkeys->sig1 = cpu_to_le64(Z_KEY_SIG);
+	znd->bmkeys->magic = cpu_to_le64(Z_TABLE_MAGIC);
+
+	_init_streams(znd);
+
+	znd->stale.binsz = STREAM_SIZE;
+	if (znd->data_zones < STREAM_SIZE)
+		znd->stale.binsz = znd->data_zones;
+	else if ((znd->data_zones / STREAM_SIZE) > STREAM_SIZE)
+		znd->stale.binsz = dm_div_up(znd->data_zones, STREAM_SIZE);
+	znd->stale.count = dm_div_up(znd->data_zones, znd->stale.binsz);
+
+	Z_ERR(znd, "Bin: Sz %u, count %u", znd->stale.binsz, znd->stale.count);
+
+	if (create && force) {
+		Z_ERR(znd, "Force Creating a clean instance.");
+	} else if (find_superblock(znd, use_wq, 1)) {
+		u64 sb_lba = 0;
+		u64 generation;
+
+/* FIXME: Logic [zdstart] not well tested or ... logicial */
+
+		Z_INFO(znd, "Found existing superblock");
+		if (zdstart != znd->zdstart) {
+			if (force) {
+				Z_ERR(znd, "  (force) zdstart: %u <- %u",
+					zdstart, znd->zdstart);
+			} else {
+				znd->zdstart = zdstart;
+				jinit = 0;
+			}
+		}
+
+		generation = mcache_greatest_gen(znd, use_wq, &sb_lba, NULL);
+		Z_DBG(znd, "Generation: %" PRIu64 " @ %" PRIx64,
+			generation, sb_lba);
+
+		rc = read_block(ti, DM_IO_KMEM, key_blk, sb_lba,
+				n4kblks, use_wq);
+		if (rc) {
+			ti->error = "Superblock read error.";
+			return rc;
+		}
+	}
+
+	rc = sb_check(znd->super_block);
+	if (rc) {
+		jinit = 0;
+		if (create) {
+			DMWARN("Check failed .. creating superblock.");
+			zoned_create_disk(ti, znd);
+			znd->super_block->nr_zones =
+				cpu_to_le64(znd->data_zones);
+			DMWARN("in-memory superblock created.");
+			znd->is_empty = 1;
+		} else {
+			ti->error = "Superblock check failed.";
+			return rc;
+		}
+	}
+
+	if (sb_test_flag(znd->super_block, SB_DIRTY)) {
+		int repair_check = zoned_repair(znd);
+
+		if (!force) {
+			/* if repair failed -- don't load from disk */
+			if (repair_check)
+				jinit = 0;
+		} else if (repair_check && jinit) {
+			Z_ERR(znd, "repair failed, force enabled loading ...");
+		}
+	}
+
+	if (jinit) {
+		Z_ERR(znd, "INIT: Reloading DM Zoned metadata from DISK");
+		znd->zdstart = le32_to_cpu(znd->super_block->zdstart);
+		set_bit(DO_JOURNAL_LOAD, &znd->flags);
+		queue_work(znd->meta_wq, &znd->meta_work);
+		Z_ERR(znd, "Waiting for load to complete.");
+		flush_workqueue(znd->meta_wq);
+	}
+
+	Z_ERR(znd, "ZONED: Build No %d marking superblock dirty.", BUILD_NO);
+
+	/* write the 'dirty' flag back to disk. */
+	sb_set_flag(znd->super_block, SB_DIRTY);
+	znd->super_block->csum = sb_crc32(znd->super_block);
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline sector_t jentry_value(struct map_sect_to_lba *e, bool is_block)
+{
+	sector_t value = 0;
+
+	if (is_block)
+		value = le64_to_lba48(e->physical, NULL);
+	else
+		value = le64_to_lba48(e->logical, NULL);
+
+	return value;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int compare_logical_sectors(const void *x1, const void *x2)
+{
+	const struct map_sect_to_lba *r1 = x1;
+	const struct map_sect_to_lba *r2 = x2;
+	const u64 v1 = le64_to_lba48(r1->logical, NULL);
+	const u64 v2 = le64_to_lba48(r2->logical, NULL);
+
+	return (v1 < v2) ? -1 : ((v1 > v2) ? 1 : 0);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int __find_sector_entry_chunk(struct map_sect_to_lba *data,
+				     s32 count, sector_t find, bool is_block)
+{
+	int at = -1;
+	s32 first = 0;
+	s32 last = count - 1;
+	s32 middle = (first + last) / 2;
+
+	while ((-1 == at) && (first <= last)) {
+		sector_t logical = BAD_ADDR;
+
+		if (count > middle && middle >= 0)
+			logical = jentry_value(&data[middle], is_block);
+
+		if (logical < find)
+			first = middle + 1;
+		else if (logical > find)
+			last = middle - 1;
+		else
+			at = middle;
+
+		middle = (first + last) / 2;
+	}
+	return at;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 z_lookup_key_range(struct zoned *znd, u64 addr)
+{
+	u64 found = 0ul;
+
+	if (test_bit(ZF_POOL_FWD, &znd->flags))
+		return found;
+
+	if ((znd->sk_low <= addr) && (addr < znd->sk_high))
+		found = FWD_TM_KEY_BASE + (addr - znd->sk_low);
+
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void _inc_zone_used(struct zoned *znd, u64 lba)
+{
+	u32 zone = _calc_zone(znd, lba);
+
+	if (zone < znd->data_zones) {
+		u32 gzno  = zone >> GZ_BITS;
+		u32 gzoff = zone & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		u32 used;
+		u32 wp;
+
+		MutexLock(&wpg->wplck);
+		used = le32_to_cpu(wpg->wp_used[gzoff]) + 1;
+		if (used == Z_BLKSZ) {
+			wp = le32_to_cpu(wpg->wp_alloc[gzoff]) | Z_WP_GC_READY;
+			wpg->wp_alloc[gzoff] = cpu_to_le32(wp);
+		}
+		wpg->wp_used[gzoff] = cpu_to_le32(used);
+		set_bit(IS_DIRTY, &wpg->flags);
+		mutex_unlock(&wpg->wplck);
+	}
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_add_one(struct zoned *znd, u64 dm_s, u64 lba, int gfp)
+{
+	int err = 0;
+
+	if (dm_s < znd->data_lba)
+		return err;
+
+	do {
+		err = z_mapped_to_list(znd, dm_s, lba, gfp);
+	} while (-EBUSY == err);
+
+	if (lba)
+		_inc_zone_used(znd, lba);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_discard(struct zoned *znd, u64 dm_s, u64 lba)
+{
+	int err;
+
+	do {
+		err = z_mapped_to_list(znd, dm_s, 0ul, CRIT);
+	} while (-EBUSY == err);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct map_cache *jalloc(struct zoned *znd, int gfp)
+{
+	struct map_cache *jrnl_first;
+
+	jrnl_first = ZDM_ALLOC(znd, sizeof(*jrnl_first), KM_07, gfp);
+	if (jrnl_first) {
+		mutex_init(&jrnl_first->cached_lock);
+		jrnl_first->jcount = 0;
+		jrnl_first->jsorted = 0;
+		jrnl_first->jdata = ZDM_CALLOC(znd, Z_UNSORTED,
+			sizeof(*jrnl_first->jdata), PG_08, gfp);
+
+		if (jrnl_first->jdata) {
+			u64 logical = Z_LOWER48;
+			u64 physical = Z_LOWER48;
+
+			jrnl_first->jdata[0].logical = cpu_to_le64(logical);
+			jrnl_first->jdata[0].physical = cpu_to_le64(physical);
+			jrnl_first->jsize = Z_UNSORTED - 1;
+
+		} else {
+			Z_ERR(znd, "%s: in memory journal is out of space.",
+			      __func__);
+			ZDM_FREE(znd, jrnl_first, sizeof(*jrnl_first), KM_07);
+			jrnl_first = NULL;
+		}
+	}
+	return jrnl_first;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct map_cache *jfirst_entry(struct zoned *znd, int n)
+{
+	unsigned long flags;
+	struct map_cache *jrnl;
+
+	spin_lock_irqsave(&znd->mclck[n], flags);
+	jrnl = list_first_entry_or_null(&znd->mclist[n], typeof(*jrnl), mclist);
+	if (jrnl && (&jrnl->mclist != &znd->mclist[n]))
+		atomic_inc(&jrnl->refcount);
+	else
+		jrnl = NULL;
+	spin_unlock_irqrestore(&znd->mclck[n], flags);
+
+	return jrnl;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void mclist_add(struct zoned *znd, int n, struct map_cache *mc)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&znd->mclck[n], flags);
+	list_add(&mc->mclist, &znd->mclist[n]);
+	spin_unlock_irqrestore(&znd->mclck[n], flags);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void jderef(struct zoned *znd, int n, struct map_cache *mcache)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&znd->mclck[n], flags);
+	atomic_dec(&mcache->refcount);
+	spin_unlock_irqrestore(&znd->mclck[n], flags);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline struct map_cache *jnext_entry(struct zoned *znd, int n,
+					    struct map_cache *mcache)
+{
+	unsigned long flags;
+	struct map_cache *next;
+
+	spin_lock_irqsave(&znd->mclck[n], flags);
+	next = list_next_entry(mcache, mclist);
+	if (next && (&next->mclist != &znd->mclist[n]))
+		atomic_inc(&next->refcount);
+	else
+		next = NULL;
+	atomic_dec(&mcache->refcount);
+	spin_unlock_irqrestore(&znd->mclck[n], flags);
+
+	return next;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void memcache_sort(struct zoned *znd, struct map_cache *mcache, int no)
+{
+	if (mcache->jsorted < mcache->jcount) {
+		int locked = mutex_is_locked(&znd->mcmutex[no]);
+
+		if (!locked)
+			MutexLock(&znd->mcmutex[no]);
+
+		sort(&mcache->jdata[1], mcache->jcount,
+		     sizeof(*mcache->jdata),
+		     compare_logical_sectors, NULL);
+		mcache->jsorted = mcache->jcount;
+
+		if (!locked)
+			mutex_unlock(&znd->mcmutex[no]);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int memcache_lock_and_sort(struct zoned *znd, struct map_cache *mcache,
+				  int no)
+{
+	if (mcache->jsorted < mcache->jcount &&
+	    mcache->busy_locked.counter == 0) {
+		mutex_lock_nested(&mcache->cached_lock, SINGLE_DEPTH_NESTING);
+		memcache_sort(znd, mcache, no);
+		mutex_unlock(&mcache->cached_lock);
+		return 0;
+	}
+	return -EBUSY;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int jlinear_find(struct map_cache *mcache, u64 dm_s)
+{
+	int at = -1;
+	int jentry;
+
+	for (jentry = mcache->jcount; jentry > 0; jentry--) {
+		u64 logi = le64_to_lba48(mcache->jdata[jentry].logical, NULL);
+
+		if (logi == dm_s) {
+			at = jentry - 1;
+			goto out;
+		}
+	}
+
+out:
+	return at;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 z_lookup_cache(struct zoned *znd, u64 addr)
+{
+	struct map_cache *mcache;
+	u64 found = 0ul;
+	int no = addr % Z_HASH_SZ;
+
+	mcache = jfirst_entry(znd, no);
+	while (mcache) {
+		int at;
+		int err;
+
+		/* sort, if needed. only err is -EBUSY so do a linear find. */
+		err = memcache_lock_and_sort(znd, mcache, no);
+		if (!err)
+			at = __find_sector_entry_chunk(&mcache->jdata[1],
+				mcache->jcount, addr, 0);
+		else
+			at = jlinear_find(mcache, addr);
+
+		if (at != -1) {
+			struct map_sect_to_lba *data = &mcache->jdata[at + 1];
+
+			found = le64_to_lba48(data->physical, NULL);
+		}
+		if (found) {
+			jderef(znd, no, mcache);
+			goto out;
+		}
+
+		mcache = jnext_entry(znd, no, mcache);
+	}
+out:
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int lba_in_zone(struct zoned *znd, struct map_cache *mcache, u32 zone)
+{
+	int jentry;
+
+	if (zone >= znd->data_zones)
+		goto out;
+
+	for (jentry = mcache->jcount; jentry > 0; jentry--) {
+		u64 lba = le64_to_lba48(mcache->jdata[jentry].physical, NULL);
+
+		if (lba && _calc_zone(znd, lba) == zone)
+			return 1;
+	}
+out:
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_verify_cache(struct zoned *znd, u32 zone)
+{
+	struct map_cache *mcache = NULL;
+	int err = 0;
+	int no;
+
+	for (no = 0; no < Z_HASH_SZ; no++) {
+		mcache = jfirst_entry(znd, no);
+		while (mcache) {
+			MutexLock(&mcache->cached_lock);
+			if (lba_in_zone(znd, mcache, zone)) {
+				Z_ERR(znd, "GC: **ERR** %" PRIx32
+				      " LBA in cache <= Corrupt", zone);
+				err = 1;
+				znd->meta_result = -ENOSPC;
+			}
+			mutex_unlock(&mcache->cached_lock);
+			mcache = jnext_entry(znd, no, mcache);
+		}
+	}
+
+	return err;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int __cached_to_tables(struct zoned *znd, int no, u32 zone)
+{
+	struct map_cache *jrnl = NULL;
+	int err = 0;
+
+	jrnl = jfirst_entry(znd, no);
+	while (jrnl) {
+		int try_free = 0;
+		struct map_cache *jskip;
+
+		atomic_inc(&jrnl->busy_locked);
+		MutexLock(&jrnl->cached_lock);
+		if (jrnl->jcount == jrnl->jsize) {
+			memcache_sort(znd, jrnl, no);
+			err = move_to_map_tables(znd, jrnl);
+			if (!err && (jrnl->jcount == 0))
+				try_free = 1;
+		} else {
+			if (lba_in_zone(znd, jrnl, zone)) {
+				Z_ERR(znd,
+					"Moving %d Runts because z: %u",
+					jrnl->jcount, zone);
+
+				memcache_sort(znd, jrnl, no);
+				err = move_to_map_tables(znd, jrnl);
+			}
+		}
+		mutex_unlock(&jrnl->cached_lock);
+		atomic_dec(&jrnl->busy_locked);
+
+		if (err) {
+			jderef(znd, no, jrnl);
+			Z_ERR(znd, "%s: Sector map failed.", __func__);
+			goto out;
+		}
+
+		jskip = jnext_entry(znd, no, jrnl);
+		if (try_free && mutex_trylock(&znd->mcmutex[no])) {
+			if (jrnl->refcount.counter == 0) {
+				unsigned long flags;
+				size_t sz = Z_UNSORTED * sizeof(*jrnl->jdata);
+
+				spin_lock_irqsave(&znd->mclck[no], flags);
+				list_del(&jrnl->mclist);
+				spin_unlock_irqrestore(&znd->mclck[no], flags);
+
+				ZDM_FREE(znd, jrnl->jdata, sz, PG_08);
+				ZDM_FREE(znd, jrnl, sizeof(*jrnl), KM_07);
+				jrnl = NULL;
+
+				znd->mc_entries--;
+			}
+			mutex_unlock(&znd->mcmutex[no]);
+		}
+		jrnl = jskip;
+	}
+out:
+
+	return err;
+}
+
+
+/**
+ * _cached_to_tables() - Migrate memcache entries to lookup tables
+ * @znd: ZDM instance
+ * @zone: zone to force migration for partial memcache block
+ *
+ * Scan the memcache and move any full blocks to lookup tables
+ * If a (the) partial memcache block contains lbas that map to zone force
+ * early migration of the memcache block to ensure it is properly accounted
+ * for and migrated during and upcoming GC pass.
+ *
+ * Return: 0 on success or -errno value
+ */
+static int _cached_to_tables(struct zoned *znd, u32 zone)
+{
+	int err = 0;
+	int no;
+
+	for (no = 0; no < Z_HASH_SZ; no++) {
+		int sub = __cached_to_tables(znd, no, zone);
+
+		if (sub)
+			err = sub;
+	}
+	return err;
+}
+
+
+/**
+ * z_flush_bdev() - Request backing device flushed to disk.
+ * @param znd: ZDM instance
+ *
+ * Return: 0 on success or -errno value
+ */
+static int z_flush_bdev(struct zoned *znd)
+{
+	int err;
+	sector_t bi_done;
+
+	err = blkdev_issue_flush(znd->dev->bdev, GFP_KERNEL, &bi_done);
+	if (err)
+		Z_ERR(znd, "%s: flush failing sector %lu!", __func__, bi_done);
+
+	return err;
+}
+
+/**
+ * _meta_sync_to_disk() - Write ZDM state to disk.
+ * @znd: ZDM instance
+ *
+ * Return: 0 on success or -errno value
+ */
+static int _meta_sync_to_disk(struct zoned *znd)
+{
+	int err = 0;
+
+	err = do_sync_metadata(znd);
+	if (err) {
+		Z_ERR(znd, "Uh oh: do_sync_metadata -> %d", err);
+		goto out;
+	}
+
+	err = z_mapped_sync(znd);
+	if (err) {
+		Z_ERR(znd, "Uh oh. z_mapped_sync -> %d", err);
+		goto out;
+	}
+
+out:
+	return err;
+}
+
+/**
+ * do_init_from_journal() - Restore ZDM state from disk.
+ * @znd: ZDM instance
+ *
+ * Return: 0 on success or -errno value
+ */
+static int do_init_from_journal(struct zoned *znd)
+{
+	int err = 0;
+
+	if (test_and_clear_bit(DO_JOURNAL_LOAD, &znd->flags))
+		err = z_mapped_init(znd);
+
+	return err;
+}
+
+/**
+ * do_journal_to_table() - Migrate memcache entries to lookup tables
+ * @znd: ZDM instance
+ *
+ * Return: 0 on success or -errno value
+ */
+static int do_journal_to_table(struct zoned *znd)
+{
+	int err = 0;
+
+	if (test_and_clear_bit(DO_JOURNAL_MOVE, &znd->flags))
+		err = _cached_to_tables(znd, znd->data_zones);
+
+	return err;
+}
+
+/**
+ * do_free_pages() - Release some pages to RAM
+ * @znd: ZDM instance.
+ *
+ * Return: 0 on success or -errno value
+ */
+static int do_free_pages(struct zoned *znd)
+{
+	int err = 0;
+
+	if (test_and_clear_bit(DO_MEMPOOL, &znd->flags)) {
+		int pool_size = MZ_MEMPOOL_SZ * 4;
+
+		if (is_expired_msecs(znd->age, MEM_PURGE_MSECS))
+			pool_size = MZ_MEMPOOL_SZ;
+
+		if (is_expired_msecs(znd->age, 5000))
+			pool_size = 3;
+
+		err = keep_active_pages(znd, pool_size);
+	}
+	return err;
+}
+
+/**
+ * do_sync_to_disk() - Write ZDM state to disk.
+ * @znd: ZDM instance
+ *
+ * Return: 0 on success or -errno value
+ */
+static int do_sync_to_disk(struct zoned *znd)
+{
+	int err = 0;
+
+	if (test_and_clear_bit(DO_SYNC, &znd->flags))
+		err = _meta_sync_to_disk(znd);
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void meta_work_task(struct work_struct *work)
+{
+	int err = 0;
+	struct zoned *znd;
+
+	if (!work)
+		return;
+
+	znd = container_of(work, struct zoned, meta_work);
+	if (!znd)
+		return;
+
+	MutexLock(&znd->mz_io_mutex);
+	err = do_init_from_journal(znd);
+
+	/*
+	 * Reduce memory pressure on journal list of arrays
+	 * by pushing them into the sector map lookup tables
+	 */
+	if (!err)
+		err = do_journal_to_table(znd);
+
+	/*
+	 *  Reduce memory pressure on sector map lookup tables
+	 * by pushing them onto disc
+	 */
+	if (!err)
+		err = do_free_pages(znd);
+
+	/* force a consistent set of meta data out to disk */
+	if (!err)
+		err = do_sync_to_disk(znd);
+
+	znd->age = jiffies_64;
+	if (err < 0)
+		znd->meta_result = err;
+
+	clear_bit(DO_METAWORK_QD, &znd->flags);
+
+	mutex_unlock(&znd->mz_io_mutex);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_to_list(struct zoned *znd, u64 dm_s, u64 lba, int gfp)
+{
+	struct map_cache *jrnl = NULL;
+	struct map_cache *jrnl_first = NULL;
+	int handled = 0;
+	int list_count = 0;
+	int err = 0;
+	int no = dm_s % Z_HASH_SZ;
+
+	jrnl = jfirst_entry(znd, no);
+	while (jrnl) {
+		struct map_cache *jskip;
+		int try_free = 0;
+		int at;
+
+		MutexLock(&jrnl->cached_lock);
+		memcache_sort(znd, jrnl, no);
+		at = __find_sector_entry_chunk(&jrnl->jdata[1], jrnl->jcount,
+					       dm_s, 0);
+		if (at != -1) {
+			struct map_sect_to_lba *data;
+			u64 lba_was;
+			u64 physical;
+
+			MutexLock(&znd->mcmutex[no]);
+			data = &jrnl->jdata[at + 1];
+			lba_was = le64_to_lba48(data->physical, NULL);
+			physical = lba & Z_LOWER48;
+
+			if (lba != lba_was) {
+				Z_DBG(znd, "Remap %" PRIx64 " -> %" PRIx64
+					   " (was %" PRIx64 "->%" PRIx64 ")",
+				      dm_s, lba,
+				      le64_to_lba48(data->logical, NULL),
+				      le64_to_lba48(data->physical, NULL));
+				err = unused_phy(znd, lba_was, 0, gfp);
+				if (err == 1)
+					err = 0;
+			}
+			data->physical = cpu_to_le64(physical);
+			handled = 1;
+			mutex_unlock(&znd->mcmutex[no]);
+		} else if (!jrnl_first) {
+			if (jrnl->jcount < jrnl->jsize)
+				jrnl_first = jrnl;
+		} else if (jrnl->jcount == 0 && jrnl->refcount.counter == 0) {
+			try_free = 1;
+		}
+		mutex_unlock(&jrnl->cached_lock);
+		if (handled) {
+			jderef(znd, no, jrnl);
+			goto out;
+		}
+
+		jskip = jnext_entry(znd, no, jrnl);
+		if (try_free) {
+			unsigned long flags;
+			size_t sz;
+
+			MutexLock(&znd->mcmutex[no]);
+			if (!jrnl->refcount.counter == 0) {
+
+				sz = Z_UNSORTED * sizeof(*jrnl->jdata);
+				spin_lock_irqsave(&znd->mclck[no], flags);
+				list_del(&jrnl->mclist);
+				spin_unlock_irqrestore(&znd->mclck[no], flags);
+
+				ZDM_FREE(znd, jrnl->jdata, sz, PG_08);
+				ZDM_FREE(znd, jrnl, sizeof(*jrnl), KM_07);
+				jrnl = NULL;
+
+				znd->mc_entries--;
+			}
+			mutex_unlock(&znd->mcmutex[no]);
+		}
+		jrnl = jskip;
+		list_count++;
+	}
+
+	/* ------------------------------------------------------------------ */
+	/* ------------------------------------------------------------------ */
+	if (jrnl_first) {
+		atomic_inc(&jrnl_first->refcount);
+	} else {
+		jrnl_first = jalloc(znd, gfp);
+		if (jrnl_first) {
+			atomic_inc(&jrnl_first->refcount);
+			mclist_add(znd, no, jrnl_first);
+		} else {
+			Z_ERR(znd, "%s: in memory journal is out of space.",
+			      __func__);
+			err = -ENOMEM;
+			goto out;
+		}
+
+		if (list_count > JOURNAL_MEMCACHE_BLOCKS)
+			set_bit(DO_JOURNAL_MOVE, &znd->flags);
+		znd->mc_entries = list_count + 1;
+	}
+
+	/* ------------------------------------------------------------------ */
+	/* ------------------------------------------------------------------ */
+
+	if (jrnl_first) {
+		MutexLock(&znd->mcmutex[no]);
+		MutexLock(&jrnl_first->cached_lock);
+		if (jrnl_first->jcount < jrnl_first->jsize) {
+			u16 idx = ++jrnl_first->jcount;
+
+			WARN_ON(jrnl_first->jcount > jrnl_first->jsize);
+
+			jrnl_first->jdata[idx].logical = lba48_to_le64(0, dm_s);
+			jrnl_first->jdata[idx].physical = lba48_to_le64(0, lba);
+		} else {
+			Z_ERR(znd, "%s: cached bin out of space!", __func__);
+			err = -EBUSY;
+		}
+		mutex_unlock(&jrnl_first->cached_lock);
+		jderef(znd, no, jrnl_first);
+		mutex_unlock(&znd->mcmutex[no]);
+	}
+out:
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline u64 next_generation(struct zoned *znd)
+{
+	u64 generation = le64_to_cpu(znd->bmkeys->generation);
+
+	if (generation == 0)
+		generation = 2;
+
+	generation++;
+	if (generation == 0)
+		generation++;
+
+	return generation;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_sync(struct zoned *znd)
+{
+	struct dm_target *ti = znd->ti;
+	struct map_cache *jrnl;
+	int nblks = 1;
+	int use_wq = 0;
+	int rc = 1;
+	int jwrote = 0;
+	int cached = 0;
+	int idx = 0;
+	int no;
+	int need_sync_io = 1;
+	u64 lba = LBA_SB_START;
+	u64 generation = next_generation(znd);
+	u64 modulo = CACHE_COPIES;
+	u64 incr = MAX_CACHE_INCR;
+	struct io_4k_block *io_vcache;
+
+	MutexLock(&znd->vcio_lock);
+	io_vcache = get_io_vcache(znd, NORMAL);
+	if (!io_vcache) {
+		Z_ERR(znd, "%s: FAILED to get IO CACHE.", __func__);
+		rc = -ENOMEM;
+		goto out;
+	}
+
+
+/* TIME TO DO SOME WORK!! */
+
+/* dirty wp blocks (zone pointers) [allow for 64*4 blocks] */
+/* md_crcs-> 0x2148/9 */
+
+	lba = 0x2048;
+	for (idx = 0; idx < znd->gz_count; idx++) {
+		struct meta_pg *wpg = &znd->wp[idx];
+
+		if (test_bit(IS_DIRTY, &wpg->flags)) {
+			znd->bmkeys->wp_crc[idx] = crc_md_le16(wpg->wp_alloc,
+				Z_CRC_4K);
+			znd->bmkeys->zf_crc[idx] = crc_md_le16(wpg->zf_est,
+				Z_CRC_4K);
+			if (test_bit(IS_DIRTY, &wpg->flags)) {
+				memcpy(io_vcache[0].data, wpg->wp_alloc, Z_C4K);
+				memcpy(io_vcache[1].data, wpg->zf_est, Z_C4K);
+
+				rc = write_block(ti, DM_IO_VMA, io_vcache, lba,
+					cached, use_wq);
+				if (rc)
+					goto out;
+			}
+			clear_bit(IS_DIRTY, &wpg->flags);
+		}
+		lba += 2;
+	}
+
+	lba += (generation % modulo) * incr;
+	if (lba == 0)
+		lba++;
+
+	znd->bmkeys->generation = cpu_to_le64(generation);
+	znd->bmkeys->gc_resv = cpu_to_le32(znd->z_gc_resv);
+	znd->bmkeys->meta_resv = cpu_to_le32(znd->z_meta_resv);
+
+	idx = 0;
+	for (no = 0; no < Z_HASH_SZ; no++) {
+		jrnl = jfirst_entry(znd, no);
+		while (jrnl) {
+			u64 phy = le64_to_lba48(jrnl->jdata[0].physical, NULL);
+			u16 jcount = jrnl->jcount & 0xFFFF;
+
+			jrnl->jdata[0].physical = lba48_to_le64(jcount, phy);
+			znd->bmkeys->crcs[idx] =
+				crc_md_le16(jrnl->jdata, Z_CRC_4K);
+			idx++;
+
+			memcpy(io_vcache[cached].data, jrnl->jdata, Z_C4K);
+			cached++;
+
+			if (cached == IO_VCACHE_PAGES) {
+				rc = write_block(ti, DM_IO_VMA, io_vcache, lba,
+						 cached, use_wq);
+				if (rc) {
+					Z_ERR(znd, "%s: cache-> %" PRIu64
+					      " [%d blks] %p -> %d",
+					      __func__, lba, nblks,
+					      jrnl->jdata, rc);
+					jderef(znd, no, jrnl);
+					goto out;
+				}
+				lba    += cached;
+				jwrote += cached;
+				cached  = 0;
+			}
+			jrnl = jnext_entry(znd, no, jrnl);
+		}
+		jwrote += cached;
+	}
+	if (jwrote > 20)
+		Z_ERR(znd, "**WARNING** large map cache %d", jwrote);
+
+	znd->bmkeys->md_crc = crc_md_le16(znd->md_crcs, 2 * Z_CRC_4K);
+	znd->bmkeys->n_crcs = cpu_to_le16(jwrote);
+	znd->bmkeys->crc32 = 0;
+	znd->bmkeys->crc32 = cpu_to_le32(crc32c(~0u, znd->bmkeys, Z_CRC_4K));
+	if (cached < (IO_VCACHE_PAGES - 3)) {
+		memcpy(io_vcache[cached].data, znd->bmkeys, Z_C4K);
+		cached++;
+		memcpy(io_vcache[cached].data, znd->md_crcs, Z_C4K * 2);
+		cached++;
+		need_sync_io = 0;
+	}
+
+	do {
+		if (cached > 0) {
+			rc = write_block(ti, DM_IO_VMA, io_vcache, lba, cached,
+					 use_wq);
+			if (rc) {
+				Z_ERR(znd, "%s: Jrnl-> %" PRIu64
+				      " [%d blks] %p -> %d",
+				      __func__, lba, cached, io_vcache, rc);
+				goto out;
+			}
+			lba += cached;
+		}
+
+		cached = 0;
+		if (need_sync_io) {
+			memcpy(io_vcache[cached].data, znd->bmkeys, Z_C4K);
+			cached++;
+			need_sync_io = 0;
+		}
+	} while (cached > 0);
+
+out:
+	put_io_vcache(znd, io_vcache);
+	mutex_unlock(&znd->vcio_lock);
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_key_page(void *_data)
+{
+	int is_key = 0;
+	struct mz_superkey *data = _data;
+
+	/* Starts with Z_KEY_SIG and ends with magic */
+
+	if (le64_to_cpu(data->sig1) == Z_KEY_SIG &&
+	    le64_to_cpu(data->magic) == Z_TABLE_MAGIC) {
+		__le32 orig = data->crc32;
+		__le32 crc_check;
+
+		data->crc32 = 0;
+		crc_check = cpu_to_le32(crc32c(~0u, data, Z_CRC_4K));
+		data->crc32 = orig;
+		if (crc_check == orig)
+			is_key = 1;
+	}
+	return is_key;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void zoned_personality(struct zoned *znd,
+				     struct zdm_superblock *sblock)
+{
+	znd->zdstart = le32_to_cpu(sblock->zdstart);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int find_superblock_at(struct zoned *znd, u64 lba, int use_wq,
+			      int do_init)
+{
+	struct dm_target *ti = znd->ti;
+	int found = 0;
+	int nblks = 1;
+	int rc = -ENOMEM;
+	u32 count = 0;
+	u64 *data = ZDM_ALLOC(znd, Z_C4K, PG_10, NORMAL);
+
+	if (!data) {
+		Z_ERR(znd, "No memory for finding generation ..");
+		return 0;
+	}
+	if (lba == 0)
+		lba++;
+	do {
+		rc = read_block(ti, DM_IO_KMEM, data, lba, nblks, use_wq);
+		if (rc) {
+			Z_ERR(znd, "%s: read @%" PRIu64 " [%d blks] %p -> %d",
+			      __func__, lba, nblks, data, rc);
+			goto out;
+		}
+		if (is_key_page(data)) {
+			struct mz_superkey *kblk = (struct mz_superkey *) data;
+			struct zdm_superblock *sblock = &kblk->sblock;
+			int err = sb_check(sblock);
+
+			if (!err) {
+				found = 1;
+				if (do_init)
+					zoned_personality(znd, sblock);
+			}
+			goto out;
+		}
+		if (data[0] == 0 && data[1] == 0) {
+			/* No SB here. */
+			Z_ERR(znd, "FGen: Invalid block %" PRIx64 "?", lba);
+			goto out;
+		}
+		lba++;
+		count++;
+		if (count > MAX_CACHE_SYNC) {
+			Z_ERR(znd, "FSB: Too deep to be useful.");
+			goto out;
+		}
+	} while (!found);
+
+out:
+	ZDM_FREE(znd, data, Z_C4K, PG_10);
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int find_superblock(struct zoned *znd, int use_wq, int do_init)
+{
+	int found = 0;
+	u64 lba;
+
+	for (lba = 0; lba < 0x800; lba += MAX_CACHE_INCR) {
+		found = find_superblock_at(znd, lba, use_wq, do_init);
+		if (found)
+			break;
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 mcache_find_gen(struct zoned *znd, u64 lba, int use_wq,
+				    u64 *sb_lba)
+{
+	struct dm_target *ti = znd->ti;
+	u64 generation = 0;
+	int nblks = 1;
+	int rc = 1;
+	int done = 0;
+	u32 count = 0;
+	u64 *data = ZDM_ALLOC(znd, Z_C4K, PG_11, NORMAL);
+
+	if (!data) {
+		Z_ERR(znd, "No memory for finding generation ..");
+		return 0;
+	}
+	do {
+		rc = read_block(ti, DM_IO_KMEM, data, lba, nblks, use_wq);
+		if (rc) {
+			Z_ERR(znd, "%s: Jrnl-> %" PRIu64 " [%d blks] %p -> %d",
+			      __func__, lba, nblks, data, rc);
+			goto out;
+		}
+		if (is_key_page(data)) {
+			struct mz_superkey *kblk = (struct mz_superkey *) data;
+
+			generation = le64_to_cpu(kblk->generation);
+			done = 1;
+			if (sb_lba)
+				*sb_lba = lba;
+			goto out;
+		}
+		if (data[0] == 0 && data[1] == 0) {
+			/* No SB here... */
+			Z_DBG(znd, "FGen: Invalid block %" PRIx64 "?", lba);
+			goto out;
+		}
+		lba++;
+		count++;
+		if (count > MAX_CACHE_SYNC) {
+			Z_ERR(znd, "FGen: Too deep to be useful.");
+			goto out;
+		}
+	} while (!done);
+
+out:
+	ZDM_FREE(znd, data, Z_C4K, PG_11);
+	return generation;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int cmp_gen(u64 left, u64 right)
+{
+	int result = 0;
+
+	if (left != right) {
+		u64 delta = (left > right) ? left - right : right - left;
+
+		result = -1;
+		if (delta > 1) {
+			if (left == BAD_ADDR)
+				result = 1;
+		} else {
+			if (right > left)
+				result = 1;
+		}
+	}
+
+	return result;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 mcache_greatest_gen(struct zoned *znd, int use_wq, u64 *sb,
+				u64 *at_lba)
+{
+	u64 lba = LBA_SB_START;
+	u64 gen_no[CACHE_COPIES] = { 0ul, 0ul, 0ul };
+	u64 gen_lba[CACHE_COPIES] = { 0ul, 0ul, 0ul };
+	u64 gen_sb[CACHE_COPIES] = { 0ul, 0ul, 0ul };
+	u64 incr = MAX_CACHE_INCR;
+	int locations = ARRAY_SIZE(gen_lba);
+	int pick = 0;
+	int idx;
+
+	for (idx = 0; idx < locations; idx++) {
+		u64 *pAt = &gen_sb[idx];
+
+		gen_lba[idx] = lba;
+		gen_no[idx] = mcache_find_gen(znd, lba, use_wq, pAt);
+		if (gen_no[idx])
+			pick = idx;
+		lba += incr;
+	}
+
+	for (idx = 0; idx < locations; idx++) {
+		if (cmp_gen(gen_no[pick], gen_no[idx]) > 0)
+			pick = idx;
+	}
+
+	if (gen_no[pick]) {
+		if (at_lba)
+			*at_lba = gen_lba[pick];
+		if (sb)
+			*sb = gen_sb[pick];
+	}
+
+	return gen_no[pick];
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u64 count_stale_blocks(struct zoned *znd, u32 gzno, struct meta_pg *wpg)
+{
+	u32 gzcount = 1 << GZ_BITS;
+	u32 iter;
+	u64 stale = 0;
+
+	if ((gzno << GZ_BITS) > znd->data_zones)
+		gzcount = znd->data_zones & GZ_MMSK;
+
+	/* mark as empty */
+	for (iter = 0; iter < gzcount; iter++) {
+		u32 wp = le32_to_cpu(wpg->wp_alloc[iter]) & Z_WP_VALUE_MASK;
+		u32 nf = le32_to_cpu(wpg->zf_est[iter]) & Z_WP_VALUE_MASK;
+
+		if (wp > (Z_BLKSZ - nf))
+			stale += (wp - (Z_BLKSZ - nf));
+	}
+
+	return stale;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_init(struct zoned *znd)
+{
+	struct dm_target *ti = znd->ti;
+	int nblks = 1;
+	int wq = 0;
+	int rc = 1;
+	int done = 0;
+	int jfound = 0;
+	int idx = 0;
+	struct list_head hjload;
+	u64 lba = 0;
+	u64 generation;
+	__le32 crc_chk;
+	struct io_4k_block *io_vcache;
+
+	MutexLock(&znd->vcio_lock);
+	io_vcache = get_io_vcache(znd, NORMAL);
+
+	if (!io_vcache) {
+		Z_ERR(znd, "%s: FAILED to get SYNC CACHE.", __func__);
+		rc = -ENOMEM;
+		goto out;
+	}
+
+	/*
+	 * Read write printers
+	 */
+	lba = 0x2048;
+	for (idx = 0; idx < znd->gz_count; idx++) {
+		struct meta_pg *wpg = &znd->wp[idx];
+
+		rc = read_block(ti, DM_IO_KMEM, wpg->wp_alloc, lba, 1, wq);
+		if (rc)
+			goto out;
+		rc = read_block(ti, DM_IO_KMEM, wpg->zf_est,   lba + 1, 1, wq);
+		if (rc)
+			goto out;
+		lba += 2;
+	}
+
+	INIT_LIST_HEAD(&hjload);
+
+	generation = mcache_greatest_gen(znd, wq, NULL, &lba);
+	if (generation == 0) {
+		rc = -ENODATA;
+		goto out;
+	}
+
+	if (lba == 0)
+		lba++;
+
+	do {
+		struct map_cache *jrnl = jalloc(znd, NORMAL);
+
+		if (!jrnl) {
+			rc = -ENOMEM;
+			goto out;
+		}
+
+		rc = read_block(ti, DM_IO_KMEM,
+				jrnl->jdata, lba, nblks, wq);
+		if (rc) {
+			Z_ERR(znd, "%s: Jrnl-> %" PRIu64 " [%d blks] %p -> %d",
+			      __func__, lba, nblks, jrnl->jdata, rc);
+
+			goto out;
+		}
+		lba++;
+
+		if (is_key_page(jrnl->jdata)) {
+			size_t sz = Z_UNSORTED * sizeof(*jrnl->jdata);
+
+			memcpy(znd->bmkeys, jrnl->jdata, Z_C4K);
+			jrnl->jcount = 0;
+			done = 1;
+			ZDM_FREE(znd, jrnl->jdata, sz, PG_08);
+			ZDM_FREE(znd, jrnl, sizeof(*jrnl), KM_07);
+			jrnl = NULL;
+		} else {
+			u16 jcount;
+
+			(void)le64_to_lba48(jrnl->jdata[0].physical, &jcount);
+			jrnl->jcount = jcount;
+
+			list_add(&(jrnl->mclist), &hjload);
+			jfound++;
+		}
+
+		if (jfound > MAX_CACHE_SYNC) {
+			rc = -EIO;
+			goto out;
+		}
+	} while (!done);
+
+	rc = read_block(ti, DM_IO_VMA, znd->md_crcs, lba, 2, wq);
+	if (rc)
+		goto out;
+
+	crc_chk = znd->bmkeys->crc32;
+	znd->bmkeys->crc32 = 0;
+	znd->bmkeys->crc32 = cpu_to_le32(crc32c(~0u, znd->bmkeys, Z_CRC_4K));
+
+	if (crc_chk != znd->bmkeys->crc32) {
+		Z_ERR(znd, "Bad Block Map KEYS!");
+		Z_ERR(znd, "Key CRC: Ex: %04x vs %04x <- calculated",
+		      le32_to_cpu(crc_chk),
+		      le32_to_cpu(znd->bmkeys->crc32));
+		rc = -EIO;
+	}
+
+	if (jfound != le16_to_cpu(znd->bmkeys->n_crcs)) {
+		Z_ERR(znd, " ... mcache entries: found = %u, expected = %u",
+		      jfound, le16_to_cpu(znd->bmkeys->n_crcs));
+		rc = -EIO;
+	}
+	if ((crc_chk == znd->bmkeys->crc32) && !list_empty(&hjload)) {
+		struct map_cache *jrnl;
+		struct map_cache *jsafe;
+
+		idx = 0;
+		list_for_each_entry_safe(jrnl, jsafe, &hjload, mclist) {
+			__le16 crc = crc_md_le16(jrnl->jdata, Z_CRC_4K);
+
+			Z_DBG(znd, "JRNL CRC: %u: %04x [vs %04x] (c:%d)",
+			      idx, le16_to_cpu(crc),
+			      le16_to_cpu(znd->bmkeys->crcs[idx]),
+			      jrnl->jcount);
+
+			if (crc == znd->bmkeys->crcs[idx]) {
+				int no = le64_to_lba48(jrnl->jdata[1].logical,
+						NULL) % Z_HASH_SZ;
+
+				mclist_add(znd, no, jrnl);
+			} else {
+				Z_ERR(znd, ".. %04x [vs %04x] (c:%d)",
+				      le16_to_cpu(crc),
+				      le16_to_cpu(znd->bmkeys->crcs[idx]),
+				      jrnl->jcount);
+				rc = -EIO;
+			}
+			idx++;
+		}
+	}
+	crc_chk = crc_md_le16(znd->md_crcs, Z_CRC_4K * 2);
+	if (crc_chk != znd->bmkeys->md_crc) {
+		Z_ERR(znd, "CRC of CRC PGs: Ex %04x vs %04x  <- calculated",
+		      le16_to_cpu(znd->bmkeys->md_crc),
+		      le16_to_cpu(crc_chk));
+	}
+
+	znd->discard_count = 0;
+	for (idx = 0; idx < znd->gz_count; idx++) {
+		struct meta_pg *wpg = &znd->wp[idx];
+		__le16 crc_wp = crc_md_le16(wpg->wp_alloc, Z_CRC_4K);
+		__le16 crc_zf = crc_md_le16(wpg->zf_est, Z_CRC_4K);
+
+		if (znd->bmkeys->wp_crc[idx] == crc_wp &&
+		    znd->bmkeys->zf_crc[idx] == crc_zf) {
+			znd->discard_count += count_stale_blocks(znd, idx, wpg);
+		}
+	}
+
+	znd->z_gc_resv   = le32_to_cpu(znd->bmkeys->gc_resv);
+	znd->z_meta_resv = le32_to_cpu(znd->bmkeys->meta_resv);
+
+out:
+	put_io_vcache(znd, io_vcache);
+	mutex_unlock(&znd->vcio_lock);
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_mapped_addmany(struct zoned *znd, u64 dm_s, u64 lba, u64 c, int fp)
+{
+	int rc = 0;
+	sector_t blk;
+
+	for (blk = 0; blk < c; blk++) {
+		rc = z_mapped_add_one(znd, dm_s + blk, lba + blk, fp);
+		if (rc)
+			goto out;
+	}
+out:
+	return rc;
+}
+
+/**
+ * current_mapping() - Lookup a logical sector address to find the disk LBA
+ * @param znd: ZDM instance
+ * @param addr: Logical LBA of page.
+ * @param gfp: Memory allocation rule
+ *
+ * Return: Disk LBA or 0 if not found.
+ */
+static u64 current_mapping(struct zoned *znd, u64 addr, int gfp)
+{
+	u64 found = 0ul;
+
+	if (addr < znd->data_lba)
+		found = addr;
+	if (!found)
+		found = z_lookup_key_range(znd, addr);
+	if (!found)
+		found = z_lookup_cache(znd, addr);
+	if (!found)
+		found = z_lookup_table(znd, addr, gfp);
+
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static struct map_pg *map_entry_for(struct zoned *znd, u64 lba, int gfp,
+				    struct mpinfo *mpi)
+{
+	int entry;
+	struct map_pg *found = NULL;
+	struct mutex *lock;
+
+	entry = to_table_entry(znd, lba, mpi);
+	lock = mpi->bit_type == IS_LUT ? &znd->mapkey_lock : &znd->ct_lock;
+	MutexLock(lock);
+	if (entry > -1)
+		found = mpi->table[entry];
+	if (!found) {
+		/* if we didn't find one .. create it */
+		found = ZDM_ALLOC(znd, sizeof(*found), KM_20, gfp);
+		if (found) {
+			found->lba = lba;
+			mutex_init(&found->md_lock);
+
+			set_bit(mpi->bit_dir, &found->flags);
+			set_bit(mpi->bit_type, &found->flags);
+
+			mpi->table[entry] = found;
+		} else {
+			Z_ERR(znd, "NO MEM for mapped_t !!!");
+		}
+	}
+	mutex_unlock(lock);
+	return found;
+}
+
+
+/**
+ * put_map_entry() - Decrement refcount of mapped page.
+ * @mapped: mapped page
+ */
+static inline void put_map_entry(struct map_pg *mapped)
+{
+	if (mapped)
+		atomic_dec(&mapped->refcount);
+}
+
+/**
+ * get_map_entry() - Find page of LUT or CRC table map.
+ * @znd: ZDM instance
+ * @lba: Logical LBA of page.
+ * @gfp: Memory allocation rule
+ *
+ * Return: struct map_pg * or NULL on error.
+ *
+ * Page will be loaded from disk it if is not already in core memory.
+ */
+static struct map_pg *get_map_entry(struct zoned *znd, u64 lba, int gfp)
+{
+	struct mpinfo mpi;
+	struct map_pg *mapped;
+
+	mapped = map_entry_for(znd, lba, gfp, &mpi);
+	if (mapped) {
+		atomic_inc(&mapped->refcount);
+		if (!mapped->data.addr) {
+			int rc;
+
+			rc = cache_pg(znd, mapped, gfp, &mpi);
+			if (rc < 0) {
+				znd->meta_result = rc;
+				mapped = NULL;
+			}
+		}
+	} else {
+		Z_ERR(znd, "%s: No table for %" PRIx64, __func__, lba);
+	}
+	return mapped;
+}
+
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int metadata_dirty_fling(struct zoned *znd, u64 dm_s)
+{
+	struct map_pg *Smap = NULL;
+	int is_flung = 0;
+
+	/*
+	 *  not sure why this would ever happen during GC
+	 * nothing in the GC reverse map should point to
+	 * a block *before* a data pool block
+	 */
+	if (dm_s < znd->data_lba)
+		return is_flung;
+
+	Smap = NULL;
+	if (dm_s >= znd->r_base && dm_s < znd->c_end) {
+		Smap = get_map_entry(znd, dm_s, NORMAL);
+		if (!Smap)
+			Z_ERR(znd, "Failed to fling: %" PRIx64, dm_s);
+	}
+	if (Smap) {
+		MutexLock(&Smap->md_lock);
+		Smap->age = jiffies_64;
+		set_bit(IS_DIRTY, &Smap->flags);
+		is_flung = 1;
+		mutex_unlock(&Smap->md_lock);
+
+		if (Smap->lba != dm_s)
+			Z_ERR(znd, "Excess churn? lba %"PRIx64
+				   " [last: %"PRIx64"]", dm_s, Smap->lba);
+		put_map_entry(Smap);
+	}
+	return is_flung;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline void z_do_copy_more(struct gc_state *gc_entry)
+{
+	unsigned long flags;
+	struct zoned *znd = gc_entry->znd;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	set_bit(DO_GC_PREPARE, &gc_entry->gc_flags);
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+}
+
+/**
+ * gc_post() - Add a tLBA and current bLBA origin.
+ * @znd: ZDM Instance
+ * @dm_s: tLBA
+ * @lba: bLBA
+ *
+ * Return: 1 if tLBA is added, 0 if block was stale.
+ *
+ * Stale block checks are performed before tLBA is added.
+ * Add a non-stale block to the list of blocks for moving and
+ * metadata updating.
+ */
+static int gc_post(struct zoned *znd, u64 dm_s, u64 lba)
+{
+	struct map_cache *post = &znd->gc_postmap;
+	int handled = 0;
+
+	if (post->jcount < post->jsize) {
+		u16 idx = ++post->jcount;
+
+		WARN_ON(post->jcount > post->jsize);
+
+		post->jdata[idx].logical = lba48_to_le64(0, dm_s);
+		post->jdata[idx].physical = lba48_to_le64(0, lba);
+		handled = 1;
+	} else {
+		Z_ERR(znd, "*CRIT* post overflow L:%" PRIx64 "-> S:%" PRIx64,
+		      lba, dm_s);
+	}
+	return handled;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_metadata_to_ram(struct gc_state *gc_entry)
+{
+	struct zoned *znd = gc_entry->znd;
+	u64 from_lba = (gc_entry->z_gc << Z_BLKBITS) + znd->md_end;
+	struct map_pg *ORmap;
+	struct map_pg *Smap;
+	struct map_addr ORaddr;
+	struct map_addr Saddr;
+	int count;
+	int rcode = 0;
+
+	/* pull all of the affected struct map_pg and crc pages into memory: */
+	for (count = 0; count < Z_BLKSZ; count++) {
+		__le32 ORencoded;
+		u64 ORlba = from_lba + count;
+
+		map_addr_calc(znd, ORlba, &ORaddr);
+		ORmap = get_map_entry(znd, ORaddr.lut_r, NORMAL);
+		if (ORmap && ORmap->data.addr) {
+			atomic_inc(&ORmap->refcount);
+			MutexLock(&ORmap->md_lock);
+			ORencoded = ORmap->data.addr[ORaddr.pg_idx];
+			mutex_unlock(&ORmap->md_lock);
+			if (ORencoded != MZTEV_UNUSED) {
+				u64 dm_s = map_value(znd, ORencoded);
+				u64 lut_s;
+
+				if (dm_s < znd->nr_blocks) {
+					map_addr_calc(znd, dm_s, &Saddr);
+					lut_s = Saddr.lut_s;
+					Smap = get_map_entry(znd, lut_s,
+							     NORMAL);
+					if (!Smap)
+						rcode = -ENOMEM;
+					put_map_entry(Smap);
+				} else {
+					Z_ERR(znd, "Invalid rmap entry: %x.",
+					      ORencoded);
+				}
+				if (!metadata_dirty_fling(znd, dm_s))
+					gc_post(znd, dm_s, ORlba);
+			}
+			atomic_dec(&ORmap->refcount);
+		}
+		put_map_entry(ORmap);
+	}
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int append_blks(struct zoned *znd, u64 lba,
+		       struct io_4k_block *io_buf, int count)
+{
+	int rcode = 0;
+	int rc;
+	u32 chunk;
+	struct io_4k_block *io_vcache;
+
+	MutexLock(&znd->gc_vcio_lock);
+	io_vcache = get_io_vcache(znd, NORMAL);
+	if (!io_vcache) {
+		Z_ERR(znd, "%s: FAILED to get SYNC CACHE.", __func__);
+		rc = -ENOMEM;
+		goto out;
+	}
+
+	for (chunk = 0; chunk < count; chunk += IO_VCACHE_PAGES) {
+		u32 nblks = count - chunk;
+
+		if (nblks > IO_VCACHE_PAGES)
+			nblks = IO_VCACHE_PAGES;
+
+		rc = read_block(znd->ti, DM_IO_VMA, io_vcache, lba, nblks, 0);
+		if (rc) {
+			Z_ERR(znd, "Reading error ... disable zone: %u",
+				(u32)(lba >> 16));
+			rcode = -EIO;
+			goto out;
+		}
+		memcpy(&io_buf[chunk], io_vcache, nblks * Z_C4K);
+		lba += nblks;
+	}
+out:
+	put_io_vcache(znd, io_vcache);
+	mutex_unlock(&znd->gc_vcio_lock);
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_read(struct gc_state *gc_entry)
+{
+	struct zoned *znd = gc_entry->znd;
+	struct io_4k_block *io_buf = znd->gc_io_buf;
+	struct map_cache *post = &znd->gc_postmap;
+	unsigned long flags;
+	u64 start_lba;
+	int nblks;
+	int rcode = 0;
+	int fill = 0;
+	int jstart;
+	int jentry;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	jstart = gc_entry->r_ptr;
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	if (!jstart)
+		jstart++;
+
+	MutexLock(&post->cached_lock);
+
+	/* A discard may have puched holes in the postmap. re-sync lba */
+	jentry = jstart;
+	while (jentry <= post->jcount && (Z_LOWER48 ==
+			le64_to_lba48(post->jdata[jentry].physical, NULL))) {
+		jentry++;
+	}
+	/* nothing left to move */
+	if (jentry > post->jcount)
+		goto out_finished;
+
+	/* skip over any discarded blocks */
+	if (jstart != jentry)
+		jstart = jentry;
+
+	start_lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+	post->jdata[jentry].physical = lba48_to_le64(GC_READ, start_lba);
+	nblks = 1;
+	jentry++;
+
+	while (jentry <= post->jcount && (nblks+fill) < GC_MAX_STRIPE) {
+		u64 dm_s = le64_to_lba48(post->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+
+		if (Z_LOWER48 == dm_s || Z_LOWER48 == lba) {
+			jentry++;
+			continue;
+		}
+
+		post->jdata[jentry].physical = lba48_to_le64(GC_READ, lba);
+
+		/* if the block is contiguous add it to the read */
+		if (lba == (start_lba + nblks)) {
+			nblks++;
+		} else {
+			if (nblks) {
+				int err;
+
+				err = append_blks(znd, start_lba,
+						  &io_buf[fill], nblks);
+				if (err) {
+					rcode = err;
+					goto out;
+				}
+				fill += nblks;
+			}
+			start_lba = lba;
+			nblks = 1;
+		}
+		jentry++;
+	}
+
+	/* Issue a copy of 'nblks' blocks */
+	if (nblks > 0) {
+		int err;
+
+		err = append_blks(znd, start_lba, &io_buf[fill], nblks);
+		if (err) {
+			rcode = err;
+			goto out;
+		}
+		fill += nblks;
+	}
+
+out_finished:
+	Z_DBG(znd, "Read %d blocks from %d", fill, gc_entry->r_ptr);
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	gc_entry->nblks = fill;
+	gc_entry->r_ptr = jentry;
+	if (fill > 0)
+		set_bit(DO_GC_WRITE, &gc_entry->gc_flags);
+	else
+		set_bit(DO_GC_COMPLETE, &gc_entry->gc_flags);
+
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+out:
+	mutex_unlock(&post->cached_lock);
+
+	return rcode;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_write(struct gc_state *gc_entry, u32 stream_id)
+{
+	struct zoned *znd = gc_entry->znd;
+	struct dm_target *ti = znd->ti;
+	struct io_4k_block *io_buf = znd->gc_io_buf;
+	struct map_cache *post = &znd->gc_postmap;
+	unsigned long flags;
+	u32 aq_flags = Z_AQ_GC | Z_AQ_STREAM_ID | stream_id;
+	u64 lba;
+	u32 nblks;
+	u32 out = 0;
+	int err = 0;
+	int jentry;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	jentry = gc_entry->w_ptr;
+	nblks = gc_entry->nblks;
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	if (!jentry)
+		jentry++;
+
+	MutexLock(&post->cached_lock);
+	while (nblks > 0) {
+		u32 nfound = 0;
+		u32 added = 0;
+
+		/*
+		 * When lba is zero blocks were not allocated.
+		 * Retry with the smaller request
+		 */
+		lba = z_acquire(znd, aq_flags, nblks, &nfound);
+		if (!lba) {
+			if (nfound) {
+				u32 avail = nfound;
+
+				nfound = 0;
+				lba = z_acquire(znd, aq_flags, avail, &nfound);
+			}
+		}
+
+		if (!lba) {
+			err = -ENOSPC;
+			goto out;
+		}
+
+		err = write_block(ti, DM_IO_VMA, &io_buf[out], lba, nfound, 0);
+		if (err) {
+			Z_ERR(znd, "Write %d blocks to %"PRIx64". ERROR: %d",
+			      nfound, lba, err);
+			goto out;
+		}
+		out += nfound;
+
+		while ((jentry <= post->jcount) && (added < nfound)) {
+			u16 rflg;
+			u64 orig = le64_to_lba48(
+					post->jdata[jentry].physical, &rflg);
+			u64 dm_s = le64_to_lba48(
+					post->jdata[jentry].logical, NULL);
+
+			if ((Z_LOWER48 == dm_s || Z_LOWER48 == orig)) {
+				jentry++;
+
+				if (rflg & GC_READ) {
+					Z_ERR(znd, "ERROR: %" PRIx64
+					      " read and not written %" PRIx64,
+					      orig, dm_s);
+					lba++;
+					added++;
+				}
+				continue;
+			}
+			rflg &= ~GC_READ;
+			post->jdata[jentry].physical = lba48_to_le64(rflg, lba);
+			lba++;
+			added++;
+			jentry++;
+		}
+		nblks -= nfound;
+	}
+	Z_DBG(znd, "Write %d blocks from %d", gc_entry->nblks, gc_entry->w_ptr);
+	set_bit(DO_GC_META, &gc_entry->gc_flags);
+
+out:
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	gc_entry->nblks = 0;
+	gc_entry->w_ptr = jentry;
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+	mutex_unlock(&post->cached_lock);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_finalize(struct gc_state *gc_entry)
+{
+	int err = 0;
+	struct zoned *znd = gc_entry->znd;
+	struct map_cache *post = &znd->gc_postmap;
+	int jentry;
+
+	MutexLock(&post->cached_lock);
+	for (jentry = post->jcount; jentry > 0; jentry--) {
+		u64 dm_s = le64_to_lba48(post->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+
+		if (dm_s != Z_LOWER48 || lba != Z_LOWER48) {
+			Z_ERR(znd, "GC: Failed to move %" PRIx64
+			      " from %"PRIx64" [%d]", dm_s, lba, jentry);
+			err = -EIO;
+		}
+	}
+	mutex_unlock(&post->cached_lock);
+	post->jcount = jentry;
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void clear_gc_target_flag(struct zoned *znd)
+{
+	int z_id;
+
+	for (z_id = 0; z_id < znd->data_zones; z_id++) {
+		u32 gzno  = z_id >> GZ_BITS;
+		u32 gzoff = z_id & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		u32 wp;
+
+		MutexLock(&wpg->wplck);
+		wp = le32_to_cpu(wpg->wp_alloc[gzoff]);
+		if (wp & Z_WP_GC_TARGET) {
+			wp &= ~Z_WP_GC_TARGET;
+			wpg->wp_alloc[gzoff] = cpu_to_le32(wp);
+		}
+		set_bit(IS_DIRTY, &wpg->flags);
+		mutex_unlock(&wpg->wplck);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int z_zone_gc_metadata_update(struct gc_state *gc_entry)
+{
+	struct zoned *znd = gc_entry->znd;
+	struct map_cache *post = &znd->gc_postmap;
+	u32 used = post->jcount;
+	int err = 0;
+	int jentry;
+
+	for (jentry = post->jcount; jentry > 0; jentry--) {
+		int discard = 0;
+		int mapping = 0;
+		struct map_pg *mapped = NULL;
+		u64 dm_s = le64_to_lba48(post->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(post->jdata[jentry].physical, NULL);
+
+		if ((znd->r_base <= dm_s) && dm_s < (znd->r_base + Z_BLKSZ)) {
+			u64 off = dm_s - znd->r_base;
+
+			mapped = znd->rev_tm[off];
+			mapping = 1;
+		} else if ((znd->s_base <= dm_s) &&
+			   (dm_s < (znd->s_base + Z_BLKSZ))) {
+			u64 off = dm_s - znd->s_base;
+
+			mapped = znd->fwd_tm[off];
+			mapping = 1;
+		}
+
+		if (mapping && !mapped)
+			Z_ERR(znd, "MD: dm_s: %" PRIx64 " -> lba: %" PRIx64
+				   " no mapping in ram.", dm_s, lba);
+
+		if (mapped) {
+			u32 in_z;
+
+			atomic_inc(&mapped->refcount);
+			MutexLock(&mapped->md_lock);
+			in_z = _calc_zone(znd, mapped->last_write);
+			if (in_z != gc_entry->z_gc) {
+				Z_ERR(znd, "MD: %" PRIx64
+				      " Discarded - %" PRIx64
+				      " already flown to: %x",
+				      dm_s, mapped->last_write, in_z);
+				discard = 1;
+			} else if (mapped->data.addr &&
+				   test_bit(IS_DIRTY, &mapped->flags)) {
+				Z_ERR(znd,
+				      "MD: %" PRIx64 " Discarded - %"PRIx64
+				      " is in-flight",
+				      dm_s, mapped->last_write);
+				discard = 2;
+			}
+			if (!discard)
+				mapped->last_write = lba;
+			mutex_unlock(&mapped->md_lock);
+			atomic_dec(&mapped->refcount);
+		}
+
+		MutexLock(&post->cached_lock);
+		if (discard == 1) {
+			Z_ERR(znd, "Dropped: %" PRIx64 " ->  %"PRIx64,
+			      le64_to_cpu(post->jdata[jentry].logical),
+			      le64_to_cpu(post->jdata[jentry].physical));
+
+			post->jdata[jentry].logical = MC_INVALID;
+			post->jdata[jentry].physical = MC_INVALID;
+		}
+		if (post->jdata[jentry].logical  == MC_INVALID &&
+		    post->jdata[jentry].physical == MC_INVALID) {
+			used--;
+		} else if (lba) {
+			_inc_zone_used(znd, lba);
+		}
+		mutex_unlock(&post->cached_lock);
+	}
+	err = move_to_map_tables(znd, post);
+	if (err)
+		Z_ERR(znd, "Move to tables post GC failure");
+
+	clear_gc_target_flag(znd);
+
+	return err;
+}
+
+/**
+ * _blkalloc() - Attempt to reserve blocks at z_at in ZDM znd
+ * @znd:	 ZDM instance.
+ * @z_at:	 Zone to write data to
+ * @flags:	 Acquisition type.
+ * @nblks:	 Number of blocks desired.
+ * @nfound:	 Number of blocks allocated or available.
+ *
+ * Attempt allocation of @nblks within fron the current WP of z_at
+ * When nblks are not available 0 is returned and @nfound is the
+ * contains the number of blocks *available* but not *allocated*.
+ * When nblks are available the starting LBA in 4k space is returned and
+ * nblks are allocated *allocated* and *nfound is the number of blocks
+ * remaining in zone z_at from the LBA returned.
+ *
+ * Return: LBA if request is met, otherwise 0. nfound will contain the
+ *         available blocks remaining.
+ */
+static sector_t _blkalloc(struct zoned *znd, u32 z_at, u32 flags,
+			  u32 nblks, u32 *nfound)
+{
+	sector_t found = 0;
+	u32 avail = 0;
+	int do_open_zone = 0;
+	u32 gzno  = z_at >> GZ_BITS;
+	u32 gzoff = z_at & GZ_MMSK;
+	struct meta_pg *wpg = &znd->wp[gzno];
+	u32 wp;
+	u32 wptr;
+	u32 gc_tflg;
+
+	if (gzno >= znd->gz_count || z_at >= znd->data_zones) {
+		Z_ERR(znd, "Invalid zone for allocation: %u", z_at);
+		dump_stack();
+		return 0ul;
+	}
+
+	MutexLock(&wpg->wplck);
+	wp      = le32_to_cpu(wpg->wp_alloc[gzoff]);
+	gc_tflg = wp & (Z_WP_GC_TARGET|Z_WP_NON_SEQ);
+	wptr    = wp & ~(Z_WP_GC_TARGET|Z_WP_NON_SEQ);
+	if (wptr < Z_BLKSZ)
+		avail = Z_BLKSZ - wptr;
+
+#if 0 /* DEBUG START: Testing zm_write_pages() */
+	if (avail > 7)
+		avail = 7;
+#endif /* DEBUG END: Testing zm_write_pages() */
+
+	*nfound = avail;
+	if (nblks <= avail) {
+		u64 lba = ((u64)z_at << Z_BLKBITS) + znd->data_lba;
+		u32 zf_est = le32_to_cpu(wpg->zf_est[gzoff]) & Z_WP_VALUE_MASK;
+
+		found = lba + wptr;
+		*nfound = nblks;
+		if (wptr == 0)
+			do_open_zone = 1;
+
+		wptr += nblks;
+		zf_est -= nblks;
+		if (wptr == Z_BLKSZ)
+			znd->discard_count += zf_est;
+
+		wptr |= gc_tflg;
+		if (flags & Z_AQ_GC)
+			wptr |= Z_WP_GC_TARGET;
+
+		if (flags & Z_AQ_STREAM_ID)
+			zf_est |= (flags & Z_AQ_STREAM_MASK) << 24;
+		else
+			zf_est |= le32_to_cpu(wpg->zf_est[gzoff])
+				& Z_WP_STREAM_MASK;
+
+		wpg->wp_alloc[gzoff] = cpu_to_le32(wptr);
+		wpg->zf_est[gzoff] = cpu_to_le32(zf_est);
+		set_bit(IS_DIRTY, &wpg->flags);
+	}
+	mutex_unlock(&wpg->wplck);
+
+	if (do_open_zone)
+		dmz_open_zone(znd, z_at);
+
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u16 _gc_tag = 1;
+
+static void dbg_queued(struct zoned *znd, u32 z_gc, u16 tag)
+{
+	u32 gzno  = z_gc >> GZ_BITS;
+	u32 gzoff = z_gc & GZ_MMSK;
+	struct meta_pg *wpg = &znd->wp[gzno];
+	u32 zf_est = le32_to_cpu(wpg->zf_est[gzoff]) & Z_WP_VALUE_MASK;
+	u32 wptr = le32_to_cpu(wpg->wp_alloc[gzoff]);
+
+	Z_ERR(znd, "Queue GC: Z# %x, wp: %x, free %x - tag %u",
+	      z_gc, wptr, zf_est, tag);
+}
+
+/**
+ * z_zone_compact_queue() - Queue zone compaction.
+ * @znd: ZDM instance
+ * @z_gc: Zone to queue.
+ * @delay: Delay queue metric
+ * @gfp: Allocation scheme.
+ */
+static int z_zone_compact_queue(struct zoned *znd, u32 z_gc, int delay, int gfp)
+{
+	unsigned long flags;
+	int do_queue = 0;
+	int err = 0;
+	struct gc_state *gc_entry =
+		ZDM_ALLOC(znd, sizeof(*gc_entry), KM_16, gfp);
+
+	if (!gc_entry) {
+		Z_ERR(znd, "No Memory for compact!!");
+		return -ENOMEM;
+	}
+	gc_entry->znd = znd;
+	gc_entry->z_gc = z_gc;
+	gc_entry->tag = _gc_tag++;
+	set_bit(DO_GC_NEW, &gc_entry->gc_flags);
+	znd->gc_backlog++;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	if (!znd->gc_active) {
+		znd->gc_active = gc_entry;
+		do_queue = 1;
+	}
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	if (do_queue) {
+		unsigned long tval = msecs_to_jiffies(delay);
+
+		queue_delayed_work(znd->gc_wq, &znd->gc_work, tval);
+		dbg_queued(znd, z_gc, gc_entry->tag);
+	} else {
+		Z_ERR(znd, "Queue GC: Z# %x ** fail: active", z_gc);
+		ZDM_FREE(znd, gc_entry, sizeof(*gc_entry), KM_16);
+		znd->gc_backlog--;
+	}
+
+	return err;
+}
+
+/**
+ * zone_zfest() - Queue zone compaction.
+ * @znd: ZDM instance
+ * @z_id: Zone to queue.
+ */
+static u32 zone_zfest(struct zoned *znd, u32 z_id)
+{
+	u32 gzno  = z_id >> GZ_BITS;
+	u32 gzoff = z_id & GZ_MMSK;
+	struct meta_pg *wpg = &znd->wp[gzno];
+
+	return le32_to_cpu(wpg->zf_est[gzoff]) & Z_WP_VALUE_MASK;
+}
+
+
+/**
+ * gc_compact_check() - Called periodically to initiate GC
+ *
+ * @znd: ZDM instance
+ * @bin: Bin with stale zones to scan for GC
+ * @delay: Metric for delay queuing.
+ * @gfp: Default memory allocation scheme.
+ *
+ */
+static int gc_compact_check(struct zoned *znd, int bin, int delay, int gfp)
+{
+#define GC_NFOUND	(~0u)
+	unsigned long flags;
+	int queued = 0;
+	u16 gc_tag = 0;
+	u32 top_roi = GC_NFOUND;
+	u32 stale = 0;
+	u32 z_gc = bin * znd->stale.binsz;
+	u32 s_end = z_gc + znd->stale.binsz;
+
+	if (znd->meta_result)
+		goto out;
+
+	if (test_bit(ZF_FREEZE, &znd->flags)) {
+		Z_ERR(znd, "Is frozen -- GC paused.");
+		goto out;
+	}
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	if (znd->gc_active) {
+		queued = 1;
+		gc_tag = znd->gc_active->tag;
+	}
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	if (queued) {
+		Z_ERR(znd, "GC is active: %u", gc_tag);
+		goto out;
+	}
+
+	if (s_end > znd->data_zones)
+		s_end = znd->data_zones;
+
+	/* scan for most stale zone in STREAM [top_roi] */
+	for (; z_gc < s_end; z_gc++) {
+		u32 gzno  = z_gc >> GZ_BITS;
+		u32 gzoff = z_gc & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		u32 wp_v = le32_to_cpu(wpg->wp_alloc[gzoff]);
+		u32 nfree = le32_to_cpu(wpg->zf_est[gzoff]) & Z_WP_VALUE_MASK;
+		u32 wp_f = wp_v & Z_WP_FLAGS_MASK;
+
+		wp_v &= Z_WP_VALUE_MASK;
+		if (wp_v == 0)
+			continue;
+		if ((wp_f & Z_WP_GC_PENDING) != 0)
+			continue;
+
+		if (wp_v == Z_BLKSZ) {
+			stale += nfree;
+			if ((wp_f & Z_WP_GC_BITS) == Z_WP_GC_READY) {
+				if (top_roi == GC_NFOUND)
+					top_roi = z_gc;
+				else if (nfree > zone_zfest(znd, top_roi))
+					top_roi = z_gc;
+			}
+		}
+	}
+
+	/* determine the cut-off for GC based on MZ overall staleness */
+	if (top_roi != GC_NFOUND) {
+		u32 state_metric = GC_PRIO_DEFAULT;
+		u32 n_filled = znd->data_zones - znd->z_gc_free;
+		u32 n_empty = znd->data_zones - n_filled;
+		int pctfree = n_empty * 100 / znd->data_zones;
+
+		/*
+		 * -> at less than 5 zones free switch to critical
+		 * -> at less than 5% zones free switch to HIGH
+		 * -> at less than 25% free switch to LOW
+		 * -> high level is 'cherry picking' near empty zones
+		 */
+		if (znd->z_gc_free < 5)
+			state_metric = GC_PRIO_CRIT;
+		else if (pctfree < 5)
+			state_metric = GC_PRIO_HIGH;
+		else if (pctfree < 25)
+			state_metric = GC_PRIO_LOW;
+
+		if (zone_zfest(znd, top_roi) > state_metric)
+			if (z_zone_compact_queue(znd, top_roi, delay * 5, gfp))
+				queued = 1;
+	}
+out:
+	return queued;
+}
+
+/**
+ * z_zone_gc_compact() - Primary compaction worker.
+ * @gc_entry: GC State
+ */
+static int z_zone_gc_compact(struct gc_state *gc_entry)
+{
+	unsigned long flags;
+	int err = 0;
+	int do_meta_flush = 0;
+	struct zoned *znd = gc_entry->znd;
+	u32 z_gc = gc_entry->z_gc;
+	u32 gzno  = z_gc >> GZ_BITS;
+	u32 gzoff = z_gc & GZ_MMSK;
+	struct meta_pg *wpg = &znd->wp[gzno];
+
+	znd->age = jiffies_64;
+	if (test_and_clear_bit(DO_GC_NEW, &gc_entry->gc_flags)) {
+		u32 wp;
+
+		err = z_flush_bdev(znd);
+		if (err) {
+			gc_entry->result = err;
+			goto out;
+		}
+		MutexLock(&wpg->wplck);
+		wp = le32_to_cpu(wpg->wp_alloc[gzoff]);
+		wp |= Z_WP_GC_FULL;
+		wpg->wp_alloc[gzoff] = cpu_to_le32(wp);
+		set_bit(IS_DIRTY, &wpg->flags);
+		mutex_unlock(&wpg->wplck);
+
+		MutexLock(&znd->mz_io_mutex);
+		err = _cached_to_tables(znd, gc_entry->z_gc);
+		set_bit(DO_GC_NO_PURGE, &znd->flags);
+		mutex_unlock(&znd->mz_io_mutex);
+		if (err) {
+			Z_ERR(znd, "Failed to purge journal: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+
+		if (znd->gc_postmap.jcount > 0) {
+			Z_ERR(znd, "*** Unexpected data in postmap!!");
+			znd->gc_postmap.jcount = 0;
+		}
+
+		MutexLock(&znd->mz_io_mutex);
+		err = z_zone_gc_metadata_to_ram(gc_entry);
+		mutex_unlock(&znd->mz_io_mutex);
+		if (err) {
+			Z_ERR(znd,
+			      "Pre-load metadata to memory failed!! %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+		set_bit(DO_GC_PREPARE, &gc_entry->gc_flags);
+
+		if (znd->gc_throttle.counter == 0)
+			return -EAGAIN;
+	}
+
+next_in_queue:
+	znd->age = jiffies_64;
+	if (test_and_clear_bit(DO_GC_PREPARE, &gc_entry->gc_flags)) {
+		MutexLock(&znd->mz_io_mutex);
+		err = z_zone_gc_read(gc_entry);
+		mutex_unlock(&znd->mz_io_mutex);
+		if (err < 0) {
+			Z_ERR(znd, "z_zone_gc_chunk issue failure: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+		if (znd->gc_throttle.counter == 0)
+			return -EAGAIN;
+	}
+
+	if (test_and_clear_bit(DO_GC_WRITE, &gc_entry->gc_flags)) {
+		u32 sid = le32_to_cpu(wpg->zf_est[gzoff]) & Z_WP_STREAM_MASK;
+
+		MutexLock(&znd->mz_io_mutex);
+		err = z_zone_gc_write(gc_entry, sid >> 24);
+		mutex_unlock(&znd->mz_io_mutex);
+		if (err) {
+			Z_ERR(znd, "z_zone_gc_write issue failure: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+		if (znd->gc_throttle.counter == 0)
+			return -EAGAIN;
+	}
+
+	if (test_and_clear_bit(DO_GC_META, &gc_entry->gc_flags)) {
+		z_do_copy_more(gc_entry);
+		goto next_in_queue;
+	}
+
+	znd->age = jiffies_64;
+	if (test_and_clear_bit(DO_GC_COMPLETE, &gc_entry->gc_flags)) {
+		u32 non_seq;
+		u32 reclaimed;
+
+		MutexLock(&znd->mz_io_mutex);
+		err = z_zone_gc_metadata_update(gc_entry);
+		gc_entry->result = err;
+		mutex_unlock(&znd->mz_io_mutex);
+		if (err) {
+			Z_ERR(znd, "Metadata error ... disable zone: %u",
+			      gc_entry->z_gc);
+		}
+		err = gc_finalize(gc_entry);
+		if (err) {
+			Z_ERR(znd, "GC: Failed to finalize: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+
+		gc_verify_cache(znd, gc_entry->z_gc);
+
+		MutexLock(&znd->mz_io_mutex);
+		err = _cached_to_tables(znd, gc_entry->z_gc);
+		mutex_unlock(&znd->mz_io_mutex);
+		if (err) {
+			Z_ERR(znd, "Failed to purge journal: %d", err);
+			gc_entry->result = err;
+			goto out;
+		}
+
+		err = z_flush_bdev(znd);
+		if (err) {
+			gc_entry->result = err;
+			goto out;
+		}
+
+		/* Release the zones for writing */
+		dmz_reset_wp(znd, gc_entry->z_gc);
+
+		MutexLock(&wpg->wplck);
+		non_seq = le32_to_cpu(wpg->wp_alloc[gzoff]) & Z_WP_NON_SEQ;
+		reclaimed = le32_to_cpu(wpg->zf_est[gzoff]) & Z_WP_VALUE_MASK;
+		wpg->wp_alloc[gzoff] = cpu_to_le32(non_seq);
+		wpg->wp_used[gzoff] = cpu_to_le32(0u);
+		wpg->zf_est[gzoff] = cpu_to_le32(Z_BLKSZ);
+		znd->discard_count -= reclaimed;
+		znd->z_gc_free++;
+		if (znd->z_gc_resv & Z_WP_GC_ACTIVE)
+			znd->z_gc_resv = gc_entry->z_gc;
+		else if (znd->z_meta_resv & Z_WP_GC_ACTIVE)
+			znd->z_meta_resv = gc_entry->z_gc;
+		set_bit(IS_DIRTY, &wpg->flags);
+		mutex_unlock(&wpg->wplck);
+
+		Z_ERR(znd, "GC %d: z#0x%x, wp:%08x, free:%lx finished.",
+		      gc_entry->tag, gc_entry->z_gc, non_seq, Z_BLKSZ);
+
+		spin_lock_irqsave(&znd->gc_lock, flags);
+		znd->gc_backlog--;
+		znd->gc_active = NULL;
+		spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+		ZDM_FREE(znd, gc_entry, sizeof(*gc_entry), KM_16);
+
+		set_bit(DO_JOURNAL_MOVE, &znd->flags);
+		set_bit(DO_MEMPOOL, &znd->flags);
+		set_bit(DO_SYNC, &znd->flags);
+		do_meta_flush = 1;
+	}
+out:
+	clear_bit(DO_GC_NO_PURGE, &znd->flags);
+	if (do_meta_flush) {
+		if (!work_pending(&znd->meta_work))
+			queue_work(znd->meta_wq, &znd->meta_work);
+	}
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void gc_work_task(struct work_struct *work)
+{
+	struct gc_state *gc_entry = NULL;
+	unsigned long flags;
+	struct zoned *znd;
+	int err;
+
+	if (!work)
+		return;
+
+	znd = container_of(to_delayed_work(work), struct zoned, gc_work);
+	if (!znd)
+		return;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	if (znd->gc_active)
+		gc_entry = znd->gc_active;
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	if (!gc_entry) {
+		Z_ERR(znd, "ERROR: gc_active not set!");
+		return;
+	}
+
+	err = z_zone_gc_compact(gc_entry);
+	if (-EAGAIN == err) {
+		unsigned long tval = msecs_to_jiffies(10);
+
+		queue_delayed_work(znd->gc_wq, &znd->gc_work, tval);
+	} else {
+		on_timeout_activity(znd, 0, 10);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static inline int is_reserved(struct zoned *znd, const u32 z_pref)
+{
+	const u32 gc   = znd->z_gc_resv   & Z_WP_VALUE_MASK;
+	const u32 meta = znd->z_meta_resv & Z_WP_VALUE_MASK;
+
+	return (gc == z_pref || meta == z_pref) ? 1 : 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_can_cherrypick(struct zoned *znd, u32 bin, int delay, int gfp)
+{
+	u32 z_id = bin * znd->stale.binsz;
+	u32 s_end = z_id + znd->stale.binsz;
+
+	if (s_end > znd->data_zones)
+		s_end = znd->data_zones;
+
+	for (; z_id < s_end; z_id++) {
+		u32 gzno  = z_id >> GZ_BITS;
+		u32 gzoff = z_id & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		u32 wp = le32_to_cpu(wpg->wp_alloc[gzoff]);
+		u32 nfree = le32_to_cpu(wpg->zf_est[gzoff]) & Z_WP_VALUE_MASK;
+
+		if (((wp & Z_WP_GC_BITS) == Z_WP_GC_READY) &&
+		    ((wp & Z_WP_VALUE_MASK) == Z_BLKSZ) &&
+		    (nfree == Z_BLKSZ)) {
+			if (z_zone_compact_queue(znd, z_id, delay, gfp))
+				return 1;
+		}
+	}
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_queue_with_delay(struct zoned *znd, int delay, int gfp)
+{
+	int gc_idle = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&znd->gc_lock, flags);
+	gc_idle = !znd->gc_active;
+	spin_unlock_irqrestore(&znd->gc_lock, flags);
+
+	if (gc_idle) {
+		int bin = 0;
+		int ratio = 0;
+		u32 iter;
+
+		/* Find highest ratio stream */
+		for (iter = 0; iter < znd->stale.count; iter++)
+			if (znd->stale.bins[iter] > ratio)
+				ratio = znd->stale.bins[iter], bin = iter;
+
+		/* Cherrypick a zone in the stream */
+		if (gc_idle && gc_can_cherrypick(znd, bin, delay, gfp))
+			gc_idle = 0;
+
+		/* Otherwise cherrypick *something* */
+		for (iter = 0; iter < znd->stale.count; iter++)
+			if (gc_idle && (bin != iter) &&
+			    gc_can_cherrypick(znd, iter, delay, gfp))
+				gc_idle = 0;
+
+		/* Otherwise compact a zone in the stream */
+		if (gc_idle && gc_compact_check(znd, bin, delay, gfp))
+			gc_idle = 0;
+
+#if 0
+		/* Otherwise compact *something* */
+		for (iter = 0; iter < znd->stale.count; iter++)
+			if (gc_idle && gc_compact_check(znd, iter, delay, gfp))
+				gc_idle = 0;
+#endif
+
+	}
+	return !gc_idle;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int gc_immediate(struct zoned *znd, int gfp)
+{
+	const int delay = 0;
+	int can_retry = 0;
+	int queued;
+	int is_locked;
+
+	atomic_inc(&znd->gc_throttle);
+
+	if (znd->gc_throttle.counter > 1)
+		goto out;
+
+	is_locked = mutex_is_locked(&znd->mz_io_mutex);
+	if (is_locked)
+		mutex_unlock(&znd->mz_io_mutex);
+	flush_delayed_work(&znd->gc_work);
+	if (is_locked)
+		MutexLock(&znd->mz_io_mutex);
+
+	queued = gc_queue_with_delay(znd, delay, gfp);
+	if (!queued) {
+		Z_ERR(znd, " ... GC immediate .. failed to queue GC!!.");
+		dump_stack();
+		goto out;
+	}
+
+	if (is_locked)
+		mutex_unlock(&znd->mz_io_mutex);
+	can_retry = flush_delayed_work(&znd->gc_work);
+	if (is_locked)
+		MutexLock(&znd->mz_io_mutex);
+
+out:
+	atomic_dec(&znd->gc_throttle);
+
+	return can_retry;
+}
+
+/**
+ * update_stale_ratio() - Update the stale ratio for the finished bin.
+ * @znd: ZDM instance
+ * @zone: Zone that needs update.
+ */
+static void update_stale_ratio(struct zoned *znd, u32 zone)
+{
+	u64 total_stale = 0;
+	u64 free_zones = 1;
+	u32 bin = zone / znd->stale.binsz;
+	u32 z_id = bin * znd->stale.binsz;
+	u32 s_end = z_id + znd->stale.binsz;
+
+	if (s_end > znd->data_zones)
+		s_end = znd->data_zones;
+
+	for (; z_id < s_end; z_id++) {
+		u32 gzno  = z_id >> GZ_BITS;
+		u32 gzoff = z_id & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		u32 stale = le32_to_cpu(wpg->zf_est[gzoff]) & Z_WP_VALUE_MASK;
+		u32 wp = le32_to_cpu(wpg->wp_alloc[gzoff]) & Z_WP_VALUE_MASK;
+
+		if (wp == Z_BLKSZ)
+			total_stale += stale;
+		else
+			free_zones++;
+	}
+
+	total_stale /= free_zones;
+	znd->stale.bins[bin] = (total_stale > ~0u) ? ~0u : total_stale;
+}
+
+static inline void set_current(struct zoned *znd, u32 flags, u32 zone)
+{
+	if (flags & Z_AQ_STREAM_ID) {
+		u32 stream_id = flags & Z_AQ_STREAM_MASK;
+
+		znd->bmkeys->stream[stream_id] = cpu_to_le32(zone);
+	}
+	znd->z_current = zone;
+	znd->z_gc_free--;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static u32 next_open_zone(struct zoned *znd, u32 z_at)
+{
+	u32 zone = ~0u;
+	u32 z_id;
+
+	if (znd->data_zones < z_at)
+		z_at = znd->data_zones;
+
+	/* scan higher lba zones */
+	for (z_id = z_at; z_id < znd->data_zones; z_id++) {
+		u32 gzno  = z_id >> GZ_BITS;
+		u32 gzoff = z_id & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		u32 wp = le32_to_cpu(wpg->wp_alloc[gzoff]);
+
+		if ((wp & Z_WP_VALUE_MASK) == 0) {
+			u32 check = gzno << GZ_BITS | gzoff;
+
+			if (!is_reserved(znd, check)) {
+				zone = check;
+				goto out;
+			}
+		}
+	}
+
+	/* scan lower lba zones */
+	for (z_id = 0; z_id < z_at; z_id++) {
+		u32 gzno  = z_id >> GZ_BITS;
+		u32 gzoff = z_id & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		u32 wp = le32_to_cpu(wpg->wp_alloc[gzoff]);
+
+		if ((wp & Z_WP_VALUE_MASK) == 0) {
+			u32 check = gzno << GZ_BITS | gzoff;
+
+			if (!is_reserved(znd, check)) {
+				zone = check;
+				goto out;
+			}
+		}
+	}
+
+	/* No empty zones .. start co-mingling streams */
+	for (z_id = 0; z_id < znd->data_zones; z_id++) {
+		u32 gzno  = z_id >> GZ_BITS;
+		u32 gzoff = z_id & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		u32 wp = le32_to_cpu(wpg->wp_alloc[gzoff]);
+
+		if ((wp & Z_WP_VALUE_MASK) < Z_BLKSZ) {
+			u32 check = gzno << GZ_BITS | gzoff;
+
+			if (!is_reserved(znd, check)) {
+				zone = check;
+				goto out;
+			}
+		}
+	}
+
+out:
+	return zone;
+}
+
+
+static u64 z_acquire(struct zoned *znd, u32 flags, u32 nblks, u32 *nfound)
+{
+	sector_t found = 0;
+	u32 z_pref = znd->z_current;
+	u32 stream_id = 0;
+	u32 z_find;
+	int gfp = flags & Z_AQ_NORMAL ? CRIT : NORMAL;
+
+	if (flags & Z_AQ_STREAM_ID) {
+		stream_id = flags & Z_AQ_STREAM_MASK;
+		z_pref = le32_to_cpu(znd->bmkeys->stream[stream_id]);
+	}
+	if (z_pref >= znd->data_zones) {
+		z_pref = next_open_zone(znd, znd->z_current);
+		if (z_pref < znd->data_zones)
+			set_current(znd, flags, z_pref);
+	}
+
+	if (z_pref < znd->data_zones) {
+		found = _blkalloc(znd, z_pref, flags, nblks, nfound);
+		if (found || *nfound)
+			goto out;
+
+		/* no space left in zone .. explicitly close it */
+		dmz_close_zone(znd, z_pref);
+		update_stale_ratio(znd, z_pref);
+	}
+
+	if (znd->z_gc_free < 5) {
+		Z_ERR(znd, "... alloc - gc low on free space.");
+		gc_immediate(znd, gfp);
+	}
+
+retry:
+	z_find = next_open_zone(znd, znd->z_current);
+	if (z_find < znd->data_zones) {
+		found = _blkalloc(znd, z_find, flags, nblks, nfound);
+		if (found || *nfound) {
+			set_current(znd, flags, z_find);
+			goto out;
+		}
+	}
+
+	if (flags & Z_AQ_GC) {
+		u32 gresv = znd->z_gc_resv & Z_WP_VALUE_MASK;
+
+		Z_ERR(znd, "Using GC Reserve (%u)", gresv);
+		found = _blkalloc(znd, gresv, flags, nblks, nfound);
+		znd->z_gc_resv |= Z_WP_GC_ACTIVE;
+	}
+
+	if (flags & Z_AQ_META) {
+		int can_retry = gc_immediate(znd, gfp);
+		u32 mresv = znd->z_meta_resv & Z_WP_VALUE_MASK;
+
+		if (can_retry)
+			goto retry;
+
+		Z_ERR(znd, "Using META Reserve (%u)", znd->z_meta_resv);
+		found = _blkalloc(znd, mresv, flags, nblks, nfound);
+	}
+
+out:
+	if (!found && (*nfound == 0)) {
+		if (gc_immediate(znd, gfp))
+			goto retry;
+
+		Z_ERR(znd, "%s: -> Out of space.", __func__);
+	}
+	return found;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static void do_free_mapped(struct zoned *znd, struct map_pg *expg)
+{
+	struct mpinfo mpi;
+	int entry;
+	struct map_pg *found = NULL;
+	struct mutex *lock;
+
+	entry = to_table_entry(znd, expg->lba, &mpi);
+	lock = mpi.bit_type == IS_LUT ? &znd->mapkey_lock : &znd->ct_lock;
+	MutexLock(lock);
+	if (entry > -1)
+		found = mpi.table[entry];
+	if (found) {
+		int drop = 0;
+
+		if (!mutex_trylock(&expg->md_lock))
+			goto out;
+
+		if (expg->refcount.counter == 0) {
+			list_del(&expg->inpool);
+			drop = 1;
+			if (expg->data.addr) {
+				ZDM_FREE(znd, expg->data.addr, Z_C4K, PG_27);
+				znd->incore_count--;
+			}
+		}
+		mutex_unlock(&expg->md_lock);
+
+		if (drop) {
+			ZDM_FREE(znd, found, sizeof(*found), KM_20);
+			mpi.table[entry] = NULL;
+		}
+	} else {
+		Z_ERR(znd, " *** purge: %"PRIx64 " @ %d ????",
+		      expg->lba, entry);
+	}
+
+out:
+	mutex_unlock(lock);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int compare_lba(const void *x1, const void *x2)
+{
+	const struct map_pg *v1 = *(const struct map_pg **)x1;
+	const struct map_pg *v2 = *(const struct map_pg **)x2;
+	int cmp = (v1->lba < v2->lba) ? -1 : ((v1->lba > v2->lba) ? 1 : 0);
+
+	return cmp;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int is_old_and_dirty(struct map_pg *expg, int bit_type)
+{
+	return (is_expired(expg->age) &&
+		test_bit(bit_type, &expg->flags) &&
+		test_bit(IS_DIRTY, &expg->flags));
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int is_dirty(struct map_pg *expg, int bit_type)
+{
+	return (test_bit(bit_type, &expg->flags) &&
+		test_bit(IS_DIRTY, &expg->flags));
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int is_old_del(struct map_pg *expg, int bit_type)
+{
+	int inc = 0;
+
+	if (!test_bit(IS_DIRTY, &expg->flags) && is_expired(expg->age))
+		if (expg->refcount.counter == 1)
+			inc = 1;
+
+	(void)bit_type;
+
+	return inc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int _pool_fill(struct zoned *znd,
+		      struct map_pg **stale,
+		      int count,
+		      int (*match)(struct map_pg *, int),
+		      int bit_type)
+{
+	int stale_count = 0;
+	struct map_pg *expg;
+
+	MutexLock(&znd->map_pool_lock);
+	stale_count = 0;
+	expg = _pool_last(znd);
+	while (expg && stale_count < count) {
+		struct map_pg *aux = _pool_prev(znd, expg);
+
+		if (match(expg, bit_type))
+			stale[stale_count++] = expg;
+
+		atomic_dec(&expg->refcount);
+		expg = aux;
+	}
+	if (expg)
+		atomic_dec(&expg->refcount);
+	mutex_unlock(&znd->map_pool_lock);
+
+	return stale_count;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int _pool_write(struct zoned *znd, struct map_pg **stale, int count)
+{
+	const int use_wq = 0;
+	int err;
+
+	/* write dirty table pages */
+	if (count > 0) {
+		int iter;
+		int last = count - 1;
+
+		sort(stale, count, sizeof(*stale), compare_lba, NULL);
+
+		for (iter = 0; iter < count; iter++)
+			cache_if_dirty(znd, stale[iter], use_wq);
+
+		for (iter = 0; iter < count; iter++) {
+			int sync = (iter == last) ? 1 : 0;
+
+			err = write_if_dirty(znd, stale[iter], use_wq, sync);
+			if (err) {
+				Z_ERR(znd, "Write failed: %d", err);
+				goto out;
+			}
+		}
+	}
+	err = count;
+
+out:
+	return err;
+}
+
+/**
+ * expire_old_pool() - Write old pages to disk and purge from RAM
+ * @znd: ZDM instance.
+ * @count: Goal number of LUT pages to write/purge [capped at 1024].
+ *
+ * Return: >= 0 on success or -errno value
+ */
+static int expire_old_pool(struct zoned *znd, int count)
+{
+	int iter;
+	int err = 0;
+	int stale_count = 0;
+	struct map_pg **stale = NULL;
+
+	if (count > 1024)
+		count = 1024;
+
+	if (!count)
+		goto out;
+
+	/* fill stale with OLD dirty lookup-table pages [Ignore CRC pgs] */
+	stale = ZDM_CALLOC(znd, sizeof(*stale), count, KM_19, NORMAL);
+	stale_count = _pool_fill(znd, stale, count, is_old_and_dirty, IS_LUT);
+	err = _pool_write(znd, stale, stale_count);
+	if (err < 0)
+		goto out;
+
+	sync_crc_pages(znd);
+
+	/* Stage 2: Drop clean table pages from ram */
+	if (test_bit(DO_GC_NO_PURGE, &znd->flags))
+		goto out;
+
+	stale_count = _pool_fill(znd, stale, count, is_old_del, 0);
+	for (iter = 0; iter < stale_count; iter++) {
+		if (stale[iter])
+			do_free_mapped(znd, stale[iter]);
+	}
+
+out:
+	if (stale)
+		ZDM_FREE(znd, stale, sizeof(*stale) * count, KM_19);
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * z_metadata_lba() - Alloc a block of metadata.
+ * @znd: ZDM Target
+ * @map: Block of metadata.
+ * @num: Number of blocks allocated.
+ *
+ * Return: lba or 0 on failure.
+ *
+ * When map->lba is less than data_lba the metadata is pinned to it's logical
+ * location.
+ * When map->lba lands in data space it is dynmaically allocated and intermixed
+ * within the datapool.
+ *
+ * [FUTURE: Pick a stream_id for floating metadata]
+ */
+static u64 z_metadata_lba(struct zoned *znd, struct map_pg *map, u32 *num)
+{
+	u32 nblks = 1;
+	u64 lba = map->lba;
+
+	if (lba < znd->data_lba) {
+		*num = 1;
+		return map->lba;
+	}
+	return z_acquire(znd, Z_AQ_META, nblks, num);
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * callback from dm_io notify.. cannot hold mutex here
+ */
+static void pg_crc(struct zoned *znd, struct map_pg *pg, __le16 md_crc)
+{
+	struct mpinfo mpi;
+
+	to_table_entry(znd, pg->lba, &mpi);
+	if (pg->crc_pg) {
+		int entry = mpi.crc.pg_idx;
+		struct map_pg *crc_pg = pg->crc_pg;
+
+		if (crc_pg && crc_pg->data.crc) {
+			atomic_inc(&crc_pg->refcount);
+			crc_pg->data.crc[entry] = md_crc;
+			set_bit(IS_DIRTY, &crc_pg->flags);
+			crc_pg->age = jiffies_64;
+			atomic_dec(&crc_pg->refcount);
+		}
+		put_map_entry(crc_pg);
+		if (crc_pg->refcount.counter == 0)
+			pg->crc_pg = NULL;
+	} else if (!test_bit(IS_LUT, &pg->flags)) {
+		znd->md_crcs[mpi.crc.pg_idx] = md_crc;
+	} else {
+		Z_ERR(znd, "unexpected state.");
+		dump_stack();
+	}
+}
+
+
+/**
+ * pg_written() - Handle accouting related to lookup table page writes
+ * @pg: The page of lookup table [or CRC] that was written.
+ * @error: non-zero if an error occurred.
+ *
+ * callback from dm_io notify.. cannot hold mutex here, cannot sleep.
+ *
+ */
+static int pg_written(struct map_pg *pg, unsigned long error)
+{
+	int rcode = 0;
+	struct zoned *znd = pg->znd;
+	__le16 md_crc;
+
+	if (error) {
+		Z_ERR(znd, "write_page: %" PRIx64 " -> %" PRIx64
+			   " ERR: %ld", pg->lba, pg->last_write, error);
+		rcode = -EIO;
+		goto out;
+	}
+
+	/*
+	 * Re-calculate CRC on current memory page. If unchanged then on-disk
+	 * is stable and in-memory is not dirty. Otherwise in memory changed
+	 * during write back so leave the dirty flag set. For the purpose of
+	 * the CRC table we assume that in-memory == on-disk although this
+	 * is not strictly true as the page could have updated post disk write.
+	 */
+
+	md_crc = crc_md_le16(pg->data.addr, Z_CRC_4K);
+	pg->age = jiffies_64;
+	if (md_crc == pg->md_crc)
+		clear_bit(IS_DIRTY, &pg->flags);
+
+	pg_crc(znd, pg, md_crc);
+
+	if (pg->last_write < znd->data_lba)
+		goto out;
+
+/*
+ * NOTE: If we reach here it's a problem.
+ *       TODO: mutex free path for adding a map entry ...
+ */
+
+	/* written lba was allocated from data-pool */
+	rcode = z_mapped_addmany(znd, pg->lba, pg->last_write, 1, CRIT);
+	if (rcode) {
+		Z_ERR(znd, "%s: Journal MANY failed.", __func__);
+		goto out;
+	}
+
+out:
+	return rcode;
+}
+
+
+/**
+ * on_pg_written() - A block of map table was written.
+ * @error: Any error code that occurred during the I/O.
+ * @context: The map_pg that was queued/written.
+ */
+static void on_pg_written(unsigned long error, void *context)
+{
+	struct map_pg *pg = context;
+	int rcode;
+
+	rcode = pg_written(pg, error);
+	atomic_dec(&pg->refcount);
+	if (rcode < 0)
+		pg->znd->meta_result = rcode;
+}
+
+/**
+ * queue_pg() - Queue a map table page for writeback
+ * @znd: ZDM Instance
+ * @pg: The target page to ensure the cover CRC blocks is cached.
+ * @lba: The address to write the block to.
+ */
+static int queue_pg(struct zoned *znd, struct map_pg *pg, u64 lba)
+{
+	const int use_wq = 0;
+	sector_t block = lba << Z_SHFT4K;
+	unsigned int nDMsect = 1 << Z_SHFT4K;
+	int rc;
+
+	atomic_inc(&pg->refcount);
+	pg->znd = znd;
+
+	MutexLock(&pg->md_lock);
+	pg->md_crc = crc_md_le16(pg->data.addr, Z_CRC_4K);
+	pg->last_write = lba; /* presumably */
+	mutex_unlock(&pg->md_lock);
+
+	rc = znd_async_io(znd, DM_IO_KMEM, pg->data.addr, block, nDMsect, WRITE,
+			  use_wq, on_pg_written, pg);
+	if (rc) {
+		Z_ERR(znd, "queue error: %d Q: %" PRIx64 " [%u dm sect] (Q:%d)",
+		      rc, lba, nDMsect, use_wq);
+		dump_stack();
+	}
+
+	return rc;
+}
+
+/**
+ * cache_if_dirty() - Load a page of CRC's into memory.
+ * @znd: ZDM Instance
+ * @pg: The target page to ensure the cover CRC blocks is cached.
+ * @wp: If a queue is needed for I/O.
+ *
+ * The purpose of loading is to ensure the pages are in memory with the
+ * async_io (write) completes the CRC accounting doesn't cause a sleep
+ * and violate the callback() API rules.
+ */
+static void cache_if_dirty(struct zoned *znd, struct map_pg *pg, int wq)
+{
+	atomic_inc(&pg->refcount);
+	if (test_bit(IS_DIRTY, &pg->flags) && test_bit(IS_LUT, &pg->flags) &&
+	    pg->data.addr) {
+		u64 base = znd->c_base;
+		struct map_pg *crc_pg;
+		struct mpinfo mpi;
+
+		if (test_bit(IS_REV, &pg->flags))
+			base = znd->c_mid;
+
+		to_table_entry(znd, pg->lba, &mpi);
+		base += mpi.crc.pg_no;
+		crc_pg = get_map_entry(znd, base, NORMAL);
+		if (!crc_pg)
+			Z_ERR(znd, "Out of memory. No CRC Pg");
+		pg->crc_pg = crc_pg;
+	}
+}
+
+/**
+ * write_if_dirty() - Write old pages (flagged as DIRTY) pages of table map.
+ * @znd: ZDM instance.
+ * @pg: A page of table map data.
+ * @wq: Use worker queue for sync writes.
+ * @snc: Performa a Sync or Async write.
+ *
+ * Return: 0 on success or -errno value
+ */
+static int write_if_dirty(struct zoned *znd, struct map_pg *pg, int wq, int snc)
+{
+	int rcode = 0;
+
+	if (!pg)
+		return rcode;
+
+	if (test_bit(IS_DIRTY, &pg->flags) && pg->data.addr) {
+		u64 dm_s = pg->lba;
+		u32 nf;
+		u64 lba = z_metadata_lba(znd, pg, &nf);
+
+		Z_DBG(znd, "Write if dirty: %" PRIx64" -> %" PRIx64, dm_s, lba);
+
+		if (lba && nf) {
+			int rcwrt;
+			int count = 1;
+			__le16 md_crc;
+
+			if (!snc) {
+				rcode = queue_pg(znd, pg, lba);
+				goto out;
+			}
+
+			md_crc = crc_md_le16(pg->data.addr, Z_CRC_4K);
+			rcwrt = write_block(znd->ti, DM_IO_KMEM,
+					    pg->data.addr, lba, count, wq);
+			pg->age = jiffies_64;
+			pg->last_write = lba;
+			pg_crc(znd, pg, md_crc);
+
+			if (rcwrt) {
+				Z_ERR(znd, "write_page: %" PRIx64 " -> %" PRIx64
+				      " ERR: %d", pg->lba, lba, rcwrt);
+				rcode = rcwrt;
+				goto out;
+			}
+
+			if (crc_md_le16(pg->data.addr, Z_CRC_4K) == md_crc)
+				clear_bit(IS_DIRTY, &pg->flags);
+
+			if (lba < znd->data_lba)
+				goto out;
+
+			rcwrt = z_mapped_addmany(znd, dm_s, lba, nf, NORMAL);
+			if (rcwrt) {
+				Z_ERR(znd, "%s: Journal MANY failed.",
+				      __func__);
+				rcode = rcwrt;
+				goto out;
+			}
+		} else {
+			Z_ERR(znd, "%s: Out of space for metadata?", __func__);
+			rcode = -ENOSPC;
+			goto out;
+		}
+	}
+out:
+	atomic_dec(&pg->refcount);
+
+	if (rcode < 0)
+		znd->meta_result = rcode;
+
+	return rcode;
+}
+
+/**
+ * keep_active_pages() - Write old pages to disk and purge from RAM
+ * @znd: ZDM instance.
+ * @allowed_pages: Goal number of pages to retain.
+ *
+ * Return: 0 on success or -errno value
+ */
+static int keep_active_pages(struct zoned *znd, int allowed_pages)
+{
+	int err = 0;
+
+	if (znd->incore_count > allowed_pages) {
+		int count = znd->incore_count - allowed_pages;
+		int rc = expire_old_pool(znd, count);
+
+		if (rc < 0) {
+			err = rc;
+			Z_ERR(znd, "%s: Failed to remove oldest pages!",
+				__func__);
+		}
+	}
+
+	return err;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+static int sync_dirty(struct zoned *znd, int bit_type, int count)
+{
+	int err = 0;
+	int stale_count;
+	struct map_pg **stale = NULL;
+
+	if (count > 1024)
+		count = 1024;
+
+	if (!count)
+		goto out;
+
+	stale = ZDM_CALLOC(znd, sizeof(*stale), count, KM_19, NORMAL);
+	stale_count = _pool_fill(znd, stale, count, is_dirty, bit_type);
+	err = _pool_write(znd, stale, stale_count);
+	if (err < 0)
+		goto out;
+out:
+	if (stale)
+		ZDM_FREE(znd, stale, sizeof(*stale) * count, KM_19);
+
+	return err;
+}
+
+/**
+ * sync_crc_pages() - Migrate crc pages to disk
+ * @znd: ZDM instance
+ *
+ * Return: 0 on success or -errno value
+ */
+static int sync_crc_pages(struct zoned *znd)
+{
+	const int max = 1024;
+	int stale;
+
+	do {
+		stale = sync_dirty(znd, IS_CRC, max);
+	} while (stale > 0);
+
+	return stale;
+}
+
+/**
+ * sync_mapped_pages() - Migrate lookup tables and crc pages to disk
+ * @znd: ZDM instance
+ *
+ * Return: 0 on success or -errno value
+ */
+static int sync_mapped_pages(struct zoned *znd)
+{
+	const int max = 1024;
+	int stale;
+
+	do {
+		stale = sync_dirty(znd, IS_LUT, max);
+	} while (stale > 0);
+
+	/* on error return */
+	if (stale < 0)
+		return stale;
+
+	stale = sync_crc_pages(znd);
+
+	return stale;
+}
+
+/**
+ * do_sync_metadata() - Migrate memcache entries through lookup tables to disk
+ * @param znd: ZDM instance
+ *
+ * Return: 0 on success or -errno value
+ */
+static int do_sync_metadata(struct zoned *znd)
+{
+	int err = _cached_to_tables(znd, znd->data_zones);
+
+	if (err)
+		goto out;
+
+	err = sync_mapped_pages(znd);
+out:
+	return err;
+}
+
+/**
+ * dm_s is a logical sector that maps 1:1 to the whole disk in 4k blocks
+ * Here the logical LBA and field are calculated for the lookup table
+ * where the physical LBA can be read from disk.
+ */
+static int map_addr_aligned(struct zoned *znd, u64 dm_s, struct map_addr *out)
+{
+	u64 block	= dm_s >> 10;
+
+	out->zone_id	= block >> 6;
+	out->lut_s	= block + znd->s_base;
+	out->lut_r	= block + znd->r_base;
+	out->pg_idx	= dm_s & 0x3FF;
+
+	return 0;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/**
+ * dm_s is a logical sector that maps 1:1 to the whole disk in 4k blocks
+ * Here the logical LBA and field are calculated for the lookup table
+ * where the physical LBA can be read from disk.
+ */
+static int map_addr_calc(struct zoned *znd, u64 origin, struct map_addr *out)
+{
+	u64 offset = ((origin < znd->md_end) ? znd->md_start : znd->md_end);
+
+	out->dm_s = origin;
+	return map_addr_aligned(znd, origin - offset, out);
+}
+
+/**
+ * read_pg() - Read a page of LUT/CRC from disk.
+ * @znd: ZDM instance
+ * @pg: Page to fill
+ * @gfp: Memory allocation rule
+ * @mpi: Backing page locations.
+ *
+ * Load a page of the sector lookup table that maps to pg->lba
+ * If pg->lba is not on disk return 0
+ *
+ * Return: 1 if page exists, 0 if unmodified, else -errno on error.
+ */
+static int read_pg(struct zoned *znd, struct map_pg *pg, u64 lba48, int gfp,
+		   struct mpinfo *mpi)
+{
+	int rcode = 0;
+	const int count = 1;
+	const int wq = 1;
+	int rd;
+	__le16 check;
+	__le16 expect = 0;
+
+	/**
+	 * This table entry may be on-disk, if so it needs to
+	 * be loaded.
+	 * If not it needs to be initialized to 0xFF
+	 *
+	 * When ! ZF_POOL_FWD and mapped block is FWD && LUT
+	 *   the mapped->lba may be pinned in sk_ pool
+	 *   if so the sk_ pool entry may need to be pulled
+	 *   to resvole the current address of mapped->lba
+	 */
+
+/* TODO: Async reads */
+
+	if (warn_bad_lba(znd, lba48))
+		Z_ERR(znd, "Bad PAGE %" PRIx64, lba48);
+
+	rd = read_block(znd->ti, DM_IO_KMEM, pg->data.addr, lba48, count, wq);
+	if (rd) {
+		Z_ERR(znd, "%s: read_block: ERROR: %d", __func__, rd);
+		rcode = -EIO;
+		goto out;
+	}
+
+	/*
+	 * Now check block crc
+	 */
+	check = crc_md_le16(pg->data.addr, Z_CRC_4K);
+	if (test_bit(IS_LUT, &pg->flags)) {
+		struct map_pg *crc_pg;
+		u64 base = znd->c_base;
+
+		if (test_bit(IS_REV, &pg->flags))
+			base = znd->c_mid;
+
+		crc_pg = get_map_entry(znd, mpi->crc.pg_no + base, gfp);
+		if (crc_pg) {
+			atomic_inc(&crc_pg->refcount);
+			if (crc_pg->data.crc) {
+				MutexLock(&crc_pg->md_lock);
+				expect = crc_pg->data.crc[mpi->crc.pg_idx];
+				incore_hint(znd, crc_pg);
+				mutex_unlock(&crc_pg->md_lock);
+				crc_pg->age = jiffies_64;
+			}
+			atomic_dec(&crc_pg->refcount);
+		}
+		put_map_entry(crc_pg);
+	} else {
+		expect = znd->md_crcs[mpi->crc.pg_idx];
+	}
+
+	if (check != expect) {
+		znd->meta_result = -ENOSPC;
+
+		Z_ERR(znd,
+		      "Corrupt metadata: %" PRIx64 " from %" PRIx64
+		      " [%04x != %04x]",
+		      pg->lba, lba48,
+		      le16_to_cpu(check),
+		      le16_to_cpu(expect));
+	}
+	rcode = 1;
+
+out:
+	return rcode;
+}
+
+/**
+ * cache_pg() - Load a page of LUT/CRC into memory from disk, or default values.
+ * @znd: ZDM instance
+ * @pg: Page to fill
+ * @gfp: Memory allocation rule
+ * @mpi: Backing page locations.
+ *
+ * Return: 1 if page loaded from disk, 0 if empty, else -errno on error.
+ */
+static int cache_pg(struct zoned *znd, struct map_pg *pg, int gfp,
+		    struct mpinfo *mpi)
+{
+	int rc = 0;
+	int empty_val = 0;
+	u64 lba48 = current_mapping(znd, pg->lba, gfp);
+
+	if (test_bit(IS_LUT, &pg->flags))
+		empty_val = 0xff;
+
+	atomic_inc(&pg->refcount);
+
+	MutexLock(&pg->md_lock);
+	pg->data.addr = ZDM_ALLOC(znd, Z_C4K, PG_27, gfp);
+	if (pg->data.addr) {
+		if (lba48)
+			rc = read_pg(znd, pg, lba48, gfp, mpi);
+		else
+			memset(pg->data.addr, empty_val, Z_C4K);
+	}
+	mutex_unlock(&pg->md_lock);
+
+	if (!pg->data.addr) {
+		Z_ERR(znd, "%s: Out of memory.", __func__);
+		goto out;
+	}
+
+	if (rc < 0) {
+		Z_ERR(znd, "%s: read_pg from %" PRIx64" [to? %d] error: %d",
+		      __func__, pg->lba, test_bit(IS_FWD, &pg->flags), rc);
+		ZDM_FREE(znd, pg->data.addr, Z_C4K, PG_27);
+		goto out;
+	}
+	znd->incore_count++;
+	pg->age = jiffies_64;
+	pool_add(znd, pg);
+
+	Z_DBG(znd, "Page loaded: lba: %" PRIx64, pg->lba);
+out:
+	atomic_dec(&pg->refcount);
+	return rc;
+}
+
+/* -------------------------------------------------------------------------- */
+/* -------------------------------------------------------------------------- */
+
+/* resolve a sector mapping to the on-block-device lba via the lookup table */
+static u64 z_lookup_table(struct zoned *znd, u64 addr, int gfp)
+{
+	struct map_addr maddr;
+	struct map_pg *mapped;
+	u64 old_phy = 0;
+
+	map_addr_calc(znd, addr, &maddr);
+	mapped = get_map_entry(znd, maddr.lut_s, gfp);
+	if (mapped) {
+		atomic_inc(&mapped->refcount);
+		if (mapped->data.addr) {
+			__le32 delta;
+
+			MutexLock(&mapped->md_lock);
+			delta = mapped->data.addr[maddr.pg_idx];
+			incore_hint(znd, mapped);
+			mutex_unlock(&mapped->md_lock);
+			old_phy = map_value(znd, delta);
+			mapped->age = jiffies_64;
+		}
+		atomic_dec(&mapped->refcount);
+		put_map_entry(mapped);
+	}
+	return old_phy;
+}
+
+/**
+ * update_map_entry() - Migrate memcache to lookup table map entries.
+ * @znd: ZDM instance
+ * @mapped: memcache block.
+ * @maddr: map_addr
+ * @to_addr: LBA or sector #.
+ * @is_fwd: flag forward or reverse lookup table.
+ *
+ * when is_fwd is 0:
+ *  - maddr->dm_s is a sector -> lba.
+ *         in this case the old lba is discarded and scheduled for cleanup
+ *         by updating the reverse map lba tables noting that this location
+ *         is now unused.
+ * when is_fwd is 0:
+ *  - maddr->dm_s is an lba, lba -> dm_s
+ *
+ * Return: non-zero on error.
+ */
+static int update_map_entry(struct zoned *znd, struct map_pg *mapped,
+			    struct map_addr *maddr, u64 to_addr, int is_fwd)
+{
+	int err = -ENOMEM;
+
+	if (mapped && mapped->data.addr) {
+		u64 index = maddr->pg_idx;
+		__le32 delta;
+		__le32 value;
+		int was_updated = 0;
+
+		atomic_inc(&mapped->refcount);
+		MutexLock(&mapped->md_lock);
+		delta = mapped->data.addr[index];
+		err = map_encode(znd, to_addr, &value);
+		if (!err) {
+			/*
+			 * if the value is modified update the table and
+			 * place it on top of the active [inpool] list
+			 * this will keep the chunk of lookup table in
+			 * memory.
+			 */
+			if (mapped->data.addr[index] != value) {
+				mapped->data.addr[index] = value;
+				mapped->age = jiffies_64;
+				set_bit(IS_DIRTY, &mapped->flags);
+				was_updated = 1;
+				incore_hint(znd, mapped);
+			} else {
+				Z_ERR(znd, "*ERR* %" PRIx64
+					   " -> data.addr[index] (%x) == (%x)",
+				      maddr->dm_s, mapped->data.addr[index],
+				      value);
+				dump_stack();
+			}
+		} else {
+			Z_ERR(znd, "*ERR* Mapping: %" PRIx64 " to %" PRIx64,
+			      to_addr, maddr->dm_s);
+		}
+		mutex_unlock(&mapped->md_lock);
+
+		if (was_updated && is_fwd && (delta != MZTEV_UNUSED)) {
+			u64 old_phy = map_value(znd, delta);
+
+			/*
+			 * add to discard list of the controlling mzone
+			 * for the 'delta' physical block
+			 */
+			Z_DBG(znd, "%s: unused_phy: %" PRIu64
+			      " (new lba: %" PRIu64 ")",
+			      __func__, old_phy, to_addr);
+
+			WARN_ON(old_phy >= znd->nr_blocks);
+
+			err = unused_phy(znd, old_phy, 0, NORMAL);
+			if (err)
+				err = -ENOSPC;
+		}
+		atomic_dec(&mapped->refcount);
+	}
+	return err;
+}
+
+/**
+ * move_to_map_tables() - Migrate memcache to lookup table map entries.
+ * @znd: ZDM instance
+ * @jrnl: memcache block.
+ *
+ * Return: non-zero on error.
+ */
+static int move_to_map_tables(struct zoned *znd, struct map_cache *jrnl)
+{
+	struct map_pg *smtbl = NULL;
+	struct map_pg *rmtbl = NULL;
+	struct map_addr maddr = { .dm_s = 0ul };
+	struct map_addr rev = { .dm_s = 0ul };
+	u64 lut_s = BAD_ADDR;
+	u64 lut_r = BAD_ADDR;
+	int jentry;
+	int err = 0;
+	int is_fwd = 1;
+
+	/* the journal being move must remain stable so sorting
+	 * is disabled. If a sort is desired due to an unsorted
+	 * page the search devolves to a linear lookup.
+	 */
+	atomic_inc(&jrnl->busy_locked);
+
+	for (jentry = jrnl->jcount; jentry > 0;) {
+		u64 dm_s = le64_to_lba48(jrnl->jdata[jentry].logical, NULL);
+		u64 lba = le64_to_lba48(jrnl->jdata[jentry].physical, NULL);
+
+		if (dm_s == Z_LOWER48 || lba == Z_LOWER48) {
+			jrnl->jcount = --jentry;
+			continue;
+		}
+
+		map_addr_calc(znd, dm_s, &maddr);
+		if (lut_s != maddr.lut_s) {
+			put_map_entry(smtbl);
+			if (smtbl)
+				atomic_dec(&smtbl->refcount);
+			smtbl = get_map_entry(znd, maddr.lut_s, NORMAL);
+			if (!smtbl) {
+				err = -ENOMEM;
+				goto out;
+			}
+			atomic_inc(&smtbl->refcount);
+			lut_s = smtbl->lba;
+		}
+
+		is_fwd = 1;
+		if (lba == 0ul)
+			lba = BAD_ADDR;
+		err = update_map_entry(znd, smtbl, &maddr, lba, is_fwd);
+		if (err < 0)
+			goto out;
+
+		/*
+		 * In this case the reverse was handled as part of
+		 * discarding the forward map entry -- if it was in use.
+		 */
+		if (lba != BAD_ADDR) {
+			map_addr_calc(znd, lba, &rev);
+			if (lut_r != rev.lut_r) {
+				put_map_entry(rmtbl);
+				if (rmtbl)
+					atomic_dec(&rmtbl->refcount);
+				rmtbl = get_map_entry(znd, rev.lut_r, NORMAL);
+				if (!rmtbl) {
+					err = -ENOMEM;
+					goto out;
+				}
+				atomic_inc(&rmtbl->refcount);
+				lut_r = rmtbl->lba;
+			}
+			is_fwd = 0;
+			err = update_map_entry(znd, rmtbl, &rev, dm_s, is_fwd);
+			if (err == 1)
+				err = 0;
+		}
+
+		if (err < 0)
+			goto out;
+
+		jrnl->jdata[jentry].logical = MC_INVALID;
+		jrnl->jdata[jentry].physical = MC_INVALID;
+		jrnl->jcount = --jentry;
+	}
+out:
+	if (smtbl)
+		atomic_dec(&smtbl->refcount);
+	if (rmtbl)
+		atomic_dec(&rmtbl->refcount);
+	put_map_entry(smtbl);
+	put_map_entry(rmtbl);
+	set_bit(DO_MEMPOOL, &znd->flags);
+
+	atomic_dec(&jrnl->busy_locked);
+
+	return err;
+}
+
+/**
+ * unused_phy() - Mark a block as unused.
+ * @znd: ZDM instance
+ * @lba: Logical LBA of block.
+ * @orig_s: Sector being marked.
+ * @gfp: Memory allocation rule
+ *
+ * Return: non-zero on error.
+ *
+ * Add an unused block to the list of blocks to be discarded during
+ * garbage collection.
+ */
+static int unused_phy(struct zoned *znd, u64 lba, u64 orig_s, int gfp)
+{
+	int err = 0;
+	struct map_pg *mapped;
+	struct map_addr reverse;
+	int z_off;
+
+	if (lba < znd->data_lba)
+		return 0;
+
+	map_addr_calc(znd, lba, &reverse);
+	z_off = reverse.zone_id % 1024;
+	mapped = get_map_entry(znd, reverse.lut_r, gfp);
+	if (!mapped) {
+		err = -EIO;
+		Z_ERR(znd, "unused_phy: Reverse Map Entry not found.");
+		goto out;
+	}
+
+	if (!mapped->data.addr) {
+		Z_ERR(znd, "Catastrophic missing LUT page.");
+		dump_stack();
+		err = -EIO;
+		goto out;
+	}
+	atomic_inc(&mapped->refcount);
+
+	/*
+	 * if the value is modified update the table and
+	 * place it on top of the active [inpool] list
+	 */
+	if (mapped->data.addr[reverse.pg_idx] != MZTEV_UNUSED) {
+		u32 gzno  = reverse.zone_id >> GZ_BITS;
+		u32 gzoff = reverse.zone_id & GZ_MMSK;
+		struct meta_pg *wpg = &znd->wp[gzno];
+		u32 wp;
+		u32 zf;
+		u32 stream_id;
+
+		if (orig_s) {
+			__le32 enc = mapped->data.addr[reverse.pg_idx];
+			u64 dm_s = map_value(znd, enc);
+			int drop_discard = 0;
+
+			if (dm_s < znd->data_lba) {
+				drop_discard = 1;
+				Z_ERR(znd, "Discard invalid target %"
+				      PRIx64" - Is ZDM Meta %"PRIx64" vs %"
+				      PRIx64, lba, orig_s, dm_s);
+			}
+			if (orig_s != dm_s) {
+				drop_discard = 1;
+				Z_ERR(znd,
+				      "Discard %" PRIx64
+				      " mismatched src: %"PRIx64 " vs %" PRIx64,
+				      lba, orig_s, dm_s);
+			}
+			if (drop_discard)
+				goto out_unlock;
+		}
+		MutexLock(&mapped->md_lock);
+		mapped->data.addr[reverse.pg_idx] = MZTEV_UNUSED;
+		mutex_unlock(&mapped->md_lock);
+
+		mapped->age = jiffies_64;
+		set_bit(IS_DIRTY, &mapped->flags);
+		incore_hint(znd, mapped);
+
+		MutexLock(&wpg->wplck);
+		wp = le32_to_cpu(wpg->wp_alloc[gzoff]) & Z_WP_VALUE_MASK;
+		zf = le32_to_cpu(wpg->zf_est[gzoff]) & Z_WP_VALUE_MASK;
+		stream_id = le32_to_cpu(wpg->zf_est[gzoff]) & Z_WP_STREAM_MASK;
+		if (wp > 0 && zf < Z_BLKSZ) {
+			zf++;
+			wpg->zf_est[gzoff] = cpu_to_le32(zf | stream_id);
+			set_bit(IS_DIRTY, &wpg->flags);
+		}
+		mutex_unlock(&wpg->wplck);
+		if (wp == Z_BLKSZ)
+			znd->discard_count++;
+	} else {
+		Z_DBG(znd, "lba: %" PRIx64 " alread reported as free?", lba);
+	}
+
+out_unlock:
+	atomic_dec(&mapped->refcount);
+
+out:
+	put_map_entry(mapped);
+
+	return err;
+}
-- 
2.7.0

